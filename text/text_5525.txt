BACKGROUND
affinity-purification mass spectrometry  is a powerful tool for identification of protein complexes and interactions and enables the larger scale analysis of protein networks and cellular processes  <cit> . ap-ms combines the specificity of antibody-based purification of proteins and protein complexes with the sensitivity of mass-spectrometry for identification and quantification, and it has been widely applied to diverse biological systems  <cit> . although several ap-ms methodology variants have been developed, typical high-throughput work-flows express recombinant epitope-tagged “bait” proteins in cultured cells, recover protein complexes through affinity-purification against the epitope tag, and then identify proteins using lc-ms/ms  <cit> .

principal challenges in interpreting ap-ms data are the presence of false-positive interacting proteins, misidentification at the ms database search level, and the variability of replicated experiments  <cit> . false-positives in ap-ms data can be classified as external protein contaminants introduced into the samples during sample processing , and contaminants due to the affinity-purification process itself  <cit> . in the latter case, proteins that are affinity-purified but that are not specific for the bait of interest are hereafter referred to as non-specific prey proteins. these proteins may be purified through their affinity to solid matrices, antibodies or epitope tags employed in the experiment. carefully designed control experiments may be used to identify non-specific prey proteins, such as cells expressing the epitope tag alone, the parent cells, or cells transfected with the recombinant bait protein but not induced. a second frequent challenge of ap-ms data is interpreting results with low reproducibility. low reproducibility may result from biological variability of the cells or tissues or technical variability from the affinity-purification or mass-spectrometry. replicate ap-ms experiments may identify different sets and numbers of proteins. however, selecting only highly reproducible proteins from replicate ap-ms experiments may be too conservative, and novel bait-associated proteins that occur in only a fraction of the replicates may be discarded. the approach described here carefully removes outlier experiments in ap-ms data before applying final statistical procedures aiming at removing contaminants, i.e. non-specific prey proteins.

alongside the development of experimental methods to improve ap-ms experiments, several studies have developed computational tools for processing ap-ms data that address the problems of non-specific proteins and providing protein-protein interaction  confidence scores. different scoring methods have been developed for prioritizing specific prey proteins in ap-ms data, using quantitative information that typically include a measure of frequency and/or abundance, such as spectral counts  <cit> , and scores from search-engines such as mascot <cit> . methods such as the socio-affinity index   <cit> , normalized spectral abundance factor   <cit> , compass   <cit> , saint <cit>  and decontaminator <cit>  have been designed to filter contaminants and assign confidence scores to ppis. most of these methods take into consideration the total number of replicated experiments in their scoring systems, although in general without considering the quality control of the replicated experiments. there are statistical methods available to assess reproducibility of large scale lc-ms experiments including the coefficient of variation  and anova  <cit> . alternatively, distance measures such as euclidean distance have been used to measure reproducibility in large scale lc-ms experiments  <cit> . a recent study from the national cancer institute supported clinical proteomics technologies assessment for cancer  network evaluated reproducibility in inter-laboratory lc-ms/ms proteomics studies  <cit> . this study concluded that reproducibility is higher for proteins than for peptides, and those factors such as trypsin specificity, peptide ion intensity, and the nature of the corresponding proteins influenced reproducibility in peptide identifications. although these are useful indicators for all proteomics studies, ap-ms studies, because of their additional sample processing, have additional parameters and complexities such as the presence of non-specific prey proteins that merit special attention, and for which standard statistical measures may not be appropriate.

current small and medium-scale ap-ms studies typically pair bait experiments with control experiments, in which the bait protein is not expressed. interacting proteins are then identified as proteins specific to bait experiments or through quantitation, using label-free or isotope-based methods to identify proteins more abundant in bait experiments than control  <cit> . with larger-scale ap-ms experiments, it becomes possible to define a profile of background, non-specific prey proteins, through for example analysis of a large set of bait ap-ms experiments  <cit> , or a set of control ap-ms experiments  <cit> . as the capability to perform large-scale ap-ms experiments becomes more widespread, and ap-ms methodologies become more standardized, we anticipate the development of ap-ms dataset repositories, that can be used to define the background profile for specific ap-ms biological systems and ap-ms experiments. the computational problem that we address in this paper is then the problem of distinguishing specific prey proteins in ap-ms data from the background of non-specific preys.

we propose a comprehensive method for selecting reproducible ap-ms replicated experiments and subsequently for identifying bait-specific preys when experimental replicates and control experiments are available. we focus on the analysis of reproducibility in ap-ms experiments, and the improved sensitivity and specificity of prey protein identification that can be achieved by rigorous application of quality control to ap-ms data. we emphasize that an accurate identification of the sought-after bait-specific prey proteins can only be achieved by carefully trading off the sensitivity/specificity through a combined analysis of false positive and false negative bait-prey ppis, and that this is highly dependent on which set of prey proteins is used beforehand to make inferences, i.e. whether one uses the entire prey protein space vs. a selected subspace of prey proteins.

unlike existing scoring methods, our method rocs first curates the set of experiments used in the scoring, effectively removing replicates that are outliers and therefore eliminating noise that would affect adequate further analysis. first, our method introduces the concept of indicator prey proteins that can be used to identify reproducible  ap-ms experiments. second, the method defines the concept of confidence score to select specific preys by simultaneously controlling the false discovery rate  and a measure of biological coherence against biological annotations such as gene ontology . we show that improved accuracy of predicted interacting proteins can be achieved, where accuracy is to be understood here in the usual classification sense, that is, as the degree to which each new prey protein is correctly classified as a specific or non-specific interacting protein. we use as a test case a subset of systematically generated ap-ms data from a previous large-scale ap-ms study of the human interactome  <cit>  to show that our method may be used on its own or in conjunction with other ap-ms scoring schemes to improve the accuracy of ppi inferences. our method is also applicable to smaller ap-ms datasets. indeed, we show that our method is scalable to ap-ms experiments with varying numbers of replicates, and that we are able to determine the minimum number of replicates that an ap-ms study should have in order to make reliable inferences. we illustrate the performance of the method on five separate small-scale ap-ms experiments for which we could successfully identify the most replicated experiments and rank the bait-specific prey proteins. as ap-ms data continues to be acquired and deposited in publicly available repositories , we anticipate that our method will be applicable to larger sets of ap-ms data as well as other types of proteomics data and databases in general. finally, the method is available as an r package called “rocs”, freely available from the cran repository http://cran.r-project.org/.

methods
underlying premise
we consider a large scale affinity purification-mass spectrometry study  consisting of a set of k uniquely identified experimental replicatese <dig> …,ek. see additional file 1: supplemental methods for more details on the initial input dataset structure . our premise is that prior identification of a subset of most reproducible experimental replicates greatly improves the differentiation of signal from noise and the ability to identify true specific protein-protein interactions. we show that this is achieved by first finding a set of highly reproducible proteins/peptides.

probability-based peptide identification statistics such as mascot peptide identification score  <cit>  is one of the objective measures available for feature  identifiability . although high-scoring features are intuitively correlated to higher experimental reproducibility, we argue that this is not necessarily the case. in fact, we observed that although measures of feature abundance and feature reliability, such as spectral count and search-engine score, are broadly correlated, these measures are not necessarily correlated with feature experimental reproducibility [additional file 2: figure s <dig>  this may not come as a surprise since feature reliability and experimental feature reproducibility are conceptually distinct, and the reason for using reliability as a surrogate measure of reproducibility has rather been a practical one alone. in addition, how the feature identification score threshold  is chosen in practice, is currently not justified.

we hypothesized that a combined measure of feature reliability with reproducibility would not necessarily be monotonic with respect to the reliability score threshold used for filtering in the features. by combining the information contained within a measure of feature reliability, such as a search-engine feature identification score, with a standardized measure of experimental reproducibility, such as the feature frequency of occurrence across experimental replicates, we show that an optimal feature reliability score threshold can be determined to filter the features in. moreover, we propose an automatic and objective way of finding this optimal reliability score threshold, above which one should select the peptides and corresponding proteins. we named these specific proteins/peptides thereby selected as indicator prey proteins. they appear in the most replicated ap-ms experiments, thus allowing differentiation of the reproducible experimental replicates from the outliers. the resulting filtered dataset is then used for specificity analysis and removal of non-specific prey proteins.

also, at least for the application proposed here, probability-based peptide reliability statistics such as mascot peptide identification score  <cit> , and peptide prophet probability  <cit>  appear somewhat interchangeable, [additional file 2: figure s <dig>  in the remainder of the study, we used mascot peptide identification score as our primary score, although a probability measure is fully compatible with our method and may be used instead.

reproducibility index and indicator prey proteins
for a given bait experiment, we consider the initial set of n prey proteinsn <dig> …,nn that can be uniquely identified by their ipi accession number across all their peptides and experimental replicatese <dig> …,ek. we denote this initial procedural stage by “n”, standing for “naïve”. next, we consider a subset p <dig> …,pp⊆n <dig> …,nn of uniquely identified p prefiltered prey proteins for which their corresponding mascot peptide identification scores are greater than the mascot score threshold  as described in details in additional file  <dig> supplemental methods.

to further refine our set of prefiltered prey proteins and identify the reproducible experiments, we introduce a reproducibility index , motivated as follows. on the one hand, including lower mascot scores proteins would include many unreliable proteins and thus degrade reproducibility across experimental replicatese <dig> …,ek. on the other hand, including higher mascot score-only proteins would include reliable-only proteins across all experimental replicatese <dig> …,ek, that is, fewer proteins in number. the idea is to account for measures of protein reliability  as well as protein reproducibility . hence, our reproducibility index, designed as a normalized  measure of reproducibility, and defined as the average frequency of occurrences of prefiltered prey proteins across all experimental replicates {e <dig> …,ek} and across their corresponding peptides whose mascot scores are greater than a given threshold, denoted s. it is formally defined for fixed k as:

  ris=1k·ps∑j=1ps∑k=1kipj∈ek∧scorepj≥s 

where pj∈ek denotes a unique occurrence of the prefiltered prey protein pj in experimental replicate ek for k ∈{ <dig>  … ,k}, and where p denotes the number of prefiltered prey proteins for which their corresponding peptide scores are greater than a given peptide score threshold, denoted s. i denotes the indicator function throughout the article. note that ri ∈  <cit>  and that higher ri represents greater glob reproducibility across experimental replicates.

it follows from the above that the reproducibility index  is expected to vary as a function of the mascot score, when used as a threshold. so, we define the reproducibility index threshold  as the peptide mascot score threshold maximizing the reproducibility index , i.e. formally rit=argmaxs∈mst,+∞ris. the reproducibility index threshold  is used for subseting a set of q uniquely identified proteins from the prefiltered prey proteins, which we termed indicator prey proteins, denoted by q <dig> …,qq, and for which their corresponding peptide mascot scores are greater than rit:

  q <dig> …,qq=pj,j∈ <dig> …,p:score≥rit 

next, for each indicator prey protein, we define its marginal inclusion probability, denoted pm for j ∈ <dig> …,q, across all experimental replicatese <dig> …,ek. one may now define a subset of indicator prey proteins for which their marginal inclusion probability is greater than a given marginal inclusion probability threshold p˜min, as well as a corresponding joint inclusion probability pj across all experimental replicatese <dig> …,ek. details on definitions and estimates are provided in additional file 1: supplemental methods. hence, by fixing a marginal inclusion probability threshold p˜min, a subset of highly reproducible indicator prey proteins can be identified for which their marginal inclusion probability is greater than the p˜min threshold and their joint inclusion probability is relatively high. how this threshold is chosen in any bait or control experiment is described in the subsequent subsection “setting a marginal inclusion probability threshold”.

identification of reproducible experimental replicates and reproducible prey proteins
one may determine a subset of e <dig> …,ek for which all indicator prey proteinsjointly appear in each individual experimental replicate ek for k ∈ { <dig>  … , k}. we claim that these are the most reproducible experiments, which we term reproducible experimental replicates. we denote this subset of l experiments by f <dig> …,fl⊆e <dig> …,ek where the dependency notation with the marginal inclusion probability threshold p˜min has been dropped for simplification. for a given p˜min, this cardinal can be estimated as:

  l^=∑k=1kiq <dig> …,qq∈ekforp˜min∈ <dig>  

from the reduced sets of reproducible experimental replicatesf <dig> …,fl, one may now select the corresponding subset r <dig> …,rr⊆p <dig> …,pp of cardinal r of uniquely identified and most reproducible prefiltered prey proteins, which appear at least once in reproducible experimental replicatesf <dig> …,fl. in keeping with previous notations and simplifications, we further term this subset by reproducible prey proteins and denote it by r <dig> …,rr. we denote this procedural stage by “r”, standing for “reproducible”.

setting a marginal inclusion probability threshold
the choice of the marginal inclusion probability threshold p˜min in any bait or control experiment depends on the goal and is guided by some simple considerations. first, one can set this threshold to higher probability levels in order to accommodate larger sets of indicator prey proteins as well as reproducible experimental replicates. conversely, this threshold can be set to lower probability levels in order to remove outlier experiments as thoroughly as possible. so, the setting of this threshold controls the level of experimental reproducibility and is a matter of tradeoff between specificity and sensitivity and the goals of the experiment.

second, in every experiment the marginal inclusion probability threshold p˜min should be lower bounded so as to get at least a strictly positive number of reproducible experimental replicates, that is l >  <dig>  where for simplification reasons dependency with respect to p˜min is dropped, but understood. so, the interval for the marginal inclusion probability threshold p˜min should always be as follows:

  argminl>0p˜min≤p˜min≤ <dig> 

in practice, we noted that the choice of the marginal inclusion probability threshold has relatively little influence as long as it remains within admissible boundaries =prrjb∈f1b,…,flbb for j ∈ { <dig>  …, rb} and p′mc=prrjc∈f1c,…,flcc for j ∈ { <dig>  …, rc}. their estimates are given in details in supplemental methods .

next, we introduce a  score of individual bait-prey interaction specificity, which we term the confidence score for the j-th prey protein in r1b,…,rrbb, and for fixed p˜minb and p˜minc, denoted by cs, as follows:

  csj=pˆ′mb-pˆ′mcpˆ′mb-pˆ′mc.pˆ′mbforj∈ <dig> …,rb 

as can be seen, this score is an individual bait-prey interaction specificity measure. it is a standardized ratio accounting for the probability of occurrence of each reproducible prey protein relative to the bait and the control experiments, weighted up/down by the marginal inclusion probability of each reproducible prey protein in the bait experiment alone. in other words, the confidence score accounts for measures of bait-prey specificity and bait-prey frequency altogether. note that by definition cs∈− <dig> , where negative and positive scores correspond to non-specific and specific interacting prey proteins respectively. in keeping with previous notations, a subset of specific prey proteins may be found by taking the reproducible prey protein in r1b,…,rrbb for which the confidence score is greater than a specificity cutoff, denoted cscutoff, which is to be estimated . we denote the subset of specific prey proteins by s1b,…,ssbb of cardinal set sb=s1b,…,ssbb, and defined as:

s1b,…,ssbb=rjb,j∈ <dig> …,rb:cs≥cscutoff for 

  cscutoff∈ <dig>  

we denote this last procedural stage by “s”, standing for “specific”. in practice the confidence score is computed after the aforementioned identification of unique indicator prey proteins and reproducible experimental replicates, that is, after pre-specifying marginal inclusion probability thresholds in both control and bait experiments: p˜minb and p˜minc as explained in . also, in the search for bait specific protein-protein-interactions , the confidence score cutoff is to be estimated from the data. we show in the following section how to do so automatically by simultaneously controlling the false discovery rate  and the gene ontology  semantic similarity of the candidate preys to the bait.

automatic estimation of an optimal confidence score cutoff
to objectively validate any protein-protein-interaction  identification procedure in ap-ms data analysis, one needs to simultaneously assess the sensitivity/specificity of the final sets of prey proteins identified. in the case of our ppi identification procedure, an optimal confidence score cutoff cscutoff should be objectively estimated in order to yield optimal sensitivity-specificity trade-offs. we observed that higher cscutoff values yield better specificity  but lower sensitivity , and vice-versa. the decision is a matter of false negative - false positive trade-off.

although an fdr analysis does not control by definition the overall false positive detections, it can indirectly enable the control of the specificity inherent to such ppi procedure . by definition, the fdr is the expected proportion of the number of erroneous rejected null hypotheses to the total number of rejected null hypotheses in the context of multiple hypotheses testing  <cit> . here, the fdr corresponds to the expected fraction of falsely identified bait-prey ppis  among all bait-prey ppis discoveries. practically, the fdr estimate for any given confidence score cutoff cscutoff is computed as: fd^rcscutoff=f^pcscutoff/f^pcscutoff+t^pcscutoff, where f^pcscutoff and t^pcscutoff represent the estimated false positive and true positives respectively, and f^pcscutoff+t^pcscutoff represents the estimated total positives i.e. all identified bait-prey ppi discoveries. also, as usual in fdr analysis, the theory allows two possible goals  <cit> : one may fix the confidence score cutoff cscutoff beforehand, then determine the corresponding estimated fd^rcscutoff. alternatively, one may impose the fdr to be bounded to some significance level fd^rcscutoff≤θ and then determine which values of the estimated confidence score cutoff are permissible to keep the fdr below that level. in practice, we opted for the first goal of analysis by fixing the confidence score cutoff to the value achieving the lowest estimated fdr . in the following, for simplification reasons, dependencies with respect to cscutoff, p˜minb and p˜minc, will be dropped, but understood.

to estimate the false positives ppis in our rocs procedure and control for specificity, we adopted an approach similar to the “target decoy” approach that is widely used in database searching for estimating false positives and/or false discovery rates  <cit> . the f^p estimate is calculated by averaging the number of identified bait-prey ppi due to contaminants only from the entire set of control experiments. technically, the f^p estimate is computed by applying the entire rocs identification procedure to repeated  random samples  of prey proteins identified from the stage “n” of control experiments. the f^p estimate is then computed as the average number of identified bait-prey ppi above the confidence score cutoff cscutoff expected in the monte-carlo replicates. with monte-carlo replications from the control experiment, we assume that all confidence score observed above a given confidence score cutoff cscutoff should be considered as false positives. details on how fdr estimates are computed are provided in additional file 1: supplemental methods.

to indirectly control for sensitivity in our rocs procedure, we used the most appropriate surrogate measure of biological relevance/coherence that is available in real datasets. although it cannot gauge the sensitivity for any new datasets where baits and preys or their interactions are functionally uncharacterized, it remains one of the best available ways to benchmark the sensitivity of rocs for all known baits-preys ppis. this is an estimation technique that is generally missing in analytical methods of ap-ms data. specifically, we used the resnik measure of semantic similarity  <cit> , one of the most common semantic similarity measures used with gene ontology   <cit> , to assess the biological relevance between a go term from the bait protein and another one for each specific prey protein. the pairwise resnik measure of semantic similarity is a node-based measure relying on a quantitative characterization of information called information content   <cit>  that is computed between two concepts . the information shared by two concepts is indicated by the information content of the concepts that encompass them  <cit> , formally defined as simresc <dig> c2=maxc∈sic, where s is the set of concepts that encompass both c <dig> and c <dig>  here, we computed the pairwise resnik measure of semantic similarity between two gene ontology  terms, one for the bait protein  and the other for each specific prey protein , within a given ontology , and denoted simcb,cp. the pairwise resnik measure of semantic similarity between two go terms is simply the information content of their most informative common ancestor  in the ontology  <cit> . we performed go semantic similarity analyses as a function of the confidence score cutoff cscutoff both for the set of specific prey proteins  that was found by considering either the entire set of bait experimental replicates  or the selected set of bait reproducible experimental replicates .

to assess significance in the difference of go biological relevance between the two groups being compared  for every confidence score cutoff cscutoff, we tested the null hypothesis that the median resnik measures of semantic similarity for the two groups at a given cscutoff, denoted simcscutoffcb,cp, do not differ statistically. we built 100% confidence intervals  of the median resnik measure of semantic similarity for the two groups for every confidence score cutoff cscutoff, and reject the null hypothesis at the θ level if their 100% cis do not overlap, or if the 100% ci of the difference of their medians does not contain zero. the distance between the cis of these medians was computed for every confidence score cutoff cscutoff as the difference between the lower bound  of the 100% ci from the “r” stage and the upper bound  of the 100% ci from the “n” stage, formally dcscutoff=lbsimcscutoffrcb,cp−ubsimcscutoffncb,cp. so, values of the confidence score cutoff cscutoff for which dcscutoff is positive represent significant increase  in go biological relevance from stage “n” to “r”, thereby indicating corresponding choices for the confidence score cutoff cscutoff.

to get an approximate100% ci for comparing two medians , we used mcgill et al.'s approximation  <cit> . the 100% ci of the median can be approximated based on its asymptotic normality, and is said to be rather insensitive to the underlying distribution of the samples. the approximate 100% ci of the median m extends to m^±c/ <dig> ·iq^r/n, where m^ is the median estimate and iq^r is the interquartile range estimate and n is the sample size   <cit> . to estimate these parameters, b monte-carlo replicates were performed by repeated random sampling without replacement of a sample of prey proteins from the entire set of bait experiments , of size sb, i.e. equal to that of the set of specific prey proteins . finally, c is chosen such thatc∈ <dig> , <dig>  for θ= <dig> , depending on how similar group sample sizes and group standard deviations are  <cit> .

finally, to reach the optimal sensitivity/specificity trade-off for bait-prey ppis, one finds the optimal estimate fdrc^scutoff of the confidence score cutoff achieving simultaneously the lowest estimated fdrc^scutoff, and the largest increase in go semantic similarity distance dc^scutoff, interpreted as a significance measure of increase in gene ontology  biological relevance. in practice the confidence score cutoff is to be estimated only within the positive range c^scutoff∈ <dig>  since only positive confidence scores are relevant to find specific ppis, and in order to avoid the singularity c^scutoff= <dig>  in addition, a range even shorter than c^scutoff∈ <dig>  is often good enough to find the c^scutoff estimate.

testing stability on multi-scale sets of experimental replicates
to validate our identification procedure, we tested its performance on multiple experimental scales of ap-ms data to see how its output remains “stable” as a function of the number of experimental replicates. here, the output was taken as the joint inclusion probabilitypj of indicator prey proteins , computed across all experimental replicatese <dig> …,ek, where k∈ <dig> k is the experimental scale. in our case, maximum experimental scale  ranged from small  to large . in the following, the maximum experimental scale  and the marginal inclusion probability threshold  are supposed to be fixed, so we further dropped their dependencies throughout the following formal definitions. eventually, the test reveals the minimum experimental scale required by an ap-ms experiment in order to reliably assess the reproducible experimental replicates .

we sought to derive bootstrap estimates of the joint inclusion probability of interest by applying the idea of the bootstrap resampling technique  <cit>  to our problem. this technique has been well recognized for instance in cluster analysis in phylogenetic studies  <cit> . here, we adapted the idea of bootstrap resampling technique to account for the uncertainty of results caused by sampling error of data. to assess this uncertainty, efron and shimodaira recently introduced a correction called the multiscale bootstrap resampling method  <cit>  to better agree with standard ideas of confidence levels and hypothesis testing and to account for the possible bias in the computation of the bootstrap probability value of a cluster. specifically, we computed a so-called multiscale unbiased joint inclusion probability estimate ˆpj*b for each experimental scale k∈ <dig> k by means of l*b bootstrapped reproducible experimental replicates, where b∈ <dig> …,b <dig> denotes the bootstrap sample  <cit> . finally, the entire procedure is repeated b <dig> times to get the corresponding mean and standard error estimates pÂ¯uj and se, simply by taking the average over the b <dig> replicates. details on how to compute multiscale bootstraps and derive the probability estimates are given in additional file 1: supplemental methods.

the goal was to look at how multiscale joint inclusion probability estimates distribute with respect to a range of experimental scales k∈ <dig> k, and specifically, whether there was any drop in the uniformity of its distribution. a drop in uniformity indicates a change-point, denoted k^min, below which the identification procedure is not reliable any more. this corresponds to the minimum scale of experimental replicates that an ap-ms experiment should have to reliably assess which replicated experiments are reproducible.

workflow
RESULTS
we use data from a previously published human ap-ms dataset to develop our approach  <cit> . this dataset corresponds to ap-ms experiments using multiple different bait proteins as well as control ap-ms experiments. hereafter, superscripts b and c correspond to the bait and the control experiments respectively. the control dataset consists of kc=200experimental replicates with an initial number of nc= <dig> control proteins, uniquely identified by their ipi accession numbers, and their corresponding unique control peptide sequences  additional file 1: supplemental methods. bait ap-ms datasets used here correspond to the following bait genes: vhl, ctnnbip <dig>  nme <dig>  ppm1b and stk <dig> with the following initial experimental replicates: vhl kb= <dig>  ctnnbip <dig> kb= <dig>  nme <dig> kb= <dig>  ppm1b kb= <dig>  and stk <dig> kb= <dig> [table  <dig>  we illustrate our methodology using the ctnnbip <dig> and stk <dig> dataset since they contain multiple well known interacting prey proteins  and small numbers of experimental replicates  with which we can validate our approach in a realistic way. complete results of all bait ap-ms datasets are provided in the supplemental information. in the ctnnbip <dig> and stk <dig> ap-ms dataset, the initial number of uniquely identified prey proteins was nb= <dig> and nb= <dig> with corresponding unique prey peptide sequences  [table  <dig> 

results are reported for cs>c^scutoff and the marginal inclusion probability thresholds p˜minc and p˜minb as determined in each ap-ms bait experiment.

determination of the reproducibility index and reproducibility index threshold in control and bait experiments
we first plotted the peptide prophet probabilities versus the peptide mascot scores in all bait experiments to objectively determine our mascot score threshold  and filter out un-reliable peptides  according to the method described in the methods section. we have estimated three quartile curves for the b-spline model for visual purposes, but we only used the median regression function for the computation of the mascot score threshold. in the instance of the ctnnbip <dig> bait experiment, the median mascot score threshold corresponding to a 50% peptide probability threshold was mst =  <dig> , leaving pb= <dig> uniquely identified proteins . likewise, the median mascot score threshold in the control experiment was mst =  <dig> , leaving pc= <dig> uniquely identified proteins. results for all bait experiments are reported in additional file 2: figure s <dig>  we also report the empirical probability density function  and cumulative density function  plots of the above peptide scores and peptide probabilities in the ap-ms control and all bait experiments . these plots show the locations of the peptide probability thresholdprob= <dig>  and corresponding median mascot score thresholds .

our initial finding was that the reproducibility index  is not monotonic as a function of the mascot score threshold. we computed our reproducibility index  in the control and all bait experiments and plotted it against a range of peptide score thresholds s∈mst,+∞. in all experiments tested the quantity of interest  always peaks at a certain optimal value of the peptide score threshold, which we have termed reproducibility index threshold  . for instance, in the control and in the ctnnbip <dig> bait experiments, these reproducibility index thresholds were rit =  <dig>  and rit =  <dig>  respectively .

identification of indicator prey proteins, reproducible experimental replicates, and reproducible prey proteins in control and bait experiments
the reproducibility index thresholds  in the control and all bait experiments were used with a range of marginal inclusion probability thresholds p˜minc∈ <dig>  and p˜minb∈ <dig>  to further select our so-called set of indicator prey proteins in control and bait experiments. we carried out our estimation of the numbers q^c and q^b of indicator prey proteins with reproducible experimental replicatesl^c and l^b and joint inclusion probabilities p˜jc and p˜jb respectively in control and bait experiments, each for a given marginal inclusion probability threshold p˜minc and p˜minb. we report the results for the control and ctnnbip <dig> bait experiments in table  <dig> and figure  <dig> in .

# selected indicator prey proteins
# reproducible experimental replicates
# selected indicator prey proteins
reproducible experimental replicates
text in italic represents the prohibited choices for the marginal inclusion probability thresholds p^jcp˜minc and p^jbp˜minb ), and bold texts represent the choices p˜minc= <dig>  and p˜minb= <dig> .

in keeping with objective criterion , the marginal inclusion probability threshold in the ctnnbip <dig> bait experiment was chosen to be p˜minb= <dig> , and likewise for the control experiment: p˜minc= <dig>  . for these marginal inclusion probability thresholds, results show that a total of q^c= <dig> and q^b=8indicator prey proteins could be identified out of a total of q^c= <dig> and q^b= <dig> uniquely identified indicator prey proteins in the control and ctnnbip <dig> bait experiments respectively. correspondingly, l^c= <dig> and l^b=5reproducible experimental replicates were uniquely identified in the control and bait experiment, in which the indicator prey proteins jointly appear with a joint inclusion probability of p^jc= <dig>  and p^jb= <dig>  respectively . for this combination of thresholds, we determined the subsets r1b,…,rrbb and r1c,…,rrcc of uniquely identified and most reproducible prey proteins that appear at least once in the sets of reproducible experimental replicates in the bait and control experiment respectively. the corresponding cardinal sets were rb= <dig> for the ctnnbip <dig> bait experiment and rc= <dig> for the control.

we followed a similar identification procedure in all other ap-ms bait experiments and report the results in tables  <dig> and  <dig> and additional file 2: figure s <dig> . the lists of indicator prey proteins and corresponding reproducible experimental replicates, found in all bait experiments, are provided in additional file 3: table s <dig> 

bait experiment with experimental 
# selected indicator prey proteins
# reproducible experimental replicates
results are reported for the marginal inclusion probability thresholds p˜minc and p˜minb as determined in each ap-ms bait experiment.

confidence score and specific prey proteins in bait experiments
we report here the results for the identification of specific prey proteins for instance in the ctnnbip <dig> bait experiment. this determination was made at the procedural stage “s”, which calls for specifying marginal inclusion probability thresholds p˜minb and p˜minc in both control and bait experiments as well as automatic estimation of the confidence score cutoff.

a first step in the selection of the set of specific prey proteins is to automatically estimate the confidence score cutoff from the data. to look for the confidence score cutoff achieving simultaneously the lowest estimated fdr and the largest go semantic similarity distance d> <dig> , we analyzed the fdr and bait-prey go semantic similarity as a function of the confidence score cutoff over the positive range c^scutoff∈ <dig> . <dig>  we noticed that this range was large enough to find the c^scutoff estimates in all our bait datasets tested . also, we computed approximate 95% confidence intervals of semantic similarity for the molecular function  ontology and the distance  of significance between these intervals as described in the methods section . as can be seen in the ctnnbip <dig> bait experiment, an estimated confidence score cutoff of c^scutoff= <dig>  simultaneously satisfies both objective criteria with a minimal fd^r≈0% and a positive go semantic similarity distance d^= <dig> > <dig>  higher c^scutoff values yield better specificity but lower sensitivity, and vice-versa . similar results were obtained for the other four bait experiments . our experience is that the fdr and inferences are relatively robust to the choice of the marginal inclusion probability threshold p˜minb as long as it is within the recommended boundaries ) and as long as the confidence score cutoff  is kept strictly positive. this was the case in all our bait experiments - see tables  <dig>   <dig> and  <dig> and additional file 2: figures s <dig> and s <dig> 

rocs performance on prey protein specificity and experimental variability
for the given combination of marginal inclusion probability thresholds and confidence score cutoff, we report the distribution of the bait-prey confidence scores from the initial “naïve” stage , the “reproducible” stage , and the final “specific” stage  in the bait experiments. note the accumulation of specific prey proteins with higher confidence scores as the method progresses through the procedural stages: .

we also looked at the distributions of marginal inclusion probabilities of all prey proteins in the bait versus control experiments through the procedural stages: from the initial “naïve” stage , to the “reproducible” stage , and to the final “specific” stage . observe the increase in quantiles in the bait compared to the control experiment as the method progresses through the procedural stages in all bait experiments . this corresponds to a clear separation of bait from control distributions and to the accumulations of bait and control marginal inclusion probabilities towards  <dig> and  <dig> respectively. overall, these plots pinpoint to an increased segregation of specific prey proteins vs. non-specific prey proteins by our rocs method as it proceeds through the procedural stages.

generally, we also observed in all bait experiments that the correlation between protein mascot identification scores with our measure of experimental reproducibility increases as the method progresses through the procedural stages: from the initial “naïve” stage , to the “reproducible” stage , and to the final “specific” stage  . this appears to be due to a reduction in the proportion of protein with the lowest reproducibility  and indicates a specific reduction of non-reproducible prey proteins as the method progresses through the procedural stages .

further, to check the reproducibility in all bait experiments, we compared the overall coefficient of variations  of the mean marginal inclusion probabilities of prey proteins across experimental replicates . this was carried from the initial “naïve” stage , to the “reproducible” stage , and to the final “specific” stage , i.e. with datasets restricted to the set of uniquely identified proteins from either  the entire dataset ,  the reproducible dataset ,  or the specific dataset . results are reported in additional file 2: figure s <dig> for all ap-ms bait experiments. the coefficient of variation in any bait experiment should not increase significantly  with the set of prey proteins  that is being used: whether one considers the entire prey protein space  vs. a subspace of it, such as the reproducible prey protein space , or the final specific prey protein space . indeed, notice the stability or the decrease in all cases that is achieved from procedural stage “n” onward  .

evaluation of rocs scoring in comparison to the literature
to test the validity of the rocs scoring in all bait experiments, the corresponding lists of specific prey proteins were matched against the biogrid references database  . the table gives the reciprocal matching of the two lists against each other, i.e. the matching of biogrid database references into the rocs list as well as the reciprocal matching of the rocs list into the biogrid references. altogether, results for all bait experiments are indicative that when a protein bait interaction has been characterized and published, it is largely confirmed by our rocs scoring .

to quantify the overlap and biological coherence between rocs specific prey proteins and those found in the literature , we first looked at the go semantic similarity measure between the two protein lists  of uniquely identified specific prey proteins being compared  for all gene ontologies , molecular function , and cellular component ), and in all bait experiments. this was done by computing resnik’s semantic similarity between the two protein lists ) according to wang’s algorithm  <cit> . results are reported in table  <dig>  they show a very strong to maximal  semantic similarity measure between the two lists for all bait experiments, indicating a strong biological coherence between rocs specific prey proteins and those found in the literature  [table  <dig> 

hypergeometric test rejection probabilities  are given as well as protein lists semantic similarities for each gene ontology , molecular function , and cellular component ). rocs lists are reported for c − score cs > Ĉscutoffand the marginal inclusion probability thresholds p˜minc and p˜minb as determined in each ap-ms experiment.

second, we determined the statistical significance of the overlap between the two lists of specific prey proteins . under the null hypothesis that the two lists are unrelated or that any intersection is due to chance alone , the random variable of the number of common  proteins between the two lists, denoted x, follows a hypergeometric distribution with parameters nb, sb, and b: x ∼ p, where p is given by px=x|nb,sb,b=sbxnb−sbb−x/nbb, and where the number of unique rocs specific prey proteins and biogrid proteins are denoted by sb and b respectively. the overlap analysis results with corresponding rejection probabilities  are reported in table  <dig>  they show statistical significance in all bait experiments at the α =  <dig>  significance level, meaning that we can reject the null hypothesis that the intersections found with the literature is due to chance alone, or that the rocs lists are drawn at random .

comparisons of rocs scoring to other scoring techniques
to further assess rocs's effectiveness as a stand-alone method for finding specific protein-protein interaction , we also compared its performance to similar methods such as saint <cit>  and compass <cit> . we compared rocs’s lists of specific prey proteins in all bait experiments with those obtained by saint and compass. these lists are given in additional file 5: table s <dig> [additional file 5: table s <dig>  they are ranked by decreasing significance of protein-protein interactions  according to each scoring method. additional file 5: table s <dig> also gives the reciprocal matching of saint and compass lists into the rocs list [additional file 5: table s <dig>  using a similar overlap analysis by means of the hypergeometric distribution test and go semantic similarity as above, we determined the significance of the overlap between  rocs and saint,  rocs and compass for all gene ontologies , molecular function , and cellular component ) and in all bait experiments. overlap analysis and go semantic similarity results in table  <dig> [table  <dig> show that rocs identified specific prey proteins compares very similarly to these methods and that the overlap with saint and compass is statistically significant in all bait experiments tested.

using a similar overlap analysis by means of the hypergeometric distribution test and go semantic similarity as in table  <dig>  we determined the statistical significance of the overlap between:  rocs vs. saint, and  rocs vs. compass for all gene ontologies , molecular function , and cellular component ) and in all bait experiments. saint and compass lists are reported for psaint >  <dig> and d − score >  <dig> respectively. rocs lists are reported for c − score cs > Ĉscutoffand the marginal inclusion probability thresholds p˜minc and p˜minb as determined in each ap-ms experiment.

effect of rocs improvement in combination with another scoring technique
next, we combined our method with an existing procedure for analysis of ap-ms data in the following way. although there are multiple methods for evaluation of ap-ms data , we use the probabilistic scoring approach saint developed by choi et al.  <cit>  since it allows for multiple replicated experiments and is scalable to ap-ms datasets of different size. in saint, we tested the datasets of all bait experiments at every procedural stage of our method, while keeping the same set of control experiments. saint works indeed with a small set of control experiments, which were chosen as the five most replicated ones from the initially pool of kc =  <dig> control experimental replicates . also, in the latest implementation of saint  we were able to use the mascot scores directly as quantitative input rather than spectral counts as used in earlier versions.

an objective way to assess the performance of our rocs method is to do a side-by-side comparison of saint protein-protein interaction scoring  between saint-only and saint applied in conjunction with rocs at different procedural stages, namely, the initial “naïve” stage , the “reproducible” stage , and especially the final “specific” stage . the comparison is given in additional file 6: table s <dig>  where we show the results of the pairwise resnik semantic similarity measure sim between a gene ontology  term from the bait protein  and for each specific prey protein  as described in the methods section. this was computed for all gene ontologies , molecular function , and cellular component ) and all bait experiments .

we next plotted the 95% confidence interval  of the median bait-prey semantic similarity as described in the methods section. the pairwise resnik semantic similarity measure sim is given between a gene ontology  term from the bait protein  and for each specific prey protein  as described in the methods section. this was computed for all gene ontologies , molecular function , and cellular component ). results show a statistically significant increase in the go bait-prey semantic similarity as rocs is applied in conjunction to saint, i.e. between saint-only , and rocs-saint  .

an alternative way to empirically assess the performance of our rocs method in any bait experiment is to compare the behavior of the false discovery rates  between rocs procedural stages, independently or in combination with another scoring method. here, we used the posterior probability psaint of true protein-protein interaction from the saint output to get 1 − psaint, which serves as the local false discovery rate  <cit>  . aggregate fdr up to the chosen rank is the integral of local fdr across the selection, or, in practice, the average of 1 − psaint across the selection. the latter is closer to what's called bayesian fdr, and conceptually to lfdr/fdr. results show that when rocs is used in conjunction with saint, one substantially reduces the fdr [figure  <dig> and additional file 22: figure s <dig> 

testing stability on multiscale sets of experimental replicates
to demonstrate the usability of our identification procedure, we tested its performance on several ap-ms bait datasets with varying numbers of experimental replicates: vhl , ctnnbip <dig> , nme <dig> , ppm1b , stk <dig> . table  <dig> and figure  <dig> report the results .

to test the stability of our method on a larger set of replicated ap-ms experiments, we make use of a set of  <dig> replicated control ap-ms experiments. figure  <dig> shows a stability plot for ap-ms datasets with different numbers of experimental replicates k ∈  for fixed b1 =  <dig>  b2 =  <dig>  k =  <dig> , an arbitrary α =  <dig> th-quantile of peptide probabilities, and an arbitrary marginal inclusion probability threshold of e.g. p˜min= <dig>  . as demonstrated by the stability diagnostic plot, unbiased “multiscale” joint inclusion probability estimates are very stable across a wide range of experimental scale values. only the unbiased multiscale unbiased joint inclusion probability estimate should be trusted. in fact, although regular  bootstrap estimates show a similar behavior over the range k^min∈ <dig> , there is a clear inflation of the joint inclusion probability estimate for lower experimental scale values . as expected, the stability of the unbiased estimate is lost for the smallest k values . the plot shows how to successfully identify the smallest scale values where the stability drops significantly. in our case, the sought-after k^min value, below which the identification procedure is not reliable anymore, was determined to be k^min∈ <dig>  .

CONCLUSIONS
the described method identifies and selects reproducible ap-ms experiments as well as bait specific preys when experimental replicates and control experiments are provided. the method is able to identify a subset of indicator prey proteins, which enables identification of the most reproducible experimental replicates from a larger dataset. importantly, we show that the method uniformly scales up and down, making it quite versatile to accommodate realistic studies with a range of numbers of experimental replicates. the identification of subsets of reproducible ap-ms experiments significantly improves the ability to distinguish specific from non-specific prey proteins. in the future, this approach may be used as a general selection method for quality control purposes in proteomics studies and proteomics databases, where experimental reproducibility issues arise.

we show that at least on larger ap-ms datasets, the frequency of occurrence of prey proteins provides good sensitivity for discriminating specific from non-specific prey proteins. in our study we use the prey protein frequency as a surrogate measure for more specific protein confidence metrics such as search-engine scores  <cit> , spectral counts  <cit>  or peptide and protein probabilities  <cit> . our approach may be most useful in ap-ms experimental designs incorporating sufficient number of replicates per bait protein, rather than in studies seeking to maximize the number of bait proteins analyzed at the expense of replicate ap-ms experiments.

we were also able to estimate the number of required replicate ap-ms experiments. from our analysis of stability on multiscale sets of experimental replicates , this number was determined to be at least greater than the k^min value , at least on the data in-hand. in practice, however, the required number would best be determined by a statistical power analysis. so, this approach is useful to large replicated experiments, and especially whenever one is interested in making new discovery, where large sample size  are always needed to increase statistical power and reduce false discovery rates. lastly, since frequency is a standardized metric, our method may be useful when attempting to compare different ap-ms datasets.

the concept of indicator proteins and the related reproducible experimental replicates rely on an objective measure of peptide reliability, which we chose to be the mascot peptide identification score  <cit> . however, clearly any other probability-based identification statistics such as e.g. the peptide probability from the peptide prophet <cit>  might be used. finally, the use of indicator proteins may be applied beyond ap-ms experiments to other types of mass-spectrometry based proteomics.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
j-ed and ss developed the statistical methodology and j-ed implemented the method as an r package. j-ed lead the statistical aspects of this paper, and rme lead the experimental and biological aspects. all authors formulated the problem, wrote and approved the manuscript.

supplementary material
additional file 1: supplemental methods
data set and database search. determination of protein spectral counts, protein mascot scores, and protein marginal inclusion probabilities. raw input dataset structure. initial pre-filtering. derivation of marginal and joint inclusion probabilities of indicator prey proteins. identification of reproducible experimental replicates and reproducible prey proteins. confidence score and identification of specific prey proteins. automatic estimation of an optimal confidence score cutoff derivation of coefficient of variations formulas. testing stability on multi-scale sets of experimental replicates.

click here for file

 additional file 2
figure s <dig>  scatter plots of protein spectral counts vs. protein mascot scores  and protein mascot scores vs. protein marginal inclusion probabilities  in all ap-ms control and bait experiments. figure s2: scatter plot of peptide prophet probabilities  onto the peptide mascot scores  in all ap-ms control and bait experiments. figure s3: empirical probability density function  and cumulative density function  plots of peptide scores  and peptide probabilities  in all ap-ms control and bait experiments. figure s4: optimizing the determination of the peptide mascot score threshold in all ap-ms bait experiments. figure s5: number of indicator prey proteins and reproducible experimental replicates and the joint inclusion probability for the protein-based analysis in all ap-ms bait experiments. figure s6: fdr sensitivity as a function of confidence score cutoff and marginal inclusion probability threshold in all ap-ms bait experiments. figure s7: fdr and go semantic similarity analyses in all ap-ms bait experiments. figure s8: density distribution plots of bait-prey confidence scores at procedural stages “n”, “r” and “s” in all ap-ms bait experiments. figure s9: quantile-quantile plots of bait vs. control marginal inclusion probabilities in all ap-ms bait experiments. figure s10: correlation and regression relationships between protein mascot scores and protein marginal inclusion probabilities at different procedural stages from the initial “naïve” stage , to the “reproducible” stage , and to the final “specific” stage  in all ap-ms bait experiments. figure s11: stability of the coefficient of variation  of the mean marginal inclusion probability as a function of procedural stages from the initial “naïve” stage , to the “reproducible” stage , and to the final “specific” stage  in all ap-ms bait experiments. figure s12: confidence intervals  of the median bait-prey semantic similarity for saint-only and saint in conjunction with rocs at the different rocs procedural stages “n” , “r”, and “s”  in all ap-ms bait experiments. figure s13: fdr computed from saint posterior probability output  as a function of procedural stages: from the initial “naïve” stage , to the “reproducible” stage , and to the final “specific” stage  in all ap-ms bait experiments  <cit> .

click here for file

 additional file 3: table s1
rocs lists of indicator prey proteins  and reproducible experimental replicates  in all ap-ms control and bait experiments .

click here for file

 additional file 4: table s2
biological validation of rocs protein-protein interaction  scoring results for the specific prey proteins between saint , compass  and our method rocs  in all ap-ms bait experiments .

click here for file

 additional file 5: table s3
comparison of protein-protein interaction  scoring for the specific prey proteins between saint , compass  and our method rocs  in all ap-ms bait experiments .

click here for file

 additional file 6: table s4
comparison of saint protein-protein interaction  scoring for the specific prey proteins at the different rocs procedural stages “n” , “r”, and “s”  in all ap-ms bait experiments .

click here for file

 acknowledgements
the authors thank dr. mathieu lavallée-adam and dr. hyungwon choi for scientific discussion and for providing the latest implementations of decontaminator and saint softwares. this work was supported in part by laboratory start-up funds from the cleveland foundation and the center of proteomics and bioinformatics to rme and j-ed.
