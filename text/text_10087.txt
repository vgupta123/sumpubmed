BACKGROUND
recent advances in experimental methods have enabled the development of functional genomics, a genome-wide approach to understanding the inner workings of a cell. while such large-scale approaches will undoubtedly be instrumental in extending our knowledge of molecular and cellular biology, they produce enormous amounts of heterogeneous data of varying relevance and reliability. a key challenge in interpreting these data is separating accurate, functionally relevant information from noise.

here we focus on using noisy genomic datasets to associate uncharacterized genes or proteins with biological processes. recent literature on protein function prediction focuses on integrating multiple sources of evidence  to assign proteins to processes  <cit>  or to predict functional associations or interactions between related proteins  <cit> . individual high-throughput datasets are typically noisy, but effective integration can yield precise predictions without sacrificing valuable information in the data. all of these methods require a gold standard, which is a trusted representation of the functional information one might hope to discover. such a standard, coupled with an effective means of evaluation, can be used to assess the performance of a method and serves as a basis for comparison with existing approaches. beyond methods for predicting protein function or interactions, evaluation against gold standards can be used to directly measure the quality of a single genomic dataset, a necessary step in developing and validating new experimental technology.

we have undertaken a study of proposed standards and approaches to evaluation of functional genomic data and highlight a number of important issues. we find that current approaches are inconsistent, making reported results incomparable, and often biased in such a way that the resulting evaluation cannot be trusted even in a qualitative sense. one specific problem we identify is substantial functional biases in typical gold standard datasets. we demonstrate this problem by evaluating several functional genomic datasets using the kyoto encyclopedia of genes and genomes  <cit>  as a gold standard , as is commonly employed in the literature . a na√Øve evaluation in this manner identifies co-expression data as by far the most sensitive and specific genome-scale functional genomic data type . however, this apparent superior performance is due to characteristics of a single pathway; when the ribosome  is removed from the gold standard, co-expression becomes one of the least informative datasets . in addition to such substantial functional biases, we find that commonly used gold standards are highly inconsistent even for comparative evaluations and that most current evaluation methodologies yield misleading estimates of accuracy.

in this paper, we describe these problems with current evaluation standards with the hope of instigating a community dialog on proper approaches to comparing genomic data and methods. as noted above, there are two typical approaches to using genomic data for analyzing protein function: methods that directly associate proteins with particular processes or functional classes, and methods that focus on predicting functional associations or interactions between pairs of proteins. we focus our attention toward standards for the latter, evaluating pairwise associations between genes produced by either experimental or computational techniques. many of the problems we describe, however, apply to both approaches, and we suggest an alternative standard for evaluation that is appropriate in both settings. we provide both a trusted set of functional associations between proteins as well as a specific set of biological processes that maps proteins to well-defined functional classes. both standards are based on curation by a panel of biological experts. furthermore, we propose several guidelines for using these standards to perform accurate evaluation of methods and data. the resulting evaluation framework can be used to directly measure and compare the functionally relevant information present in raw high-throughput datasets as well as to evaluate or train computational genomics methods.

our gold standard and evaluation methodology have been implemented in a web-based system  <cit>  to facilitate community use for comparison among published datasets or methods. we demonstrate the use of our approach on genomic data from saccharomyces cerevisiae. accurate evaluation methods are particularly critical for this model organism, because yeast is widely used as a platform for the development of both high-throughput experimental techniques and computational methods. however, the weaknesses we identify in existing evaluation methodologies as well as the solution we propose are applicable to data from other model organisms and humans.

RESULTS
we first discuss commonly used gold standards and several fundamental issues with current approaches to evaluation of functional genomic data and methods. to address these problems, we propose a new gold standard based on expert curation and recommend appropriate uses of the standard that ensure accurate evaluation. finally, we describe a web-based implementation of our evaluation framework, which is available for public use by computational and experimental biologists.

challenges to effective functional evaluation
existing gold standards
a number of different gold standards for evaluating yeast functional genomic data or methods have been proposed in the literature. each standard generally consists of sets of gene or protein pairs grouped as either "positive" or "negative" examples. this is due in large part to the fact that some high throughput data takes the form of associations between genes or gene products . furthermore, a pairwise approach to analysis is a natural way to view biological systems, which are composed of networks, or groups of interactions between gene products. although this is a commonly adopted approach, others have trained classifiers for specific functional classes where individual proteins or genes are directly associated with functional classes or processes  <cit> . while we focus on data and methods for pairwise associations between proteins here, many of the issues described are equally problematic for such non-pairwise approaches, and we propose an alternative gold standard appropriate for both settings .

most functional genomics evaluations derive gold standard positives from functional classification schemes that capture associations of genes or proteins with specific biological processes as reported in the literature  <cit> . such classifications are available from multiple sources including the gene ontology  <cit>   <cit> , kegg  <cit> , the munich information center for protein sequences   <cit> , and the yeast protein database   <cit> . a common source of gold standard negatives is cellular localization data  <cit> . most of these methods utilize a localization study in which 75% of the yeast proteome was gfp-tagged and classified into  <dig> different cellular compartments  <cit>  and they assume that two proteins localizing to distinct compartments do not interact. random pairs of proteins sampled from the proteome provide another common gold-standard negative, relying on the assumption that the expected number of functionally related or interacting pairs is much less than the total number of possible pairwise protein-protein combinations  <cit> .

inconsistencies among and within different standards
perhaps the most apparent issue with functional genomic evaluation arises from the diversity of possible standards and lack of agreement among them. it has been noted that gold standard positive pairs derived from kegg, mips, and go biological process ontology show little overlap  <cit> . we find even less agreement among gold standards for physical interactions predictions, which are usually based on small interaction datasets obtained from protein-protein interaction databases such as the database of interacting proteins  <cit> , the general repository for interaction datasets  <cit> , or the biomolecular interaction network database   <cit> . however, the more alarming problem is that even the relative performance of methods or datasets evaluated against these standards is not consistent. for example, using both the biological process go and the kegg pathways gold standard to evaluate the relative performance of commonly used data sets produces strikingly different results . this difference is likely due to the nature of the biological relationships each standard is trying to capture or simply variation in which specific proteins are present in the classification scheme or interaction dataset. although each standard is correctly evaluating some aspect of the data, without a common, representative evaluation framework, the community cannot assess the relative performance of novel methods or high-throughput techniques.

in addition to substantial inconsistencies among existing gold standards, variation in biological specificity within each standard has also impaired previous evaluation methods. standards based on biological ontologies  classify proteins at a broad range of resolutions . although these ontologies can provide a powerful framework for defining a gold standard, there are a few caveats. a typical approach for using go has been to pick a particular depth in the hierarchy below which term co-annotations imply gold standard positives. however, terms at the same level can vary dramatically in biological specificity  <cit>  . for example, at a depth of  <dig> in the biological process go, the term "regulation of sister chromatid cohesion"  with a single indirect gene product annotation appears alongside a much more general term "cellular protein metabolism" , which has  <dig> annotations. widely varying degrees of specificity in a gold standard not only complicate evaluation methods but can also appear as inconsistencies in the data when training machine learning algorithms, which can result in poor performance.

functional biases in prediction performance
the majority of current evaluation approaches are performed without regard to which biological processes are represented in the set of true positives , and thus they are often unknowingly skewed toward particular processes. we illustrate this bias with an example using the kegg pathways gold standard to evaluate genomic data . in this evaluation, the estimated reliability of microarray co-expression drops dramatically when a single pathway  is excluded from the analysis. the substantial drop in precision suggests that a large fraction of the true positives predicted by co-expression are exclusively ribosome relationships. in fact, of the positive examples in the 1% most co-expressed pairs, 86%  are due to co-annotation to the ribosome pathway. this bias becomes even more pronounced at higher co-expression level cutoffs: of the  <dig> % most co-expressed positive pairs, 99%  are from the ribosome pathway. we find a similar bias in evaluations using the go and mips gold standards.

thus, the traditional approach of using a general roc curve  without regard to which processes are represented can be misleading . this is particularly true when the data or computational predictions have process-dependent reliability as is often the case with genomic or proteomic data. the problem is magnified when the gold standard examples themselves are heavily skewed towards specific functional categories. while the general precision-recall characteristics such as those portrayed in figure  <dig> are technically correct, they generalize poorly to non-ribosomal protein relationships. thus, such an evaluation would be misleading for a scientist hoping to use these data to generate new hypotheses about proteins unrelated to the ribosome. we address this problem in our process-specific evaluation framework.

gold standard negatives
another shortcoming of current standards for gene/protein function prediction is the nature of the gold standard negative examples. in yeast, one proposed source of gold standard negatives is based on protein localization data  <cit>  because pairs of proteins localizing to different cellular compartments are highly enriched for non-interacting proteins. however, localization data is likely not representative of "typical" unrelated protein pairs. for instance, ben-hur and noble found the performance of svm classifiers trained with localization negatives artificially inflated because this negative set is composed entirely of high-confidence pairs  <cit> . using such a non-representative "easy" set of negatives will overestimate prediction accuracy, and the resulting classifier will generalize poorly to real biological problems.

thus, although protein localization data is a strong negative indicator of functional relationships or interactions, we caution against its use as a general negative gold standard. this is particularly problematic for higher-level questions such as function prediction, because proteins co-involved in some biological processes span cellular compartments. perhaps a safer role for localization data is as the input to computational approaches. we suggest an alternative negative standard based on the biological process gene ontology that can provide representative negative examples .

relative size of gold standard positive/negative sets
a final issue common among many evaluation standards in the literature is the relative size of the positive and negative example sets. the expected number of proteins involved in any particular biological process is a small percentage of the proteome, which should be reflected in evaluation standards. this imbalance is particularly problematic in methods based on pairwise associations between proteins, where the expected number of protein pairs sharing functional relationships is an even smaller fraction of all possible protein combinations. for instance, of the  <dig> million possible protein pairs in yeast, it is expected that less than  <dig> million are functionally related. this large difference makes the typical reporting of sensitivity and specificity misleading. for instance, a recently published method for predicting protein-protein interactions from several genomic features showed seemingly impressive 90% sensitivity and 63% specificity in evaluations  <cit> , but would make correct predictions only  <dig> out of every  <dig> times when applied on a whole-genome scale, rendering the method impractical in many experimental contexts .

given this imbalance, an appropriate measure of functional relevance of genomic data or predictions is the precision or positive predictive value 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadiqaamaalaaabagaemivaqlaemiuaafabagaemivaqlaemiuaalaey4kasiaemoraykaemiuaafaaagaayjkaiaawmcaaaaa@361b@ <cit> . this measure rewards methods that generate firm positive predictions, without regard to the accuracy of negative predictions, which are less helpful in guiding laboratory experiments. direct application of precision may be misleading, though, because this measure is only correct under the assumption that the ratio of positive to negative examples in the gold standard matches that in the application domain. if the ratio of positive to negatives in the gold standard is much larger than in whole-genome data, as is often the case in published evaluations, then the number of false positive predictions will be small and will artificially inflate the precision statistic. for instance, the 90%-63% sensitivity-specificity example above used an approximately equal number of positive and negative examples , leading to 65% precision. however, application of this method on a whole-genome scale, where the ratio of positive to negative examples is roughly  <dig> times smaller, would lead to an expected precision of just 11% .

to avoid such misleading evaluations, the balance of positives and negatives in the gold standard should match that of the application domain as closely as possible. precision, or ppv, then becomes a direct, representative measure of how well one could expect a dataset or method to perform on whole-genome tasks. of course, precision alone does not convey all of the important information, only the quality of the predictions made by a dataset or method. it must be reported in tandem with some measure of the quantity of true predictions made. a standard measure for this is the recall, or sensitivity 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadiqaamaalaaabagaemivaqlaemiuaafabagaemivaqlaemiuaalaey4kasiaemoraykaemota4eaaagaayjkaiaawmcaaaaa@3617@, which is what is used in our evaluation framework .

suggestions for representative functional evaluation of data and methods
in light of these problems with current gold standards and approaches to evaluation, we have compiled a new functional genomics gold standard and suggest several strategies for accurate comparative evaluation of genomic datasets and methods.

defining a new gold standard
as discussed previously, a major issue with the current state of the community is inconsistency among the variety of standards used. evaluations based on different standards  are often not comparable, even in a qualitative sense. deriving a standard from these hierarchies is further complicated due to varying levels of biological specificity of curated biological knowledge. furthermore, each of the sources of curated information has inherent functional biases that can lead to incorrect estimates of accuracy.

to develop a unified standard for general application in functional genomics, several key criteria must be met. the standard must be cross-organismal to ensure relevance to a broad audience. secondly, the standard should cover a wide variety of biological functions or processes to facilitate comprehensive evaluations. finally, the standard should adapt quickly as biological knowledge expands. although there are several sources of annotation that satisfy these criteria to varying extents , go is arguably the best option to serve as a foundation for the standard, as it is well-curated and was designed for complete coverage.

although go can serve as a good basis for a functional gold standard, effective mapping from organism-specific annotations to a set of positive and negative examples is critical. in particular, we have addressed the problem of varying levels of resolution in the go hierarchy by selecting the gold standard set of terms through curation by six expert biologists. through this formal curation process, the experts selected terms that are specific enough to be confirmed or refuted through laboratory experiments while also general enough to reasonably expect high-throughput assays to provide relevant information . the result of this process is a set of specific functional classes  which can be used to generate an accurate set of positively related gene pairs or to directly evaluate or train computational approaches that explicitly associate proteins with particular biological processes. this standard created using expert knowledge is quite different from go standards commonly used in the literature . it can serve as a single, common standard that addresses the specific concerns of functional genomics.

this curation can also be used to obtain a negative standard which addresses some issues with currently used methods. specifically, our standard includes a set of negatives more broadly representative than sources such as localization while excluding likely positive examples . further, the standard approximates the correct relative balance of positive and negative sets enabling biologically relevant evaluations .

evaluating genomic methods and data
in addition to defining a unifying standard, it is critical to use the standard in a manner that accurately reflects the biological reliability of datasets or methods. to expressly address the process-specific variability in accuracy, we developed an evaluation framework that facilitates identification of functional biases in current general evaluations. to accomplish this, we propose that two complementary modes of analysis accompany any evaluation of functional genomic data:  a genome-wide evaluation that estimates general reliability but also reports the functional composition of the results and  a process-specific evaluation in which the data or method is independently evaluated against a set of expert-selected processes.

genome-wide evaluation
to provide a genome-wide analysis that also features information on the constituent biological processes, we have developed a hybrid evaluation framework that combines traditional measures of the precision-recall tradeoff with an analysis of the biological processes accurately represented in the data. in addition to the usual estimation of precision-recall characteristics, we compute the distribution of biological processes represented in the set of correctly classified positives  at every point along the precision-recall tradeoff curve . this distribution allows one to identify and measure any biases in the set of positive results toward a specific biological process and interpret evaluation results accordingly. furthermore, all of this information is summarized and presented in a dynamic and interactive visualization framework that facilitates quick but complete understanding of the underlying biological information.

in addition to identifying biases in genome-wide evaluations of datasets or methods, our evaluation framework provides a way to normalize these biases out of the analysis. a user can choose to exclude all positive examples related to one or more biological processes. figure 5b illustrates an example of this functionality for the evaluation discussed above. based on the bias we observed, we excluded all proteins involved in ribosome biogenesis and assembly  and re-evaluated the same set of datasets. while none of the interaction datasets change significantly with this process excluded, both gene expression datasets show substantial decay in their precision-recall characteristics, suggesting they are generally less reliable at predicting functional relationships over a broad range of processes. this result is quite different from what we might have concluded had we not been able to discover and correct this process-specific bias.

process-specific evaluation
many biological laboratories focus on specific processes or domains of interest, even when using high throughput data/methods. in such situations, a targeted, process-specific evaluation is often more appropriate than a genome-wide evaluation. our framework facilitates convenient and representative process-specific evaluations by performing independent precision-recall analysis for each process of interest.

for effective presentation of process-specific evaluation results, we have developed an interactive matrix-based view that facilitates comparative evaluation of multiple datasets across several targeted biological processes . this method allows for easy and dynamic inter-process and inter-dataset comparisons. in addition, precision-recall characteristics for any process are readily accessible, allowing for a more detailed view of the results. thus, our framework combines general and specific evaluations, enabling accurate interpretation of functional genomics data and computational methods. this community standard can facilitate the comparisons necessary for formulating relevant biological hypotheses and determining the most appropriate dataset or method for directing further experiments.

CONCLUSIONS
we have identified a number of serious issues with current evaluation practices in functional genomics. these problems make it practically impossible to compare computational methods or large-scale datasets and also result in conclusions or methods that generalize poorly in most biological applications. we have developed an expert-curated functional genomics standard and a methodological framework that address the problems we have identified. we hope these can serve as an alternative to current evaluation methods and will facilitate accurate and representative evaluation. furthermore, we hope our analysis will initiate a broader community discussion about appropriate evaluation techniques and practices.

in recent years, the computational community has played an influential role in the field of genomics by contributing many valuable computational methods that facilitate discovery of biological information from high-throughput data. however, without an accurate understanding of how well the computational methods perform, the role of bioinformatics in directing experimental biology will remain limited. lack of accurate assessment of the experimental methods themselves hinders both interpretation of the results and further development of genomic techniques. thus, representative evaluation of computational approaches and high throughput experimental technologies is imperative to our ability as a community to harness the full potential of biological data in the post-genome era.

