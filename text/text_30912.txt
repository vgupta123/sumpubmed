BACKGROUND
helical membrane proteins  play a crucial role in diverse cellular processes, including energy generation, signal transduction, the transport of solutes across the membrane, and the maintenance of ionic and proton concentrations. several studies have suggested that hmps account for  <dig> – 30% of the open reading frames of sequenced genomes  <cit> . in spite of their biological importance and genomic abundance, less than 1% of the proteins with known structure are hmps  <cit> , and this situation is not expected to improve dramatically in the near future. hence, it is desirable to develop sequence-based computational methods for predicting structural characteristics of hmps. in the realm of soluble proteins, two particular structural characteristics have been the main target of computational prediction methods: secondary structure  <cit>  and solvent accessibility  <cit>  . for hmps, the prediction of secondary structures does not carry as significant a momentum as for soluble proteins because transmembrane  segments, which can be relatively reliably identified from the sequence by several techniques  <cit> , are known to usually adopt helical conformations to satisfy the hydrogen bonding capacity of the backbone polar atoms. on the other hand, the problem of predicting the burial status  of tm residues of hmps has remained nearly untouched until now, in contrast to the situation for soluble proteins, which have been extensively studied  following the pioneering work of rost and sander  <cit> . this is quite "remarkable" given that it is much more difficult to determine the structures of hmps than those of soluble proteins by experimental techniques. the virtue of the ability to predict the burial status of tm residues of hmps was already appreciated by several studies around the early 90s  <cit> . the burial status prediction should be useful in several tasks. one simple example would be to help design mutational experiments aimed at identifying catalytically important tm residues  <cit>  by providing a list of tm residues highly likely to be buried in the protein core because catalytically important tm residues are usually found buried in the protein core, not being exposed to the membrane. another simple example would be to help design mutational experiments aimed at identifying tm residues important for protein-protein interactions in the membrane by providing a list of tm residues highly likely to be exposed to the membrane.

in  <dig>  beuming and weinstein pioneered the first sequence-based computational method for predicting the burial status of tm residues of hmps , which was based on sequence conservation patterns and a newly derived knowledge-based propensity scale of the  <dig> amino acids to be exposed to the membrane  <cit> . for a rather small benchmark set, the bw method achieved an impressive prediction accuracy of 80%. recently, adamian and liang reported the development of a similar method  <cit> , but it predicts the face of a tm helix exposed to the membrane, not the burial status of individual tm residues. hildebrand and his coworkers described a computational method for predicting whether a given residue is located at a helix-helix interface in the membrane  <cit> . yet, this is a distinct prediction problem from the one the current study deals with: a residue located outside of a helix-helix interface can still be buried. quite recently, yuan and his coworkers developed a method for predicting the relative solvent-accessible surface area  of tm residues based on support vector regression   <cit> . even though the yu method does not explicitly predict the burial status of tm residues, it is possible to do so using the predicted rsasa values. to our best knowledge, the bw and yu methods are the only ones currently available for predicting the burial status of tm residues of hmps.

we have developed tmx , a novel sequence-based computational method for predicting the burial status of tm residues of hmps. its accuracy is  <dig> % over a much larger data set of  <dig> tm residues, representing a considerable improvement over  <dig> % of the bw method when evaluated on the same data set. this prediction accuracy is also higher than  <dig> % of the yu method. importantly, unlike the bw and yu methods, tmx automatically yields confidence scores for the predictions made, a highly desirable component for any computational prediction method, which allows the user to selectively utilize prediction results depending on confidence scores in real application situations. in addition, a feature selection incorporated in tmx reveals interesting insights into the structural organization of hmps.

RESULTS
analysis of the bw method
tmx is novel in several aspects compared to the bw and yu methods and can be described without any reference to these previous methods. however, we prefer to describe the logic behind its development in reference to the bw method in order to contrast it with the bw method and highlight its novelties.

for predicting the burial status of a tm residue, the bw method computes its positional score and compares the score with a threshold  <cit> . if the score is higher than the threshold, it is predicted to be buried. otherwise, it is predicted to be exposed to the membrane. formally, the bw method computes a positional score for sequence position i, s, as  <dig> × - pbw), where c is the conservation index for sequence position i, and pbw the propensity of sequence position i for being exposed to the membrane, which is in turn derived from the bw scale as shown in eq.  <dig>  the bw scale is derived from a set of hmps with known structure.

 pbw=∑j=120bw×fi
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgqbaudawgaawcbagaemoqaikaem4vacfabeaakiabcicaoiabdmgapjabcmcapiabg2da9maaqahabagaemoqaikaem4vaclaeiikagiaemoaaomaeiykakiaey41aqraemozay2aasbaasqaaiabdmgapbqabagccqggoaakcqwgqbgacqggpaqkasqaaiabdqgaqjabg2da9iabigdaxaqaaiabikdayiabicdawaqdcqghris5aaaa@4936@ 

in eq.  <dig>  the index j runs over the  <dig> naturally occurring amino acids, bw is the propensity value of amino acid j in the bw scale, and fi the frequency of amino acid j in sequence position i. plugging eq.  <dig> into  <dig> × - pbw), the overall approach of the bw method for deriving a positional score can be cast as follows.

 s= <dig> ×c−∑j= <dig> ×bw×fi
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgtbwucqggoaakcqwgpbqacqggpaqkcqgh9aqpcqaiwaamcqgguaglcqai1aqncqghxdatcqwgdbwqcqggoaakcqwgpbqacqggpaqkcqghsisldaaewbqaaiabicdawiabc6cauiabiwda1iabgena0kabdkeacjabdefaxjabcicaoiabdqgaqjabcmcapiabgena0kabdagamnaabaaaleaacqwgpbqaaeqaaogaeiikagiaemoaaomaeiykakcaleaacqwgqbgacqgh9aqpcqaixaqmaeaacqaiyagmcqaiwaama0gaeyyeiuoaaaa@558d@ 

eq.  <dig> indicates that s is a linear combination of the conservation index and the  <dig> elements of the profile. thus, it can be written more generally as follows.

 s=cc×c+∑j=120cj×fi,
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgtbwucqggoaakcqwgpbqacqggpaqkcqgh9aqpcqwgdbwqdawgaawcbagaem4yamgabeaakiabgena0kabdoeadjabcicaoiabdmgapjabcmcapiabgucarmaaqahabagaem4qam0aasbaasqaaiabdqgaqbqabagccqghxdatcqwgmbgzdawgaawcbagaemyaakgabeaakiabcicaoiabdqgaqjabcmcapawcbagaemoaaomaeyypa0jaegymaedabagaegomaijaegimaadaniabgghildgccqggsaalaaa@4ea4@ 

where cc is the coefficient for the conservation index  and cj the coefficient for the jth element of the profile  in the bw method). with achieving highest possible prediction accuracies in mind, we raise the question of whether setting the coefficients in eq.  <dig> empirically as in the bw method is optimal or not. our answer is no. optimizing the coefficients would be a better idea. confirming this expectation, the coefficients optimized by linear regression led to a prediction accuracy of  <dig> %, compared to  <dig> % of the bw method as shown in table  <dig>  specifically, ridge linear regression with the complexity parameter set to  <dig>  was used throughout this study in an effort to minimize generalization errors  <cit> . it is noteworthy that we use the same formula as the bw method – eq.  <dig> – but with an entirely different philosophy. in the bw method, one first derives a propensity scale of the  <dig> amino acids to be exposed to the membrane from known hmp structures and then uses it for computing the propensity of a target residue to be exposed . this propensity of the target residue is combined with its degree of conservation to yield its positional score. our analysis reveals that this overall idea of the bw method can be concisely summarized by eq.  <dig>  which immediately suggests that there is a better way of doing the job.

1defined as the fraction of the tm residues in the data set whose burial status was correctly predicted.

2best prediction accuracy among  <dig> ones shown in table  <dig> 

there is an issue to be clarified before we move on. we implemented the bw method, and its performance was evaluated on the same data set as for tmx. this was necessary since it is often difficult to directly compare performance values of different prediction methods reported in different studies because of the variety of data sets used and the discrepancy in state definitions. a serious difficulty arose in implementing the bw method, namely setting thresholds manually. as mentioned above, upon computing the positional score of a target residue, the bw method compares it with a threshold that has been manually set. if the positional score is greater than the threshold, it is predicted to be buried. otherwise, it is predicted to be exposed. in a leave-one-out  testing scheme, thresholds need to be manually set separately for each of  <dig> protein chains in the benchmark data set . admittedly, it is impossible for us to exactly reproduce this step in the way it was performed in the original publication for the bw method  <cit> . in addition, we feel that it might be subjective to set thresholds manually. then, is there any mathematical formalism that allows thresholds to be set in such a manner that  we exactly mimic the manual setting of thresholds as was done in the bw method and  yet, thresholds are set objectively and reproducibly? our answer is a linear support vector classifier . since the hyperplane – f = β <dig> + βtx =  <dig>  where βt is the transpose of a column vector β – obtained by an lsvc in a one-dimensional space represents a scalar value of -β/β <dig>  <cit> , setting a threshold via an lsvc is an exact computational analogue to setting it manually, yet in an objective, reproducible way. it is to be noted that the introduction of an lsvc to the prediction scheme transforms it to a two-step scheme because an lsvc also needs training and, as a result, the jack-knife scheme should be applied to both steps. we want to stress that the sole purpose of using an lsvc here is to mimic the manual assignment of thresholds as exactly as possible yet in an objective, reproducible fashion. thus, we intentionally did not seek svcs with a non-linear kernel or other sophisticated classifiers at this stage .

improved use of conservation indices
another point well worth considering in eq.  <dig> is how conservation indices are incorporated. the average identities of sequences retrieved from sequence databases for different query sequences can be appreciably varying. thus, without normalization, one may assign overall high conservation indices to one protein chain while assigning overall low conservation indices to another. normalization of conservation indices effectively solves this bias problem, just as in microarray data processing. in the bw method, conservation indices are not normalized. we found that normalizing conservation indices by subtracting the mean followed by division by the standard deviation separately for each protein chain leads to a significant improvement in the prediction accuracy, raising it from  <dig> % to  <dig> %.

a second, minor aspect to be considered is how conservation indices are actually computed in the first place. the bw method computes conservation indices as follows.

 c =  <dig>  × v +  <dig>  × ic 

in eq.  <dig>  v is the volume of the polytope for sequence position i derived from a multiple sequence alignment , estimating the probability for the presence of a set of different amino acids from a set of pairwise distribution probabilities, and ic is the information content of sequence position i  <cit> . eq.  <dig> relies on many assumptions that are yet to be validated. the first are ad hoc measures taken to enforce the euclidean space to the distances between aligned sequences for computing v  <cit> . the second is the assumption used in computing ic that the  <dig> naturally occurring amino acids are equally likely to occur in the tm region. the third is that even though it seems reasonable to assign equal weights to both terms in eq.  <dig>  it is not clear whether that choice is optimal.

as in our previous studies  <cit> , we derived conservation indices using eq.  <dig>  which is mathematically well-defined and relatively free from potentially problematic assumptions.

 c=∑j−f)2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgdbwqcqggoaakcqwgpbqacqggpaqkcqgh9aqpdagcaaqaamaaqababagaeiikagiaemozay2aasbaasqaaiabdmgapbqabagccqggoaakcqwgqbgacqggpaqkcqghsislcqwgmbgzcqggoaakcqwgqbgacqggpaqkcqggpaqkdaahaawcbeqaaiabikdayaaaaeaacqwgqbgaaeqaniabgghildaaleqaaaaa@4335@ 

in eq.  <dig>  the index j runs over the  <dig> naturally occurring amino acids, c is the conservation index for sequence position i, fi is the frequency of amino acid j in sequence position i, and f is the overall frequency of amino acid j in the alignment. as expected, the use of eq.  <dig> instead of eq.  <dig> improved the prediction accuracy from  <dig> % to  <dig> %. it is to be noted that conservation indices obtained by eqs.  <dig> and  <dig> were from the same msas.

extending the window size for the input vector
at this stage, the input vector for the prediction method consists of  <dig> elements . another measure that we can take to further improve the prediction accuracy is to additionally consider the neighboring residues of the target residue . in fact, nearly all techniques developed for water-soluble proteins exploit this possibility  <cit> . we explored all symmetric window sizes . there are a couple of points to be noted in table  <dig>  when increasing the window size from  <dig> to  <dig> or  <dig>  the prediction accuracy is decreased, suggesting that the signal-to-noise ratio deteriorated . the first peak in the prediction accuracy is observed at a window size of  <dig>  it is interesting to note that, assuming the canonical helix conformation, when the length of a helix gets to  <dig>  the first and last residues  face in the same direction as the central residue . thus, our results suggest that the identities of the residues lying just above and below the target residue on the same helix face are most indicative of the burial status of the target residue, as expected from the canonical helix conformation. as it is actually  <dig>  residues per turn in the canonical helix conformation, a certain improvement is already found by including the positions i ±  <dig>  consistent with this line of reasoning, the best prediction accuracy,  <dig> %, is observed at a window size of  <dig>  based on a similar observation, adamian and liang recently developed a highly effective method for predicting membrane-exposed faces of tm helices  <cit> .

feature selection
the logic behind increasing window sizes for better predictions is that one can better account for long-range effects with enlarged windows. however, the shortcoming of enlarged windows is that the signal-to-noise ratio deteriorates as the window size is increased, as demonstrated in table  <dig>  for example, compare the prediction accuracies for window sizes of  <dig> and  <dig>  the tradeoff between long-range effects and signal-to-noise ratios would suggest a window size of  <dig> instead of  <dig>  is there any way of circumventing this unpleasant tradeoff? feature selection might be an answer. a simple illustration will make this point clearer. an input vector for a window size of  <dig> consists of  <dig> elements . it is intuitively clear that not all  <dig> elements will contribute equally to the prediction. many of them might simply be noise. thus, it might be possible to use enlarged windows for a better consideration of long-range effects and still maintain a high signal-to-noise ratio by filtering out noisy elements.

of many techniques available for feature selection, we chose the fisher's index for the following reasons. first, the fisher's index is conceptually attractive, having a clear meaning easy to understand  <cit> . put simply, the fisher's index represents the ability of a given element to maximize the distance between the centroids of the two given classes and simultaneously minimize the overlap between them. second, unlike techniques involving linear combinations of feature vectors, the fisher's index is highly interpretable. this is a big advantage given the high dimensions of our feature spaces. most importantly, one can gain interesting biological insights into the structural organization of hmps from the fisher's index . third, the fisher's index can be computed cheaply. fourth, the fisher's index is well suited to continuous features .

the  <dig> elements of a window of size  <dig> were ranked according to their fisher's indices, and increasing fractions of them  were input to the prediction . the best prediction accuracy,  <dig> %, was obtained when using the top 20% elements only. this accuracy is higher than  <dig> % obtained by an "unintelligently" increased window of size  <dig> in the above section. which elements rank top? as shown in table  <dig>  the top-ranking elements are mostly conservation indices, in line with previous findings that conservation properties of tm residues correlate strongly with their degree of exposure to the membrane  <cit> . also, table  <dig> shows that the frequencies of occurrence of l, i, v and f at the target residue are highly indicative of its burial status. in this regard, it is interesting to note that our previous study showed that these amino acids possess the highest propensities to preferentially interact with the membrane  <cit> . the frequency of occurrence of g at the target residue is also strongly correlating with its burial status, ranking at the 9th place, which is consistent with earlier findings that glycine residues play a pivotal role in mediating helix-helix interactions in the membrane  <cit> . table  <dig> also shows that the frequencies of occurrence of i, g and l at the 4th residue n terminal to the target one also strongly correlate with the burial status of the target residue, which makes sense considering the canonical helix conformation as mentioned above.

given dramatic improvements in prediction accuracy and interesting insights into the system under investigation through a feature selection as demonstrated here, it was quite surprising to find that almost all studies on predicting the solvent accessibility of water-soluble proteins  <cit>  have not considered it. hence, it would be worthwhile to investigate whether feature selection can similarly pay off in predicting the solvent-accessibility of water-soluble proteins.

1regularization constant c was set to  <dig> 

2meaning that the top 5% of the  <dig> elements when ranked by the fisher's index were input for the prediction.

non-linear regression
all approaches for computing positional scores thus far can be understood as an extension of eq.  <dig>  namely, they are all linear methods. additional improvements might be achieved by relying on non-linear methods. the power of non-linearity is illustrated by conservation indices. conservation indices are non-linear combinations of profile elements , which was motivated by the prior knowledge that conserved tm residues tend to be buried while variable ones tend to be exposed to the membrane  <cit> . in fact, table  <dig> showed that conservation indices were the features most strongly correlated with the burial status of tm residues. also, it is shown below that conservation indices play a much greater role than profile elements in boosting prediction accuracies. in theory, a perfect non-linear method should be able to find such non-linear combinations of profile elements when fed only profile elements. however, this is usually not the case. whenever prior knowledge on the system under investigation permits sensible non-linear combinations of raw features , it is always good to do so explicitly.

1t: the target residue, c4: the 4th residue c terminal to the target residue, n4: the 4th residue n terminal to the target residue. thus, the conservation index of the target residue is most indicative of its burial status, and the conservation index of the 4th residue c terminal to the target residue is second most indicative of the burial status of the target residue.

2l: leucine, g: glycine, i: isoleucine, v: valine, f: phenylalanine and s: serine.

if there still remain untapped non-linear combinations of profile elements or profile elements and conservation indices that correlate with the burial status of tm residues, the use of non-linear methods might be profitable  <cit> . of the vast array of available non-linear regression techniques, we made use of svr with a radial kernel because a nice interface with r is already available  and it has performed respectably in studies of water-soluble proteins  <cit> . our preliminary analysis showed that svr with a radial kernel tends to rival svr with other kernels. once a kernel type is chosen, another important parameter to be fine-tuned is the regularization constant c, i.e. how much weight one should put on minimizing the costs of violating a decision boundary relative to maximizing the closest distance of a data point to the boundary  <cit> . the general expectation from the theory is that as the regularization constant gets higher, a heavier weight is put on minimizing the violation costs and, as a result, a more wiggly decision boundary is obtained with a possibly larger generalization error. the default c value is  <dig>  and we tried  <dig> different c values,  <dig>   <dig>   <dig>  and  <dig> . as above, the  <dig> elements of a window of size  <dig> were ranked according to their fisher's indices, and increasing fractions of them  were input to the prediction via svr with a radial kernel. table  <dig> shows the results . it is immediately clear that, in almost all cases, linear regression outperforms svr, indicating that the generalization errors of svr are larger than those of linear regression, presumably due to its over-flexibility in fitting a separating boundary to a given data set. thus, svr does not seem advantageous over linear regression on this data set. admittedly, we can not rule out the possibility that highly fine-tuned svr can outperform linear regression. given limited computational resources and considerable amounts of computation required for a leave-one-out validation of a two-step prediction method , it is beyond our capability to exhaustively scan all possible combinations of svr parameters. however, it is our experience that svr with all parameters set to default values generally performs nearly optimally. thus, we are quite certain that, at least for the current purpose of predicting the burial status of tm residues of hmps, linear regression is at least as effective as svr. supporting this conclusion, previous studies on water-soluble proteins demonstrated that sophisticated linear methods can rival non-linear ones in performance  <cit> .

optimizing classifiers
upon computing a positional score for the target residue, a classifier is invoked to classify it as either buried in the protein core or exposed to the membrane. although any machine-learning technique can be used as a classifier, we have only considered lsvcs so far. the original reason for choosing lsvcs was, as mentioned earlier, to implement the bw method as exactly as possible, yet in an objective, reproducible manner, so that the bw method can be justly compared with ours. however, we may choose other classifiers for our prediction method. although there are tons of available classifiers, we primarily focused on svcs for practical reasons as mentioned above. preliminary analysis showed that svcs with a linear or radial kernel tend to outperform others. thus, svcs with a linear or radial kernel were pursued further in combination with  <dig> different regularization constants,  <dig>   <dig> ,  <dig> ,  <dig>  and  <dig> , chosen on the basis of the results shown in table  <dig>  in addition to searching for a better classifier, it might also be helpful in boosting prediction accuracies to refine input vectors themselves. so far, the input vectors for a classifier have been one-dimensional, i.e. consisting of a positional score for a given target residue. the input vectors for a classifier can be straightforwardly refined exactly in the same way as the input vectors for computing positional scores were refined in table  <dig> 

the performance of tmx – is it "significantly" higher than that of the bw method? as mentioned earlier, the prediction accuracy of the bw method is  <dig> % when tested on the same data set. the p value estimating the statistical significance of the  <dig> % increase in the prediction accuracy achieved by tmx relative to the bw method is < 10- <dig> according to the wilcoxon signed rank test. accordingly, tmx is judged to be a truly better method for predicting the burial status of tm residues of hmps. a final point worthy of noting is the architecture of tmx. tmx is a two-step prediction method, where binary classifications are made in the second step on the basis of positional scores computed in the first step. the two-step architecture – is it really worthwhile? obviously, one can directly apply svcs to the profiles and conservation indices of the target residue and its neighbors for predicting its burial status, without computing positional scores in the first place. several studies on water-soluble proteins noted that a two-step prediction scheme can better account for correlated patterns of properties to be predicted, leading to higher prediction accuracies  <cit> . to test whether this is also the case for us, we investigated the performance of svcs that were directly fed profiles and conservation indices for the prediction. specifically, as shown in table  <dig>  the  <dig> elements of a window of size  <dig> were sorted according to the fisher's index, and increasing fractions of them were fed to svcs. the best prediction accuracy for an svc with a linear kernel was  <dig> %, and that for an svc with a radial kernel  <dig> %. therefore, a two-step prediction scheme appears to pay off in our case, too.

comparison with the yu method
the yu method computes the positional score of a target residue via svr using position-specific scoring matrices  obtained by psi-blast  <cit> . in studies of water-soluble proteins, it has been very popular to use pssms as input vectors in order to boost the accuracy of predicting solvent accessibility  <cit> . the popularity of pssms has partially stemmed from the fact that one does not have to explicitly generate an msa for obtaining pssms. as with the bw method, we implemented the yu method for a transparent performance comparison using the r interface  <cit>  of the libsvm library  <cit> . in implementing the yu method, we set all the parameters of svr as optimized by yuan et al. and did not intentionally seek any further optimizations.

the best prediction accuracy of the yu method on the benchmark data set is  <dig> % , much lower than  <dig> % achieved by tmx . it is of interest to find out where the performance difference between tmx and the yu method comes from, except for the novelties introduced to tmx such as feature selection and a sophisticated classification in the second step. to this end, we replaced pssms by profiles or conservation indices to find out how different input vectors affect prediction accuracies. table  <dig> shows that profiles alone perform similarly to  pssms. compared with the performance of profiles or pssms, the performance of normalized conservation indices is really standing out. moreover, a comparison of the performance of profiles plus normalized conservation indices shown in table  <dig> with that of normalized conservation indices alone  also indicates that conservation indices play a crucial role in boosting prediction accuracies. thus, it may be concluded that the poor performance of the yu method is partly due to the fact that its input vectors – pssms – do not contain the information captured by conservation indices. in this regard, it is interesting to note that the most effective method for predicting the solvent accessibility of water-soluble proteins uses pssms as its sole input  <cit> . thus, it would be worthwhile to check out whether replacing pssms by profiles plus normalized conservation indices would be similarly successful for water-soluble proteins.

1regularization constant c was set to  <dig> 

analysis of the tmx predictions
in addition to prediction accuracies, there are other interesting aspects worthy of analyzing. for example, are there any amino acids for which it is easier to predict the burial status? is it easier to correctly predict buried residues as being buried than exposed residues as being exposed?

confidence scores for the predictions made
it is highly desirable to have confidence scores available for the predictions made. confidence scores allow the user to selectively utilize prediction results in real application settings. in tmx, the classification is performed using an svc. it is intuitive that predictions made by an svc with a high decision value  would be more accurate, and we found out that this is indeed the case. thus, the absolute magnitude of a decision value generated by the svc is taken to be a confidence score for the prediction  <cit> . as shown in fig.  <dig>  predictions with a high confidence score tend to be more accurate than those with a low one. the prediction accuracy rises to  <dig> % when considering the  <dig> predictions with a confidence score ≥  <dig> .  <dig> out of  <dig> means a coverage of  <dig> %. thus, a fairly high coverage is maintained for prediction accuracies of ~90%, which makes tmx well suited to real application settings.

outlook
in this study, we have focused on hmps because they are much more abundant and play a more important role in diverse cellular processes than beta-barrel membrane proteins . but, obviously, one can apply the same strategy behind the development of tmx to bmps. since the burial status of tm residues of bmps tends to alternate due to prevalent beta strand elements  <cit> , it would be interesting to see whether one can achieve higher prediction accuracies than  <dig> % for hmps. moreover, it would be worth investigating which features correlate strongly with the burial status of tm residues of bmps via a feature selection of the sort used here for table  <dig> 

CONCLUSIONS
we have presented tmx, a novel sequence-based computational method for predicting the burial status of tm residues of hmps. it significantly outperforms previously proposed methods. in addition, feature selection incorporated in tmx revealed interesting insights into the structural organization of hmps. importantly, unlike the previous methods, tmx automatically generates confidence scores for the predictions made, and it was shown that predictions with a high confidence score tend to be more accurate than those with a low one. thus, in a real application setting, the user of tmx can selectively utilize prediction results on the basis of their confidence scores. the developmental course of tmx clearly highlighted the importance of conservation indices and feature selection in boosting prediction accuracies. in this regard, it was rather surprising to find that the most effective method for predicting the solvent accessibility of water-soluble proteins considers neither conservation indices nor feature selection. it would be interesting to investigate whether these two "new" findings can be favorably transferred to one of the classical bioinformatics problems of predicting the solvent accessibility of water-soluble proteins.

