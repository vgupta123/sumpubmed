BACKGROUND
next generation sequencing has been responsible for numerous advances in the biological sciences allowing for the rapid production of sequencing data at rates not previously possible. next generation sequencing has allowed for much innovative research in fields such as cancer genomics  <cit> , epigenetics  <cit> , and metagenomics  <cit> . these instruments are capable of producing several millions to billions of short reads in a single run. these reads are usually a small fraction of the original genome and do not contain much information individually. the massive amount of data that next generation sequencing technologies have produced has necessitated the development of efficient algorithms for short read analysis. next generation sequencing technologies generate reads at high levels of genome coverage causing many of the reads to overlap. specialized software programs called assemblers utilize these overlap relationships to organize, order, and align reads to produce long stretches of continuous sequence called contigs, which can be used for downstream analysis. graph models for providing structure for the reads and their overlap relationships form the foundation of many of these assembly algorithms  <cit> .

metagenomics is a field of research that focuses on the sequencing of communities of organisms. this adds an additional layer of complexity to the analysis of short reads produced from metagenomics samples containing multiple sources of genetic information. often these reads must be clustered or binned into their respective genomes before assembly or analysis of the reads can take place to avoid chimeric assembly results  <cit> . multiple clustering and binning algorithms have been developed to address this issue  <cit> . while assembly results have been shown to be substantially improved by clustering metagenomics data before sequence assembly  <cit> , overlap relationships retained by the assembly overlap graph are lost, leading to the removal of key global read overlap relationships and read similarities.

to address this issue, we previously introduced a short read analysis algorithm  <cit>  that utilizes an overlap graph to model reads and their overlap relationships. our algorithm utilizes heavy edge matching  and graph coarsening methods  <cit>  to efficiently reduce the overlap graph iteratively and to generate clusters of reads. at each graph coarsening iteration the algorithm outputs the reduced graph, producing a series of graphs representing the original read dataset across a spectrum of granularities. in comparison, most previous methods rely on a single graph to represent the read dataset, which may not capture all dataset features. the goal of our research is to create a multilevel approach that will allow for the extraction and analysis of dataset features at different information granularities that can be integrated into the assembly or analysis process. in our previous work, we applied our algorithm to cluster simulated reads representing a metagenomics dataset produced by the  <dig> technology. we then applied our algorithm to  <dig> bacterial datasets downloaded from ncbi to test our algorithm's ability to efficiently reduce and store the overlap graph. in this paper, we test the scalability of our algorithm and its ability to accurately cluster simulated illumina read datasets at different genome coverages. we compare our algorithm's performance when applied to simulated  <dig> reads versus simulated illumina reads. we also conduct a study using an illumina metagenomics dataset downloaded from ncbi to evaluate the scalability of our algorithm. the obtained results show that our algorithm was able to substantially reduce the illumina metagenomics overlap graph size and is scalable for large datasets. results also demonstrate that our algorithm is practical for both  <dig> and illumina data.

RESULTS
in this section, we apply our graph coarsening and clustering algorithm to three illumina metagenomics read datasets simulated at 5x, 15x, and 25x genome coverage. we evaluate our algorithm's graph coarsening and clustering results and compare them to results obtained by clustering a similar  <dig> metagenomics read dataset. finally, we apply our algorithm to a large illumina metagenomics dataset to demonstrate its scalability and ability to reduce large datasets. for all datasets, we report read overlapping and graph coarsening runtimes when ran on single or multiple compute nodes in a high performance computing environment.

metagenomics clustering of simulated illumina and  <dig> reads
for this study we generated illumina read datasets from the eight reference genomes downloaded from ncbi refseq  <cit>  used to generate the  <dig> metagenomics dataset in  <cit> . these reference genomes were selected at various levels of homology. half of the bacterial genomes were chosen from the phylum firmicutes and the remaining half were chosen from the phylum actinobacteria. pairs of reference genomes were chosen from the same order.

the software package art  <cit>  was used to simulate illumina reads with a read length of  <dig> bp from each genome at 5x, 15x, and 25x coverage. the characteristics of the  <dig> metagenomics dataset and the illumina datasets can be found in table  <dig> developed in  <cit>  and table  <dig>  respectively.

eight compute nodes on the commercial strength firefly cluster located at the holland computing center were used for read overlapping of the simulated illumina metagenomics dataset  <cit> . after this was completed, the graph coarsening algorithm was applied to the read overlaps that were output by the overlap algorithm. graph coarsening was run on a single node on the firefly computer. the minimum for the ratio of nodes successfully matched to total graph size was . <dig>  the minimum edge density was set to a threshold of  <dig> 

clusters were generated from the reduced graph at each graph coarsening iteration. we assigned each cluster and its reads a classification at the species level by majority read vote. our algorithm's cluster classification error rate was defined to be the percentage of misclassified reads. the error rates for the classifications at the species level for various graph coarsening iterations of the simulated illumina datasets are shown in figure  <dig> a.  <dig> b and  <dig> c display the node and edge counts for each graph coarsening iteration, respectively. the read overlapping and graph coarsening runtimes can be found in figure  <dig> 

for the purpose of comparing our algorithm's performance when applied to datasets generated by different sequencing technologies, we reran our algorithm on the  <dig> metagenomics dataset in  <cit> . all runtime parameters remained the same for both the simulated illumina and  <dig> read datasets. the error rates for read classification at the species level for each graph coarsening iteration of the simulated  <dig> read dataset can be found in figure  <dig> a.

the node and edge counts can be found in figure  <dig> b and  <dig> c, respectively. node counts are also recorded in table  <dig>  the graph coarsening algorithm was able to reduce the node count of original overlap graph  <dig>  fold,  <dig>  fold, and  <dig>  fold for the 5x, 15x, and 25x read datasets, respectively, indicating that graph coarsening effectiveness may increase with increasing dataset coverage.

the results from the graph coarsening and clustering of the simulated illumina and  <dig> read dataset demonstrate that the read error rates for the two sequencing technologies are comparable and that our algorithm can be successfully applied to both  <dig> and illumina reads. read overlapping and graph coarsening runtimes demonstrate the scalability of the algorithm for datasets of increasing size and genome coverage. for the largest simulated illumina dataset, read overlapping was completed on eight nodes in less than twelve hours. graph coarsening was completed on a single node in less than forty-five minutes for the largest simulated illumina dataset. the read overlapping and graph coarsening runtimes for the remaining datasets can be found in tables  <dig> and  <dig>  respectively.

illumina bioreactor
metagenomics
illumina bioreactor metagenomics dataset
the results from the simulated metagenomics study demonstrated the algorithm's ability to reveal incremental levels of information in read datasets and that it can be extended to both  <dig> and illumina read datasets. however, for this algorithm to be practical for a wide range of sequencing applications, we must demonstrate the scalability of this algorithm for large read datasets. for this purpose, we applied our algorithm to a large illumina bioreactor metagenomics dataset and evaluated its runtime and ability to reduce a large overlap graph.

we downloaded an illumina bioreactor metagenomics dataset from the ncbi sequence read archive  <cit> . table  <dig> describes the characteristics of this dataset. paired-end reads were split for a total of  <dig> , <dig> single reads. any low quality read ends were trimmed with a minimum quality score of twenty using the fastq quality trimmer of the fastx-toolkit  <cit> .

read overlapping was completed on thirty nodes of the firefly computing cluster  <cit> . we then applied the graph coarsening algorithm to the overlap relationships produced by the read overlapper. we applied our graph coarsening algorithm multiple times to the dataset with varying numbers of compute nodes added to the parallel merge sort algorithm. the minimum for the ratio of nodes successfully matched to graph size was . <dig>  the minimum edge density threshold was set to  <dig>  the node counts for each graph coarsening iteration are recorded in table  <dig>  the runtime for the read overlapping of the illumina bioreactor dataset is shown in table  <dig>  figure  <dig> shows the runtime for the graph coarsening algorithm versus the number of compute nodes added to the parallel merge sort algorithm. the results demonstrate that our algorithm is scalable for larger metagenomics datasets. figure  <dig> displays the reduction in node and edge counts per graph coarsening iteration. the number of edges was reduced approximately  <dig> times from the original overlap graph by the twenty-sixth graph coarsening iteration. the number of nodes was reduced approximately twelve times from the original overlap graph by the twenty-sixth graph coarsening iteration. the greatest reduction in edge counts occurred in the first ten graph coarsening iterations and the greatest reduction in node counts occurred in the first fourteen graph coarsening iterations.

CONCLUSIONS
in this paper, we introduced a graph coarsening and clustering algorithm that is able to model reads at multiple levels across a spectrum of information granularities. we demonstrated our algorithm's ability to cluster simulated illumina metagenomics data at different levels of genome coverage. clustering error rates of the algorithm applied to simulated illumina metagenomics reads are comparable to error rates for clustering a simulated  <dig> metagenomics dataset with similar dataset characteristics. this suggests that our algorithm can be successfully applied to both illumina and  <dig> read data. algorithm runtimes were practical for all datasets. the largest simulated illumina read dataset required less than twelve hours to complete read overlapping on eight compute nodes. graph coarsening was completed on a single node in less than forty-five minutes. read overlapping of the simulated  <dig> read dataset required less than an hour on eight compute nodes. graph coarsening required less than seven minutes on a single compute node. the graph coarsening algorithm was more effectively able to reduce the higher coverage simulated illumina datasets than the lower coverage illumina datasets.

finally, we applied our algorithm to a large illumina metagenomics dataset to demonstrate its scalability. by utilizing parallel computing, our read overlapping algorithm was able to complete less than eight hours. the graph coarsening algorithm completed within approximately seven to twelve hours depending on the number of nodes added to the parallel merge sort portion of the graph coarsening algorithm. the algorithm was able to reduce the number of edges in the overlap graph nearly  <dig> fold while recording each graph level on disk, allowing a researcher to access the overlap graph at various levels of complexity. we plan to expand our graph coarsening algorithm to a full sequence assembler which will be contrasted to currently available assembly methods. we also plan to conduct further in-depth studies on the impact of input parameters on the graph coarsening process. most current approaches rely on one overlap graph to capture a single snapshot of the reads and their overlap relationships. in contrast, our proposed assembly algorithm will rely on a series of coarsened graphs to capture both local and global dataset features.

the goal of our research is to develop an analysis method that will allow us to extract features of the read dataset at multiple information granularities to incorporate into the read analysis and assembly process. in the future, we plan to configure the algorithm such that clusters or nodes can be selected at different levels of information granularity. for example, if a node in a reduced graph is over-collapsed, we can select its child nodes from the previous graph iteration instead. we can continue with this zooming-in and zooming-out process, selecting child nodes from previous graph iterations until the desired node criteria is achieved or optimized. this will facilitate a customizable, intelligent approach to the read analysis and assembly problem.

