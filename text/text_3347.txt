BACKGROUND
protein motif detection is key to identifying conserved protein domains within family of proteins as well as deducing its structure and function within the genome. the hmmer  <cit>  suite of programs is widely used for protein motif finding, building the profiled hidden markov model , scanning an entire database of hmms for all motifs etc. the current version, hmmer ver  <dig> x, is a significant improvement over its predecessors due to the scoring system used to compute the statistical significance of alignment scores. among the suite of tools in hmmer, hmmsearch is used to detect a query motif among a target database of sequences. the wide applicability of motif finding, the rapid growth of the set of protein families as well as the set of known sequences has made it target of many acceleration attempts. although the list of acceleration attempts for hmmer  <dig> x  <cit>  is not exhaustive, some representative contributions include . while hmmer  <dig> x used viterbi algorithm  to compute the scores, hmmer  <dig> x follows a scoring system that computes the total log-likelihood ratios summed over all possible alignments, via the forward-backward algorithm  <cit> . optimal alignment scores are useful in studying similarity between individual sequences , the forward scores are more meaningful in alignment of target protein sequences against a probabilistic model such as the hmm.

although the forward-backward algorithm for probabilistic inference has the same computational complexity as the viterbi algorithm, computing the forward scores requires much higher computational throughput  than the viterbi algorithm  <cit> . this is due to the sensitivity of the sequential dependency imposed by the d-d transitions in the profiled hmm. the d-d transitions in the forward algorithm are always essential to computing the overall scores, where as the d-d transitions in viterbi algorithm has an effect, only if it scores higher than the other transitions. this enables various methods to quickly assert the impact of d-d transitions in viterbi scores and increase its overall throughput, and are not applicable to accelerating forward score computation.

it is shown that  <cit>  distribution of high-scores of optimal alignment  is gumbel distributed with parameter λ= log <dig> and that of forward scores  is exponentially distributed with the same λ= log <dig>  hence, the high-scoring tails of viterbi and forward scores agree with each other, which enables designing an efficient task pipeline that can filter out sequences based on viterbi scores that are not expected to score high via the forward algorithm. although, this pipeline removes the load off the forward score computing stage, the viterbi based pre-filtering is still as expensive as the scoring system employed in hmmer  <dig> x. in order to mitigate the computational workload on the p7viterbi stage a heuristic multiple-segment-viterbi  is introduced that is analogous to word hit and ungapped extension stages implemented in blast. the msv stage employs a much simpler hidden markov model for scoring that eliminates sequential dependencies between the dynamic programming  matrix cells which was “vectorized” on a parallel machine. through choice of sensitivity parameters of msv scores in hmmer  <dig> x, an 8-bit saturating scoring system was used whose computation was vectorized on a 128-bit sse register as  <dig> parallel operations on 8-bit data, thus achieving a 16-fold speedup on a commodity processor core. furthermore, the latest version of hmmer, ver  <dig> , includes the ssv sub-stage, another heuristic to accelerate the msv stage by ignoring “j” state transitions that is designed to chain multiple matches together  <cit> . although this may result in false negatives on the score of sequences, it provides speedup over msv significantly.

previous work
due to extensive computational and scoring optimization procedures implemented in hmmer  <dig> x  <cit> , it is extremely unlikely to improve the performance further either on cpu or gpu based platforms with generic optimization techniques alone. for the previous version, hmmer  <dig> x, which is based on viterbi algorithm, several strategies were proposed to accelerate the underlying viterbi score calculation. hmmsearch in hmmer  <dig> x was initially parallelized for clusters via mpi in  <cit>  where the state loop was vectorized to process  <dig> hmm states in simd fashion or  <dig> state triplets at once. the initial work utilizing graphics processing units  to accelerate hmmsearch in hmmer  <dig> x is claw-hmmer  <cit> , and gpu-hmmer  <cit>  achieves limited speedup over it. partial prefix sums were used  <cit>  to break the chain of dependencies in computation of viterbi scores. this helped extract a hybrid task and data-level parallelism in order to solve the load imbalance problem that arises due to variations in sequence lengths. in addition to multiprocessor systems, a number of attempts to accelerate implementation of the hmmer recurrence have been carried out for fpgas . an extensive review of various acceleration attempts was compiled in  <cit> .

however, unlike the previous version, which has been target of numerous acceleration attempts, there exist only a handful of existing work aimed to improve the performance of key segments of hmmer  <dig> x pipeline. the main reason being hmmer  <dig> x is already highly optimized and is about 100- to 1000- fold faster than hmmer  <dig> x  <cit> , implemented on the commodity processors with sse support and multi-core parallel. this renders any acceleration attempt for previous versions of hmmer obsolete. hence alternative architectures such as fpga  <cit>  have been explored as an accelerator hardware for msv and p7viterbi segments in hmmer  <dig> x. the viterbi algorithm was rewritten for parallelization via prefix sums approach on the fpga and is able to achieve comparable performance for p7viterbi implemented on dual-core processors. however the hardware limitations on the fpga makes this implementation suitable for smaller models  and tiling larger models into several dataflow partitions.

in  <cit> , a speculative gpu based method was implemented to reduce the global memory access within the kernel of msvfilter. this approach aims to reduce the execution time of original reduction loop empirically. lin’s work  <cit>  also focused on msv stage by following the parallel strategy of gpu-hmmer but introducing simd instructions. different sequences were assigned to individual threads in both methods. partial optimization was proposed in  <cit> , which parallelizes the p7viterbi part without considering the d-d path dependency. although this approach claims a 14x speedup than original functions, it sacrifices the sensitivity of probabilistic inference. another attempt of accelerating p7viterbi  <cit>  was implemented on intel and amd cpus with proposed cache-oblivious strategy that offsets cache miss penalties of original work. moreover, the newest stage of hmmer  <dig> , ssvfilter, was accelerated  <cit>  through a set of optimizations that mainly include model tiling, loop unrolling, coalesced and vectorized memory access.

other work related to pairwise and multiple sequence alignment based on the smith-waterman and needleman-wunsch algorithms have been accelerated on cuda-based gpus. cuda-linsi  <cit> , a multiple sequence alignment  algorithm, accelerated cpu-based linsi of mafft  <cit>  software package by optimizing global and shared memory access as well as employing data compression techniques. pairwise and group-to-group alignments are calculated by individual threads in this work. another cuda-based msa acceleration, cuda clustalw  <cit> , assigned one pairwise alignment to a thread block by extracting parallelism along the major/minor diagonal direction.  <cit>  proposed a comprehensive acceleration solution on the gpu for all-to-all pairwise global, semi-global and local sequence alignments, using tile-based dynamic programming framework that significantly reduces the number of write/fetch through device memory. however, in contrast to sequence alignment algorithms, protein-motif finding imposes non-local and complex dependencies between the dynamic programming  cells which necessitates alternative techniques for parallelization.

cuda-enabled gpu architecture
as a parallel computing engine, cuda-enabled gpus are built around a scalable array of multi-threaded streaming multiprocessors  for large-scale data and task parallelism, which are capable of executing thousands of threads based on simt mechanism. following tesla  <cit> , fermi  <cit>  and kepler  <cit>  to latest maxwell  <cit> , each generation has more hardware resources and newer intrinsic functionalities than the previous. our proposed algorithms and methods in this paper which utilizes the latest intrinsics, are designed for the tesla k <dig> of kepler architecture with compute capability  <dig>  or higher. the kepler architecture also features more powerful streaming multiprocessor  which consists of  <dig> single-precision cuda cores,  <dig> double-precision units,  <dig> special function units and load/store units  <cit> . in highlight, the architecture offers another  <dig> kb on-chip read-only data cache with an independent datapath from the existing l <dig> and shared memory datapath, and the maximum amount of available registers for each thread is increased to  <dig> for gk <dig> instead of prior  <dig> per thread.

hmmer pipeline: msv/ssv and p7viterbi
the task pipeline of hmmer  <dig> x is optimized for computational efficiency that employs heuristics to eliminate vast majority of low scoring sequences by introducing msv- and ssvfilter. as sequences filters, msv detects contiguous match alignments while ssv captures single match aligmnet, which are analogous to the ungapped high scoring pairs implemented in blast. although blast uses a two-stage filter to detect and extend the ungapped alignments, the uniform entry/exit probability in the msv/ssv model allows for partial matches upto the size of the query motif. the profile msv/ssv models are shown in fig.  <dig>  and the full plan- <dig> viterbi model  is shown in fig.  <dig>  without inter-row dependency caused by “j” state, the potential missing of higher score will be checked in ssv stage  <cit>  followed by a regular msv processing. as shown in fig.  <dig>  the result of pipeline benchmark indicates that  <dig>  % of the sequences cross the msv/ssv threshold to be passed on to the p7viterbi stage. only  <dig>  % of all the sequences are passed on to the forward-scoring stage. the corresponding execution time is close to  <dig> % for msv/ssv,  <dig> % for p7viterbi and  <dig> % for forward-backward stage.
fig.  <dig> profiled hmm models.  msv model;  ssv model;  p7viterbi model

fig.  <dig> heuristic pipeline of hmmer  <dig> x. a sample benchmark with query model of  <dig> length and the env-nr database consisting of  <dig>  million protein sequences



methods
gpu acceleration
since the majority of the execution time is spent in the msv/ssv filtering stage, it is a prime candidate for acceleration. as vast majority of input sequences are also eliminated in this stage, any improvement in the performance will greatly impact the efficiency of the pipeline. both msv and ssv model exhibits regular and well-behaved dependencies that can be easily parallelized compared to p7viterbi. however, in order to exceed the performance of the highly optimized msv/ssv filter of latest hmmer especially on multi-core cpus, it is imperative to go beyond generic parallelization techniques and exploit architecture-specific intrinsics.

the model simplifications in the msv compared to the full core model used in p7viterbi stage eliminates the “delete” states that induce sequential dependencies between the cells of the dynamic programming matrix within each row. the “insert” states that induce dependencies to the previous rows are also eliminated, leaving only the “match” states that induce a diagonal dependency to cells in the previous row. ssv model, additionally removes the “j” state which eliminates a portion of heavy workload within the computationally intensive innermost loop. however, most of existing work with coarse-grained parallelization ignore the overhead caused by synchronization within the msv/ssv and p7viterbi kernels on gpu, which forces active threads to enter idle state and wait for other threads to complete. the problem is further amplified by the fact that the total number of alignment, each with multiple synchronizations, is equal to the total number of collective residues contained within all sequences , which can severely limit the performance. further optimization attempt must avoid unnecessary synchronization or totally eliminate them if possible.

warp-synchronous execution
with the current simt mechanism of cuda-enabled gpus, we exploit the fact that every  <dig> threads within a thread-warp are always executed synchronously by the current cuda programming model. thus, we make each warp processes a sequence residue by covering a single row of the dp matrix moving on to the successive row  of the sequence until the entire sequence is scored. hence by having a single warp update all the cells within each row, the need for synchronizations can be eliminated. moreover, in order to avoid data dependency between warps, each thread-warp processes a different sequence and continues to process the next sequence in the database independent of other warps within the smx or the device. this again eliminates need for any block-level coordination or stalling due to synchronization, and helps keep active threads always busy and maximize kernel throughput. this achieves true independence between warps and completely eliminates the demand of synchronization throughout the course of entire execution.

cuda-simd based parallellization
in addition to cuda c/c++, cuda-enabled gpus also support a low-level programming model via parallel thread execution  virtual machine with instruction set architecture   <cit> , for efficient data-parallel computing. since ptx isa version  <dig> , a set of simd  video instructions has been introduced for intra-word operations such as quads of 8-bit values and pairs of 16-bit values. table  <dig> lists all simd video instructions that are hardware accelerated for kepler architecture and only available on devices of compute capability  <dig>  or higher  <cit> . in this work, we increase the parallel throughput by embedding simd intrinsics within warp-based, self-synchronous simt mechanism of gpus. similar to the sse  instruction set on cpu, which supports 128-bit registers with 16-lane parallelism, the simd video instructions on the gpu enable 4-lane data parallelism per thread. this increases the available parallelism within a single-warp from  <dig> to  <dig>  all of which are executed without any synchronization overhead. by assigning each sequence to individual warps, both the parallel throughput as well as hardware resource utilization are greatly enhanced. this finer-grained parallelism helps obtain augmented speed-up on cuda-enabled gpus and introduces a new tier of parallelization.

vadd <dig>  vsub <dig>  vadd <dig>  vsub4
.u <dig> s <dig> sat.add

vmax <dig>  vmin <dig>  vmax <dig>  vmin4
.u <dig> s <dig> sat.add

vset <dig>  vset4
.u <dig> s <dig> cmp.add

vavrg <dig>  vavrg4
.u <dig> s <dig> sat.add

vabsdiff <dig>  vabsdiff4
.u <dig> s <dig> sat.add
respectively, u <dig> and s <dig> represent unsigned and signed values of 32-bit; sat is used to clamp the range of operand based on its bit-width; add is for accumulation; cmp consists of  <dig> comparison operators: eq, ne, lt, le, gt, ge




gpu runtime compilation
in compiled programs the parameters defined at compile-time via macro constants make various compiler optimizations possible. however, most of values of those parameters are only known at runtime, thus disabling the compiler to optimize kernel as much as possible at offline compile-time. runtime compilation enables programmer to take advantage of improved performance due to predefined macro parameters but in run-time. in addition, it also enables application driven construction of the kernel dynamically at runtime. based on the knowledge of the problem data, it is possible to dynamically construct various parts of the kernel from a repertoire of subparts, optimized for the current problem. in this work the advantage offered by nvidia runtime compilation, nvrtc, was leveraged in order to define the hmm and database parameters at runtime so as to enable compiler optimizations such as loop unrolling, as well as switchable kernels, to increase data locality and pursue better performance.

four-tiered parallelism
on a single gpu, multi-tiered parallel framework  is organized into four tiers of parallelism, comprising of simt and simd execution. the top three tiers, derive from our previous work  <cit> , are based on simt mechanism and the last tier is built on simd intrinsics. figure  <dig> gives a overview of the framework: the first tier of parallelism is built on the multiple smxs that work concurrently; the second tier is composed of multiple resident warps within each smx, that process different sequences and are independently driven by multiple warp schedulers; the third tier is due to the warp synchronism, where all threads of the same warp compute alignment scores of the same sequence concurrently; the fourth tier is built on simd intrinsics, where every thread operates on quads of 8-bit values for msv and pairs of 16-bit viterbi scores  respectively. the multiple tiers are oblivious to the cuda version and are only related to hardware resources and device properties such as the number of smxs, warp size and bit-width of registers that are simply queried at runtime and provided as built-in constants. this kind of hardware-aware optimization makes the parallelization scalable and portable thus fully utilizing the computational capability of cuda-enabled gpus.
fig.  <dig> cudampf: multi-tiered parallel framework on cuda-enabled gpu.  a single gpu consists of n smxs with m concurrently mounted blocks on each;  within each block, q resident warps are scheduled by x warp scheduler for processing assigned sequences;  a warp of threads score alignment of all residues and model states in parallel ;  based on 32-bit register and score ranges of different algorithms, each thread processes multiple model states in a single step. the virtual boundary, block, is only regarded as the container of warps rather than a separate tier



there is no demand of explicit thread-synchronization to keep the threads consistent within the same thread-block. across all smx units, warps process their own sequences proceeding to the next target upon completion, independently. the index of next sequence for each warp is calculated as: 
  inext=owarp+cwarp×nwarp×nsmx 

where owarp is the ordinal id of a warp across nsmx smxs, cwarp is a counter that records the number of sequences processed by this warp, nwarp is the total number of resident warps per smx on the launched kernel. warps always keep selecting the next sequence as long as the index inext<total, the total amount of sequences within database. without any request of synchronization, cudampf avoids thread idling caused by unbalanced length of sequences as well as correctness check across boundary due to concurrency and racing hazard amongst threads, which improves speed and throughput.

implementation details
striped layout vs. sequential layout
although sequential layout of dynamic programming cells is straightforward, it is not suitable for simd operations in the presence of diagonal dependencies between dp cells such as in msv/ssv and viterbi algorithm: each thread is dependent on the value computed by an adjacent thread ), in the previous iteration. this is because using per thread memory such as registers or local memory will require extra instructions like shifting and bitwise operations to exchange private data  between threads. moreover, calculating score of the first cell  after each iteration imposes additional sequential overhead. these overmuch instructions and thread idling result in weak parallelism that will be further ampilified within the innermost loop.
fig.  <dig> comparison of alignment with sequential and striped layout. an example of msv score alignment with hmm model of l
hmm =  <dig>    <dig> threads of w
a
r
p <dig> concurrently calculate  <dig> 8-bit scores at each iteration, and every  <dig> scores are stored together as one 32-bit datatype. orange-colored boundaries indicate private regions of different threads, where threads only take charge of cells within their own regions.  a warp of threads process model states with striped interval q= <dig> that is also the number of iteration required for the current row. yellow-colored cells represent the result after one 8-bit shifting from q=q−1= <dig>  which will be used to calculate q= <dig> for next residue



in order to avoid pitfalls of sequential layout on performance, a striped layout similar to the sse implementation in hmmer is adopted, but across all threads within the entire warp. this proposed layout of scoring alignment for gpu kernel is shown in fig.  <dig>  which does not impose any dependencies between cells. each thread calculates four or two scores  concurrently with striped intervals q that is defined as: 
  q=maxαalgor.×swarp+ <dig>  

where lhmm is the length of query model, swarp is the size of warp and αalgor. indicates lanes of parallelism for specific algorithm . by coupling simd instructions and simt mechanism, each thread handles multiple striped sub-words concurrently ). in contrast with hmmsearch ran on the sse-supported cpu which achieves only 16-fold and 8-fold  <cit>  parallelism, our proposed layout achieves 128-fold parallelism for msv/ssv and 64-fold parallelism for viterbi algorithm on a gpu: each warp calculates  <dig> or  <dig> scores in parallel and iterates q times to finish each row of the dp matrix. after q iterations, only one parallel reordering across a warp of threads is needed to satisfy the diagonal dependency for the next dp row, which guarantees every thread always process same states of the query model. private registers and local memory of each thread are able to be frequently reused for scoring alignment in the case.

in addition, the transition and emission parameter matrices are also pre-formatted to be the same striped layout  and are stored contiguously for the indexing by a warp of threads during iterations of innermost loop. as the number of cells per dp row is fixed , any query model with the size that is not an integral multiple of  <dig> or  <dig> will be padded with the initial value, −inf . this results in coalesced access to off-chip memory with only one transaction per memory request.

msv/ssv and viterbi algorithm with simd
algorithm  <dig> outlines the main structure of msv kernel with three loops. loop a  iterates over the different target sequences for each warp and loop b  iterates over all residues of current sequence. in order to decrease the latency of sequence read, the sequence data is pre-fetched from global memory into the shared memory buffer of size swarp× <dig> per warp, where each thread fetches  <dig> residues packed into a 32-bit word as shown in line  <dig>  as a result, every iteration of loop b process up to  <dig> residues with only one coalesced global memory transaction. the innermost loop, loop c , is the last tier with embedded simd instructions within the simt mechanism. the underscored simd instructions, vmaxu <dig> vaddus <dig> and vsubus <dig>  represent per-byte unsigned maximum, saturated addition and subtraction with values clamped to  <dig> and  <dig>  ssv algorithm, as shown in algorithm  <dig>  is easily implemented under the proposed framework like msv kernel. however, it removes all calculations related to xj and xb which allows significant speedup inside loop c.





algorithm  <dig> shows the outline of the p7viterbi segment, which follows the same general framework. however the presence of match and insert states introduces additional dependencies between successive iterations and the presence of delete states imposes sequential dependencies within the same iteration. the d-d dependencies imposed by the delete state is resolved via the lazy-f method introduced in  <cit> , also implemented in hmmsearch and is shown from lines 26– <dig>  the value of register rdcv will be sent to loop e after re-ordering to check for potentially higher scores across q cells . by coupling simd and simt on gpu, compared to 8-fold parallelism of sse based hmmsearch, a 64-fold parallelization per warp is achieved to accelerate parallel d-d checking of lazy-f in a finer-grained way that largely eliminates sequential overhead.



reordering and max-reduction for simd & simt
in order to implement the striped layout with simd & simt, a parallel reordering of all 8-bit or 16-bit values amongst intra-warp threads is necessary at the last iteration step q=q− <dig>  as illustrated in fig.  <dig>  we proposed an inline function of ptx assembly, reorder_uint <dig> , that extracts one sub-word value from private memory of each thread, exchanges it through intra-warp shuffling as a closed cycle  and then merges this exchanged value into private memory space again. this procedure is completely concurrent for each warp in which case all threads inside are active, and the details of this inline function are depicted in fig.  <dig>  proposed ssv algorithm shares the same idea but needs shift in 0× <dig> instead of  <dig> as −inf.
fig.  <dig> illustrations of proposed reordering and maximum functions for cudampf. assuming x4>x3>x2>x <dig> after intra-warp reductions in 

fig.  <dig> pseudo ptx assemblies of inline reordering function. an example of reordering  <dig> unsigned values of 8-bit for msv kernel whereas p7viterbi kernel will process  <dig> signed values of 16-bit



another function of ptx assembly for maximum reduction, maxred_uint <dig> , across all the threads within the warp is required to compute the terminal cost, xe of each row of the dp matrix. as shown in fig.  <dig>  butterfly shuffling and quad-lane simd maximizing make sure that large values always be broadcasted to all threads at each step. after five reductions due to swarp= <dig> fixed by current cuda model, every thread then keeps four largest values  that are packed with 32-bit datatype. and the last step is intra-word shifting with the simd maximum instructions to obtain and broadcast the maximum value . figure  <dig> gives pseudo ptx assemblies for this maximum reduction.
fig.  <dig> pseudo ptx assemblies of inline maximum reduction function. an example of maximum reduction for msv kernel. s is an auxiliary register used with vmax instruction



as to viterbi algorithm, functions with int <dig> suffix shown in algorithm  <dig> indicates the width of sub-word is increased to 16-bit that reduces the number of reductions as well as shifting bits. these inline functions of ptx assemblies enable tier  <dig> in cudampf to be feasible with our proposed striped layout, and also eliminate the overhead of accessing shared memory compared to intra-register operation.

hardware-aware resource allocation
although the multi-tiered parallelization is designed to take advantage of the execution model on massively parallel processors, in order to fully leverage the power of the underlying hardware, it is necessary to maintain and optimize the device resources, like the on chip and off chip memory/cache system, with full awareness of their capabilities and performance. any improper allocation strategy would not only impair performance but also limit the scalability with respect to the data size.

in the previous work  <cit>  we evaluated the performance by using shared memory and global memory to store score matrices of model and alignment scores, where both of them met limited speedup with large query model that reduced the number of resident warps on each smx. to solve the trouble of scalability, present work uses register and local memory to store all alignment scores like mmx, imx, dmx in algorithms  <dig> and  <dig>  this benefits from kepler architecture that supports available local memory space for each thread nlocal= <dig> kb at most  <cit> , which enables lhmm upto 4b×3−1×αviterbi×swarp+ <dig> theoretically. although local memory is off-chip memory, it is naturally organized as a layout of consecutive 32-bit words accessed by consecutive threads  <cit> , that is well consistent with our striped layout of scoring alignment since all threads within a warp access their score arrays m/i/dmx with same index. this enable our warp-based operations  always archieve coalesced access to local memory with 128-bytes memory transactions, which is the optimal accessing pattern.

as this strategy leaves most of the memory on-chip unused,  <dig> kb of it can be configured to serve as l <dig> cache, thus improving cache hit ratio and performance. furthermore, the hmm model parameters such as the emission and transition scores, emsv/ssv, evit and t, can be stored in the global memory and cached by the  <dig> kb of read-only cache, as the parameter values are fixed throughout the course of the application. the use of on-chip shared-memory for storing the hmm parameters is not beneficial because  large models cannot fit within limited size of the memory and  the indeterminate access pattern to the parameter matrix stored in the shared memory as dictated by current residue will lead to bank conflicts and loss of performance. however, the case of register spill and cache hit ratio become impact factors of performance now. for the register-intensive kernel, more active threads will tighten the amount of available registers to each thread that leads to severe register spill. escpecially for p7viterbi kernel, much more local memory and registers are consumed in comparison to msv/ssv kernel, that is not only caused by additional d and i scores but also due to instruction complexity of loop c. an effective solution is properly reducing the quantity of threads that makes each of them obtains more assigned registers clamped by compiling option “-maxrregcount”. we empirically launched  <dig> resident warps per smx for both msv/ssv and p7viterbi kernels, and it obtained good trade-off with high performance.

this work also examines the performance of using on-chip shared memory to store alignment scores. given two built-in parameters sshared and sthread as maximum amount of shared memory and resident threads per smx respectively, the relationship between optimal occupancy p and the usage of shared memory per smx, umsv/ssv/vit, can be described as: 
  umsv/ssv=×swarp×4×n^warpuvit=×swarp×4×n^warpp=n^warp×swarpsthread×100% 

with two constraints: n^warp×swarp≤sthread and umsv/ssv/vit≤sshared where n^warp is the maximum amount of resident warps can be launched. increasing the length of model, p decreases rapidly due to n^warp.

selective kernel compilation and loop optimizations via nvrtc
in cuda  <dig> , a runtime compilation library nvrtc is introduced to dynamically compile cuda kernel source against offline static compilation  <cit> . this is greatly beneficial to optimizing compilation of nvcc for complex kernel with multi-loop hierarchy, where the innermost loop is related to variables that are known at runtime only. like loop c in msv/ssv and viterbi algorithm, q is calculated as loop-count by eq.  <dig> and can be pre-defined as a constant value in runtime compilation. as shown in fig.  <dig>  compiler is able to entirely unroll the loop c with #pragma unrollq to boost the performance. this is mainly because variables within innermost loop will be calculated and assigned to registers during compilation. however, for register-intensive kernel like p7viterbi, loop unrolling leads to undesirable register spill. thus, in contrast with msv/ssv kernel, the loop-count q is not passed as pre-defined constant to runtime compilation.
fig.  <dig> cudampf program with nvrtc. after obtaining query model size and device properties, program dynamically makes decisions on unrolling innermost loop and selects the proper kernel file with compiler options



furthermore, in order to obtain optimal performance, it is essential to dynamically switch between kernels best suited for the input problem. for smaller input query models, it is possible to store dp rows  within shared memory without sacrificing occupancy whereas for larger model sizes local memory is an optimal choice. hence the application dynamically selects between the shared and local memory implementation, the kernel with best configuration, to compile and run at runtime. this not only takes advantage of computing resources on device but also avoids performance degradation due to static configurations. only the best suited kernel for the input problem is ever compiled and run, thus avoiding needless compilation of all kernels at compile-time with higher overhead. the cost of runtime compilation via nvrtc is listed in table  <dig>  as shown in the table, the maximum cost of compilation at runtime is around  <dig> ms which is negligible compared to total runtime of the application and may very well be completely hidden via multi-threading.
read.cu.cuh file into string

aintel i5-3570k quad-core  <dig>  ghz cpu and 64-bit ubuntu linux


bintel xeon e <dig> octa-core  <dig>  ghz cpu and 64-bit centos linux



RESULTS
benchmark environment
in order to evaluate proposed msv and viterbi algorithms in cudampf comprehensively, the benchmark analysis is composed of two parts:  the intrinsic comparison of different configurations in order to study the relationship between gpu kernel performance , cache hit ratio, kernel occupancy and the length of query models;  the extrinsic comparison of performance between cudampf on gpu and hmmersearch from hmmer  <dig> b <dig> on cpu.

in order to evaluate the scalability and performance,  <dig> different hidden markov models of sizes ranging from  <dig> to  <dig> were selected from  <dig>    <cit>  with following accession numbers: pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pf <dig> , pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pb <dig>  pf <dig> , pb <dig>  pb <dig>  as to protein sequence database, swissprot database  that contains  <dig> protein sequences with totally  <dig> million residues were selected.

cudampf running on single nvidia tesla k <dig> with  <dig> smxs  and  <dig> gb memory  <cit>  was compared against hmmer msvfilter, ssvfilter and viterbifilter running on a desktop workstation with an intel i5-3570k quad-core  <dig>  ghz cpu, intel i7- <dig> octa-core  <dig>  ghz and on the single node of a compute cluster with an intel xeon e <dig> octa-core  <dig>  ghz server cpu, running 64-bit linux operating system. sequence scores of all three stages obtained from our implementation are completely identical to results of hmmersearch, and the number of sequences that pass through each filter is also matched.

intrinsic performance: nvrtc vs. static compilation
tables  <dig> and  <dig> present various intrinsic parameters such as l <dig> cache-hit ratio , read-only cache-hit ratio  and register usage per thread and the overall performance  for hmm sizes ranging between  <dig> and  <dig> and two different run-cases: static and nvrtc based compilation. the models were scored against the swissprot database. the three intrinsic parameters, l <dig>  read-only cache hit ratio and register usage per thread, indicate the device memory utilization that impacts kernel performance directly. the maximum allowable number of registers per smx  was utilized by mounting  <dig> resident warps  where each thread was allocated  <dig> private registers. as shown in table  <dig>  for the msv/ssv segment, nvrtc compiled kernels yield upto 2x and  <dig> x faster in terms of gcups compared to statically compiled kernels respectively, due to the loop optimizations that utilize available registers as more as possible. the register usage per thread is always under  <dig> thus avoiding any register spill and the use of local memory.

asufficient private registers for each thread and no demand of local memory access


bthe maximum number of available registers per thread


aassigned private registers are exhausted. registers spill to local memory



on the other hand, the complexity of p7viterbi kernel requires higher number of registers per thread. hence any loop optimizations performed by nvrtc causes severe register spill and limits the overall performance. the l <dig> cache-hit ratio degrades to  <dig>  % even for short model length of  <dig> due to the use of local memory. hence, for the p7viterbi kernel the loop optimizations were not performed by keeping the inner-most loop count a runtime variable, thus achieving equivalent performance of statically compiled p7viterbi kernel.

intrinsic performance: local memory vs. shared memory
the performance was evaluated for implementations of shared memory vs. local memory based storage of dp scoring matrix row. as mentioned in earlier sections, shared memory allocation limits number of active warps and the device occupancy as evidenced by figs.  <dig>   <dig> and  <dig> for msv, ssv and p7viterbi respectively. the occupancy of msv kernel declines from  <dig> % for a hmm size of  <dig> to  <dig>  % eventually, which obviously degrades the performance. however, the local memory implementation exhibits a steady increase in performance while maintaining a constant device occupancy at  <dig> % irrespective of the size of the query model. based on the benchmark , only three smallest models of size  <dig>   <dig> and  <dig> showed slight advantage of shared memory over local memory, which are  <dig> ,  <dig>  and  <dig>  gcups respectively. ssv kernel has similar behaviours on performance between shared and local memory oriented implementations in which case larger models gain more speedup through using local memory .
fig.  <dig> performance comparison between msv kernels of local memory and shared memory

fig.  <dig> performance comparison between ssv kernels of local memory and shared memory. the slight drop of occupancy curve for local memory is caused by nvrtc compilation

fig.  <dig> performance comparison between p7viterbi kernels of local memory and shared memory



more pronounced performance gaps between two implementations were observed in p7viterbi kernel. since the usage of shared memory for p7viterbi  is about 3x that of msv kernel, occupancy in this case degrades much more rapidly as does the performance. on the other hand, the local memory implementation yields upto  <dig>  gcups for a model size of  <dig> and roughly maintains the performance at  <dig> gcups with increasing query model sizes. however, for small models with the length of  <dig> and below, shared memory kernel achieves slightly better performance.

extrinsic performance comparison: gpus vs other processors
the original work  <cit>  yields  <dig> gcups on msvfilter and  <dig>  gcups on viterbifilter by single core of an intel processor. acceleration via high-end fpga designs in  <cit>  yields upto  <dig> gcups for msv and  <dig>  gcups for p7viterbi; gpuhmmer  <cit> , an outdated acceleration with roughly  <dig>  gcups, was modified by lin  <cit>  to accelerate msvfilter in hmmer  <dig> x, which yields upto  <dig>  gcups on a quadro k <dig> gpu.  <cit>  implements cache-oblivious strategy to accelerate viterbifilter, that yields a roughly constant performance of  <dig> gcups on an intel i <dig> processor and  <dig>  gcups on amd opteron bulldozer processor.  <cit>  claims the first acceleration attempt for ssvfilter based on hmmer  <dig> , which gains upto  <dig>  gcups on a gtx <dig> gpu.

the current implementation is compared to the latest version of hmmsearch in hmmer  <dig> b <dig> on different processors. the comparison was performed by extracting and executing only the relevant methods of p7_pipeline for msv, ssv and p7viterbi segments. in order to monitor the execution time and cpu usage strictly, intel vtune amplifier’s  <cit>  hotspot profiler was used to measure the execution time of p7_msvfilter, p7_ssvfilter and p7_viterbifilter separately for the entire sequence database. the baseline is measured in wall clock time, tlast−tfirst, where tlast is time point after the last function call and tfirst is time point before the first function call. three high-end cpus, intel i <dig>  i <dig> and xeon, were evaluated with multiple cores as shown in figs.  <dig>   <dig> and  <dig> for msv, ssv and p7viterbi stages respectively. on intel xeon, the average performances by utilizing single-, quad- and octa-cores are  <dig>  ,  <dig>   and  <dig>   gcups for msvfilter , meanwhile viterbifilter shows  <dig> ,  <dig>  and  <dig>  gcups. similarly, intel i <dig> gains  <dig>  ,  <dig>   and  <dig>   gcups for msvfilter  and  <dig> ,  <dig>  and  <dig>  gcups for viterbifilter, repectively. as to intel i <dig>  for msvfilter , we observed  <dig>   and  <dig>   gcpus, and viterbifilter yields  <dig>  and  <dig>  gcups by employing single- and quad-cores. it can be seen that the performance of cudampf clearly exceeds that of all implementations on cpus and achieves a speedup of upto  <dig> -fold  over single core and  <dig> -fold  over multi-cores for msvfilter . although p7viterbi algorithm is more complex with strong dependencies, our method still runs  <dig> x and  <dig> x faster than single and multiple cores of cpus, respectively.
fig.  <dig> evaluations of cudampf with intel xeon, i <dig> and i <dig> on msvfilter

fig.  <dig> evaluations of cudampf with intel xeon, i <dig> and i <dig> on ssvfilter

fig.  <dig> evaluations of cudampf with intel xeon, i <dig> and i <dig> on viterbifilter



CONCLUSIONS
in this paper, we proposed a novel parallel framework cudampf that embeds simd intrinsics within simt mechanism on cuda-enabled gpus, which greatly accelerate msv/ssv and p7viterbi stages of latest hmmer with  <dig> % accuracy, and the overall performance exceeds all other existing optimizations. in addition to the largely enhanced kernel throughput caused by synchronize-free execution, a finer-grained parallelism is achieved by this framework that could be also adopted to other similar problems in high-throughput sequence search. based on the characteristics of the current algorithms, this work also presents an architecture-aware strategy to make optimal utilization of memory and cache system on the kepler architecture for parallel efficiency and scalability. moreover, cuda runtime compilation  is incorporated to enable further optimization on kernel that wisely unrolls computational loops for performance boost, and it also support switchable kernels without compilation overhead in static. the strict performance evaluations illustrate that cudampf gains significant speedup over cpu implementations: comparing with three different high-end cpu, our framework yields the maximum speedup of  <dig> x  and  <dig> x  over single and eight cores for msv  kernel, and the p7viterbi kernel gains  <dig> x and  <dig> x speedup, respectively.

future work: the high-throughput sequence processing schema presented here will be integrated with heterogeneous computing enabled big-data processing framework running nosql database for indexed storage and retrieval of large omics data. the integrated framework will also be available for remote access via the world-wide-web.

availability and requirements
project name: cudampfproject home page:https://github.com/super-hippo/cudampfoperating system: linuxprogramming language: cuda c/c++, ptx assemblyother requirement: cuda  <dig>  or later, gcc/g++  <dig> . <dig> or later, cuda-enabled gpus with compute capability of  <dig>  or higherlicense: mit license

competing interests

the authors declare that they have no competing interests.

authors’ contributions

hj proposed the idea of simd embedded simt framework, programmed the algorithm, designed the benchmark tests, evaluated the performance and drafted the manuscript. ng programmed the algorithm, collected and analyzed the results and drafted the manuscript. both authors read, revised and approved the final manuscript.

