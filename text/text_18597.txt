BACKGROUND
the ability to rapidly collect and genotype large numbers of genetic variants has outpaced the ability to interpret such data, leaving the genetic etiology for many diseases incomplete. the presence of gene-gene interactions, or epistasis, is believed to be a critical piece of the “missing heritability” that is a hot topic in the field  <cit> . this has in turn spurred development on advanced computational approaches to account for these interactions, with varying degrees of success  <cit> . recent work has shown that gene-gene interactions capable of influencing gene expression exist and are replicable  <cit> , reinforcing the need for methodology that can account for these genetic interactions. the main computational challenge comes from the vast number of markers that are present in a typical association study. this problem is exacerbated when interactions between two or more markers must be considered. for example, given an experiment that genotypes  <dig>  markers, examining all possible interactions between two of the markers involves consideration of nearly half a million combinations. this situation becomes exponentially worse as higher order interactions are considered. modern genome-wide association studies  routinely consider 1- <dig> million single nucleotide polymorphisms , which would require examining half a trillion potential interactions. as whole genome sequencing  methods become commonplace, methods that cannot cope with large data sets will be of little utility. data on this scale will require approaches that can find interactions without having to enumerate all possible combinations.

several distinct types of methods have emerged that attempt to address this challenge. perhaps one of the most popular approaches from the last decade has been multifactor dimensionality reduction   <cit> , and extensions of the method. mdr is a combinatorial search that considers all possible interactions of a given order and selects the best model via cross validation. because mdr is an exhaustive search, it suffers from the previously discussed scalability issue, though recent work using graphics processing units has attempted to lessen this deficit  <cit> . though mdr is a general constructive-induction algorithm that can be paired with any stochastic search technique, it is most often paired with a permutation testing strategy to assess statistical significance for each marker, so the computational burden becomes prohibitive for large datasets. permutation testing computes a p-value for a statistic of interest  by randomly permuting the class labels and calculating the statistic on the permuted dataset. this procedure is repeated many times to compute a “null” distribution for the statistic of interest. the percentage of instances in the permuted null distribution that are less than or equal to the actual statistic from the unpermuted data is taken as the desired one-sided p-value. unfortunately, this can be extremely expensive for large datasets when many hypotheses are simultaneously tested, leading to a large multiple testing scenario. to get the required resolution for a bonferroni corrected p-value of  <dig>  when considering a set of  <dig>  snps, one must perform  <dig>  permutations. this makes permutation testing infeasible for even moderately sized datasets.

another popular approach is bayesian epistasis association mapping   <cit> . beam partitions markers into groups representing individual  genetic effects, interactions, and a third group representing background markers that are uninvolved with the trait. beam employs a stochastic markov chain monte carlo  search technique to probabilistically assign markers to each group and uses a novel “b-statistic” based on the mcmc simulation to assign statistical significance to each marker. this allows beam to assign statistical significance without the need to perform a costly permutation test. this method has been demonstrated successfully on data sets with half a million markers. however, the recommended amount of mcmc iterations needed is quadratic in the number of snps considered  <cit> , possibly limiting its effectiveness for larger datasets.

many popular machine-learning algorithms have also been adopted for use in analyzing association studies. notable examples are decision trees  support vector machines   <cit> , bayesian networks  <cit> , and neural networks  <cit> . in particular, tree-based methods such as random forests and boosted decision trees have been found to perform well in a variety of association study analyses  <cit> . machine learning approaches are appealing because they assume very little a priori about the relationship between genotype and phenotype, with most methods being flexible enough to model complex relationships accurately. however, this generality is something of a double-edged sword as many machine learning algorithms function as black boxes, providing investigators with little information on which variables may be most important. typically it is the goal of an association study to determine which variables are most important, so a black box may be of little use. some approaches have easy adaptations that allow them to provide such measures. both types of tree based methods  can provide measures of relative variable importance  <cit> , but these indicators lack measures of uncertainty, so they are unable to determine how likely a variable’s relative importance score is to occur by chance without resorting to permutation testing.

in this study, we propose the use of bayesian neural networks  for association studies to directly address some the issues with current epistasis modeling. while bnns have been previously developed and applied for other tasks  <cit> , they have yet to see significant usage in bioinformatics and computational biology. like most complex bayesian models, bnns require stochastic sampling techniques that draw samples from the posterior distribution, because direct or deterministic calculation of the posterior distribution is often intractable. these posterior samples are then used to make inferences about the parameters of the model or used to make predictions for new data. standard mcmc methods that employ a random walk such as the metropolis-hastings  algorithm  <cit>   explores the posterior distribution very slowly when the number of predictors is large. if d is the number of parameters in a model, the number of iterations needed to obtain a nearly independent sample is o  <cit>  for rw-mh. this makes the rw-mh algorithm unsuitable for neural network models in high-dimensions, so the hamiltonian monte carlo  algorithm is instead used to generate samples from the posterior. hmc has more favorable scaling properties, as the number of iterations needed is only o  <cit> . hmc achieves this favorable scaling by using information about the gradient of the log-posterior distribution to guide the simulation to regions of high posterior density. readers familiar with standard neural network models will notice an inherent similarity between bayesian neural networks sampled using hmc and traditional feed-forward neural networks that are trained using the well-known back-propagation algorithm  <cit> , as both take steps in the steepest direction using gradient based information. though hmc will in general explore the posterior distribution in a more efficient manner than rw-mh, the evaluation of the gradient can very expensive for large data sets. recent work has shown that this drawback can be lessened through the use of parallel computing techniques  <cit> . we demonstrate that a graphics processing unit  computational framework can enable the use of bnns on large datasets.

the bnn framework outlined here has several features designed to address many of the challenges inherent in analyzing data from association studies. these advantages are outlined below.quantification of variable influence with uncertainty measures. this allows variable influence to be assessed relative to a null or background model using a novel bayesian testing framework. this avoids reliance on a permutation testing strategy.

automatic modeling of arbitrarily complex genetic relationships. interactions are accounted for without having to examine all possible combinations. this is achieved from the underlying neural network model.

an efficient sampling algorithm. hmc scales much better than other mcmc methods, such as the rw-mh algorithm, in high-dimensions.

computational expediency through the use of gpus. the time needed to build the model is greatly reduced using the massive parallel processing offered by gpus.



we offer evidence for these claims using several simulated scenarios and a demonstration on a real dataset. in addition, we compare the proposed approach to several popular methods so that relative performance can be assessed.

RESULTS
existing methods used for comparison
we selected several methods to serve as baselines for evaluation of the bnn’s performance. as previously mentioned beam and mdr are widely used methods and so were included in our evaluation. we used a custom compiled 64-bit version of beam using the source provided on the website  <cit>  of the authors of  <cit> . the java-based mdr package was downloaded from the mdr source-forge repository  and called from within a python script. to evaluate the effectiveness of tree-based methods, we used an approach nearly identical to that in  <cit> , which was based on boosted decision trees. the boosted decision tree model provides a measure of relative influence for each variable that indicates how important a given variable is, relative to the others in the model. to fit the boosted tree model we used the gbm package in r. finally, we also included the standard  <dig> degrees-of-freedom chi-square test of marginal effects.

as discussed, some approaches such as mdr and gbm require a permutation testing strategy to assess statistical significance. this makes assessing their performance on large datasets difficult, due to the amount time required to perform the permutation test. during our pilot investigations on a dataset containing  <dig>  snps, each individual run of mdr was found to take roughly 1 minute to complete. the time needed to complete the required  <dig>  permutations would be roughly 2 weeks. if we wish to evaluate a method’s effectiveness on hundreds or thousands of such datasets, this run time becomes prohibitive. as such, we divided our primary analysis into two sections. in the first section, we evaluated methods that do not rely on permutation testing on datasets containing  <dig>  snps each. however, since we wish to compare the results of the bnn to that of mdr and gbm, we performed a second set of analyses on smaller datasets that only contained  <dig> snps each, for which permutation testing is feasible. this two-pronged strategy allowed us to evaluate a wide range of popular approaches in a reasonable amount of time, while serving to underscore the need for methods that do not rely on permutation testing.

parametric models of multi-locus relationships
in this section we performed an analysis of three biallelic models of genotypic relationships. these models have been used previously  <cit>  and are meant to reflect theoretical and empirical evidence for genetic relationships involving multiple loci  <cit> . tables  <dig>   <dig>  and  <dig> contain the relative risk of disease for each genotype combination. capital and lower case letters represent the major and minor alleles, respectively.table  <dig> 
additive risk model



genotype
aa
aa
aa

bb

bb

bb



genotype
aa
aa
aa

bb

bb

bb



genotype
aa
aa
aa

bb

bb

bb


the symbols η and θ in the tables represent the baseline risk and effect size, respectively. we simulated genotypes for the disease snps for a range of minor allele frequencies  and simulated the disease status for  <dig>  cases and  <dig>  controls using the risks given in tables  <dig>   <dig>  and  <dig>  we embedded the causal snps in a background of  <dig> non-causal snps, for a total of  <dig>  snps to be considered. for each combination of effect size, θ ∈ { <dig> ,  <dig> ,  <dig> ,  <dig> }, maf ∈ { <dig> ,  <dig>  ,  <dig> ,  <dig> ,  <dig> }, and model type  we generated  <dig> datasets. this yielded a total of  <dig>  datasets for evaluation. all datasets in this section were created using the r statistical programming language  <cit> .

we ran bnn, beam, and the χ <dig> test on each dataset and recorded whether or not both disease snps were declared as significant by each method. we took the fraction of datasets where both disease snps were correctly identified as an estimate of statistical power. for beam and the χ <dig> test, we used the canonical bonferroni corrected significance threshold of p < <dig> . we used the recommended parameter settings for beam  <cit>  and performed 1e <dig> sampling iterations for each dataset. for the bnn approach, we used a network with  <dig> hidden layer and  <dig> logistic units and a softmax output layer with  <dig> units. the network parameters in the hidden layer are given ard priors, while the network parameters in the output are given a common gaussian prior. the hyper parameters for the inverse-gamma prior for the ard parameters were α0 =  <dig>  β0 =  <dig> while the hyper parameters for the gaussian priors were α0 =  <dig> , β0 =  <dig> . the parameters for the hmc algorithm were ε = 5e- <dig>  l =  <dig>  α =  <dig> , and t = 5e <dig>  the cutoff value for the novel bayesian ard testing framework was  <dig> . we discarded the first  <dig> samples as burn-in and kept  <dig> samples to be used for inference. processing of each dataset by the bnn took approximately 3 minutes. the results are shown below in figures  <dig>   <dig> and  <dig> figure  <dig> 
additive model. estimated power to detect both disease snps using bayesian neural networks , beam, and χ
 <dig> test  with  <dig> d.f. effect sizes of { <dig> ,  <dig> ,  <dig> ,  <dig> } are shown in order from left to right, top to bottom. within each pane results are stratified by minor allele frequency .
threshold model. estimated power to detect both disease snps using bayesian neural networks , beam, and χ
 <dig> test  with  <dig> d.f. effect sizes of { <dig> ,  <dig> ,  <dig> ,  <dig> } are shown in order from left to right, top to bottom. within each pane results are stratified by minor allele frequency .
epistatic model. estimated power to detect both disease snps using bayesian neural networks , beam, and χ
 <dig> test  with  <dig> d.f. effect sizes of { <dig> ,  <dig> ,  <dig> ,  <dig> } are shown in order from left to right, top to bottom. within each pane results are stratified by minor allele frequency .



bnns were found to be uniformly more powerful than both beam and the χ <dig> test for the additive model. bnns show excellent power, even for small effect sizes and achieve 100% power for second smallest effect size across all tested mafs. in contrast, beam showed relatively little power for the smallest effect size and never achieves 100% for all mafs, even at the highest level of effect size. the threshold model tells a similar story. for all but  <dig> combinations of maf and effect size, the bnn model is again uniformly more powerful than both beam and the χ <dig> test. the picture from the epistatic model is slightly more mixed. beam appeared to do a better job at the smallest effect size, while performing equally well as bnns on the remaining three effect size levels. all three methods had almost no power to detect the causal snps for a maf of  <dig> . these results suggest that bnn is uniformly more powerful the χ <dig> test for these genetic models, and may be more powerful than beam in most instances.

simulated epistatic relationships without marginal effects
in this section, we evaluated the performance of all the methods examined in the previous section  as well as gbm and mdr. since mdr and gbm rely on permutation testing, we reduced the size of the dataset to accommodate this strategy. to generate test datasets, we used the gametes software package  <cit> . this package allows users to specify the proportion of variance for case/control status that is due to genetic variants  as well as how many loci are involved in determining trait status. these relationships are generated such that there are minimal marginal effects, resulting in relationships that are nearly purely epistatic. relationships without marginal effects are in some sense “harder” than those with marginal effects, because the causal snps contribute to trait status only through their interaction. preliminary analysis on the reduced snp datasets indicated that if the same models were used as in the previous section, most methods would have nearly 100% power for all simulated scenarios, which would provide little useful feedback for discerning which approaches were working best. this was the primary motivation for using the “harder”, purely epistatic relationships instead of the parametric models we used previously.

using gametes, we analyzed two levels of heritability  across a range of maf . power was measured as in the previous section using  <dig> instances for each heritability/maf combination for a total of  <dig> data sets used in evaluation. the results are shown below in figures  <dig> and  <dig> figure  <dig> 
purely epistatic model with 5% heritability. estimated power to detect both disease snps of bayesian neural networks , beam, χ
 <dig> test  with  <dig> d.f., gradient boosted trees , and mdr. the results are stratified by minor allele frequency .
purely epistatic model with 10% heritability. estimated power to detect both disease snps of bayesian neural networks , beam, χ
 <dig> test  with  <dig> d.f., gradient boosted trees , and mdr. the results are stratified by minor allele frequency .



bnn outperformed all methods from the previous section  by a very wide margin. this suggests that beam may be less robust to detect causal snps in the absence of marginal effects than previously thought, as it never achieves 25% power in any of the scenarios tested. again, we find these results encouraging as they indicate that bnns are indeed powerful relative to existing approaches. additionally, bnn outperformed the gbm method in all but  <dig> scenarios, indicating that bnn maybe be more adept at detecting purely epistatic signals across a broad array of mafs and effect sizes. mdr performs well across every parameter combination tested, but as mentioned previously it is incapable of performing this analysis on a gwas scale due to the exhaustive search technique and the need to perform permutation testing to assess statistical significance. to conclude this section, we note that bnn was the only method that did well across a variety of genetic models, number of snps, and mafs while being capable of scaling to gwas-sized data. this provides evidence that bnn framework is deserving of further investigation as an analysis technique for association studies.

sensitivity and specificity analysis of the ard test
the cutoff value used for the ard test has an obvious impact on the method’s performance. in the extreme case, a cutoff of  <dig> would result in nothing being significant while a cutoff value of  <dig> would result in everything being declared as such. the cutoff value controls the tradeoff between sensitivity  and specificity . evaluation of the false positive rate for the cutoff value of  <dig>  used in the previous experiments indicates that the bnn method properly controls the amount of false positives. we observed an average false positive rate  of roughly  <dig>  and  <dig>  for the parametric models and the purely epistatic models, respectively . to examine the trade off between the true positive rate  and fpr as the cutoff value is changed, we modulated the cutoff from  <dig> to  <dig> in increments of  <dig>  and recorded the true positive and false positive rate for each data set in the two previous sections. in figure  <dig>  we averaged the tpr and fpr over effect size and maf to produce a receiver-operator characteristic  curve for each of the  <dig> genetic models. the legend displays the area under the curve  for each model.figure  <dig> 
receiver-operator characteristic  curve for bnns. each line represents the roc curve for a different genetic model, averaged over effect size and maf. the area under the curve  for each model is shown in the legend.



these results show that bnn-ard test for variable importance is able to achieve a high true positive rate, while maintaining a low false positive rate, indicating the method is performing well and as expected.

analysis of genome-wide tuberculosis data
to demonstrate the performance of bayesian neural networks on a real dataset, we analyzed a gwas designed to find genetic markers associated with tuberculosis  disease progression. the dataset described in detail in  <cit> , contains information on roughly  <dig>  snps and  <dig> subjects, which we realize is a small sample size. for our study, each subject was classified as currently infected with any form of tuberculosis  or having a latent form of tb confirmed through a positive tuberculin skin test . quality control was performed and snps with missing values were excluded, as were snps that were found to be out of hardy-weinberg equilibrium at the  <dig>  level. after qc, there were  <dig>  snps available for analysis and  <dig> subjects. based on evidence of subpopulations in this data  <cit> , subjects were assigned to one of three clusters created using the top two principal components and cluster membership was included as a covariate in the model. sampling of the bayesian neural network was conducted as outlined in the previous section, with ard hyper-parameters of α0 =  <dig>  β0 =  <dig>  we performed  <dig> burn-in iterations followed by  <dig>  sampling iterations, which took approximately 20 hours. the top five snps based on posterior ard probabilities are shown below in table  <dig>  the time needed to perform the simulation is dependent upon both the dataset size and the desired number of simulations. using this dataset as an example, the sampler completed one full hmc update approximately every minute, thus on a datasets of  <dig>  and  <dig> million snps we would expect a sample every 30 minutes and every hour, respectively. we provide these numbers as rough estimates of the needed computational time per sample of data sets with larger numbers of snps.table  <dig> 
top  <dig> snps based on posterior ard probabilities 



snp
chr
pr



the snp reported as the 2nd most significant in  <cit>   appeared in our analysis as the 31st most significant snp. only one of the snps in table  <dig> is currently known to be located within a gene  according to dbsnp. every snp reported in table  <dig> is located on the same chromosome and within 10-50 mb of loci previously reported as having a statistically significant association with pulmonary tuberculosis susceptibility  <cit> . the loci reported in  <cit>  were unfortunately either not part of the original snp library or removed during the qc process in this study. due to the small sample size of this dataset, it is hard to say conclusively which of the snps reported here and in  <cit>  are most likely to replicate in a larger study. however, we present this analysis to demonstrate that the bnn framework is capable of analyzing data sets containing a high number of snps in a relatively short time.

here we explore the types of interactions between the top  <dig> snps from the real data analysis using an entropy web. the purpose of the interaction web is to visually display the nature of the interactions  amongst the  <dig> snps. the colors used comprise a spectrum of colors representing a continuum from synergy to redundancy. the colors range from red representing a high degree of synergy , orange a lesser degree, and gold representing the midway point between synergy and redundancy. on the redundancy end of the spectrum, the highest degree is represented by the blue color  with a lesser degree represented by green. the numbers indicate the entropy explained by each of the variables or variable combinations, with the weight of connections proportional to the strength of the signal. positive numbers indicate synergy between variables, while negative number indicate redundancy. this information is displayed in figure  <dig>  the figure indicates that several of the snps are indeed weakly interacting with one another , giving confidence that the method is capable of detecting relevant snps in the presence of interactions.figure  <dig> 
entropy network for the top  <dig> snps selected by the bnn.




CONCLUSIONS
in this study we have proposed the use of bayesian neural networks for association studies. this approach was shown to be powerful across a broad spectrum of different genetic architectures, effect sizes, and mafs. of the approaches that do not rely on permutation testing, bnn was uniformly more powerful than the standard χ <dig> test and almost uniformly more powerful than the popular beam method in the scenarios considered. bnn again showed a near uniformly better performance than the gbm method. mdr was very competitive with bnn in our evaluations, however mdr is incapable of scaling to larger datasets due to both its exhaustive search technique and reliance on permutation testing. in conclusion, we have demonstrated that bnns are a powerful technique for association studies while having the capability of scaling to large gwas sized datasets.

availability of code
the code implementing the gpu-based bayesian neural network framework outlined in this paper is available at https://github.com/beamandrew/bnn.

