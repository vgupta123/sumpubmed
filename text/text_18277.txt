BACKGROUND
dna microarray technology has become a standard tool in biomedical research for large-scale transcriptional monitoring  <cit> . a growing number of microarray experiments seek to compare samples labeled with two different dyes, such as cyanine <dig>  and cyanine <dig> . however, several studies report that the dyes bind on a microarray slide differently due to the variations in their chemical characteristics  <cit> . in addition, the image scanner settings also affect dye intensity measurements. should these discrepancies not be corrected, the resulting data may not be useful for analysis purposes. thus, there is a need for dye normalization for the microarray slide prior to actual data analysis to reduce systematic variability.

microarray data preprocessing contains three phases: quality control, within-slide normalization, and between-slide normalization. within-slide normalization aims to correct dye incorporation differences which affects all the genes similarly, or genes with the same intensity similarly  <cit> . one scatterplot-based normalization technique that is particularly suitable for balancing the intensities is called locally weighted scatterplot smoothing  and its original application was for smoothing scatterplots in a weighted, least-squares fashion  <cit> . this technique is typically chosen to calibrate microarray data because a popular, freely available implementation is available in the statistical software package r  <cit>  and in many commercial microarray analysis software suites such as the agilent feature extraction software. moreover, several other freely available microarray data handling packages have incorporated this normalization technique  <cit> . it is noted that many normalization studies simply call the function without rigorous consideration for the actual algorithmic parameters  <cit> . our analysis reports that the choices of different parameter values drastically affect the quality of the normalization results. the original work on lowess clearly mentions the problem of obtaining parameter values and even offers some ideas for finding suitable data-dependent choices  <cit> . however, many microarray studies have omitted such rationale and made arbitrary selections for different experimental data sets  <cit>  and some studies even failed to report their parameter assumptions in their methods  <cit> . although this practice has not lead to significant consequences for most of the parameters in lowess, we show that the parameter that represents the fraction f of neighboring samples to be included in the weighted polynomial fit is particularly sensitive and its variation greatly affects the normalization results. this parameter should be carefully chosen through a systematic procedure where experimental assumptions are clearly specified. benefits in the normalization process may be considered to be small in their own right, but these improvements are extremely meaningful in the context of searching for subtle biological differences in gene expression.

we outline an optimization-based procedure for obtaining a systematic value for f in print-tip lowess normalization. results are compared to common, arbitrary selections of f. the proposed procedure first examines a case study where we have utilized three quality filtered, self versus self hybridization experiments. with self versus self experiments, we are able to clearly detect normalization differences. such analysis also verifies that the optimized method produces properly calibrated ratios. our proposed technique is also demonstrated on a typical set of quality filtered microarray data. we utilize a set of breast cancer data that has replicated measurements for four different tumor cell lines  <cit> . in addition to visual comparisons, we quantitatively assess the performance of the different normalization procedures using a goodness-of-fit test. our results demonstrate that arbitrarily selecting the lowess bandwidth parameter produces statistically different results for certain print-tips compared to the proposed optimized parameter selection formulation. moreover, for genes that have been verified using reverse transcription-polymerase chain reaction  experiments, we show that calibrated results are substantially affected by the choice of f. our self versus self data, including the original tiff images, are available online  <cit>  and the replicated breast cancer data is posted by the original authors of that study  <cit> .

RESULTS
within-slide normalization
within-slide normalization is used to correct the dye intensity errors introduced across one microarray slide. the result of this step provides the normalized, calibrated ratios. let  denote the background corrected selection for the intensity of the jth gene of the cy <dig>  colored sample. similarly, let  denote the jth gene of the cy <dig>  colored sample. one key issue for the dyes is that they are consistently imbalanced  <cit> . different labelling effciency between the two fluorescent dyes exists and in some labelling schemes cy <dig> is systematically less intense than cy <dig>  normalization techniques are needed in order to render the gene expression levels measured by the two different dyes comparable  <cit> . dye biases can stem from a wide variety of factors, including physical properties of the dyes, effciency of dye incorporation, and processing errors. such errors may be introduced by slight variations in the amount of mrna used to create the target hybridized to each microarray or in the quantity of dye used to fluorescently label each target.

for a single microarray experiment, there are n total gene expression ratios and we denote the observed vector of ratios for a single experiment as r ∈ ℝn ×  <dig>  the calibrated ratio of expression for each gene is obtained by dividing the test by the reference sample intensities with the proper normalization factor in the denominator,



for i =  <dig>   <dig> .., n, where n is the total number of spots on a microarray. the normalization factor, denoted by Φ, is a function of data-dependent variables. if the dyes are linearly dependent, it can be assumed that the normalization function is a constant, namely Φ = φ. many studies have looked at linear dependencies  <cit> , as well as a generalized form of the normalization factor Φ that is a function of an often times unknown number of experiment-specific parameters.

many studies perform within-slide normalization in a global manner by assuming the error effects are stationary across an entire slide. this is currently true for the cases of affymetrix genechip or agilent oligonucleotide microarrays. for cdna microarrays, however, the sources of variation typically originate in a localized or spatial manner  <cit> , mainly from the different print tips for each sub-array of the slide  <cit> . the process of determining the values for Φ is highly dependent on the characteristics of the data for each print-tip  <cit> . for example, some print-tips have highly nonlinear effects, while other print-tips in the same experiment behave quite differently and may exhibit linear trends in dye bias. furthermore, the systematic manner in which the experiment has been conducted also influences the results of different slides, but it is our intention that such effects will be satisfactorily captured in the behavior of the print-tip statistics. as a consequence, we omit global calibration considerations that neglect print-tip distinction and focus solely on scatterplot-based normalization in a termed localized manner.

lowess method
one of the most widely used nonlinear correction techniques is the lowess method, which was first applied to microarray data by yang et al.  <cit> . the main idea behind lowess is to utilize a locally weighted polynomial regression of the intensity scatterplot in order to obtain the calibration factor. compared to other techniques, like housekeeping-based normalization or dye-swap experiments, scatterplot-based normalization is more robust in many types of scenarios where assumptions of constantly expressed genes may break down  <cit> . subsequent microarray studies have also chosen this method due to the robustness of fit in the presence of a few extreme outliers. original studies have examined the -scatterplot in log2-space for determining the value of Φ. it has been suggested in separate works by dudoit et al.  <cit>  and yang et al.  <cit>  that a log2-based scatterplot of the average fluorescence intensity a versus the transformed ratio m should be used instead of a simple, log2-based intensity scatterplot. this type of scatterplot is commonly known as a bland-altman plot in the statistics literature. the values for a and m are given as,





for i =  <dig>   <dig> ..., n. equations  and  are preferred over the original intensities because the -scatterplot may reveal artifacts that are not clearly visible in the ordinary intensity scatterplot. such a transformation represents a scaled, 45° rotation of the -coordinate system  <cit> .

the smoothing procedure has been designed to accommodate measured scatterplot data obeying the form mj = g + εj, where the jth transformed ratio mj is a function of the corresponding overall intensity aj and a zero mean random variable εj. the smoothed point at aj using lowess with a degree d polynomial is , where  is the fitted value of the regression. the lowess estimate, , is a weighted linear combination of the mi



where the hi depend on ai, ∀i, but not on the mi. the lowess algorithm contains four data-specific parameters, namely the polynomial order d, the number of lowess algorithmic iterations t, the weight function w, and the fraction of the data points used in the local regression f. consequently, these parameters all affect the values of the weights hi in eq. . for a complete outline of the lowess algorithm, consult  <cit> . in practice, the polynomial order for dna microarray data is usually selected as being either d =  <dig>   <dig>  or  <dig>  depending on the choice of - or -coordinate systems, the tri-cube weight function is quite standardized for all types of data  <cit> , and the number of iterations is usually fixed at t =  <dig>  the final parameter must be chosen where f ∈ . the value of each ψk,i is a function of experiment-specific parameters such as temperature or other environment settings which may differ from sample to sample in a single experiment. accordingly, the cost function to be minimized for the kth print-tip group across all transformed ratios is



with the constraint that fk ∈  is unknown, reliable estimates that reflect experiment-specific assumptions may be used. usually there are tens of thousands of genes in a microarray study and a plausible assumption is that the mean of the log2-transformed ratios after normalization is zero. also, in a variety of experiments, platform-dependent control transcripts that are known to have certain expression at a constant level may be utilized in the optimized approach. furthermore, in our breast cancer case study we show how to obtain statistically reliable estimates of ψk,i from replicate slides. we also show how our approach may be used if replicates are not available for typical microarray studies. ultimately, the optimized approach requires experimenters to explicitly state their assumptions behind the study, which is systematically better than arbitrarily choosing parameter values. in addition, determining an experiment-specific fk by trial and error may be time consuming and will oftentimes lead to non-optimal results. the chosen optimization algorithm for minimizing the corresponding cost function is based on a combination of golden-section search and successive parabolic interpolation as outlined by forsythe et al.  <cit> . this approach finds the best fk for minimizing δk for each print-tip, k =  <dig> ..., ℓ within a tolerance of ± <dig> . each print-tip, resultingly, may have a different, optimal bandwidth parameter.

normalization step
after the estimates  have been obtained, calibrating the intensities for all the ak,i is given as



for i =  <dig> ..., nk, and k =  <dig> ..., ℓ. for the local lowess normalization within each print-tip group, the issue of how the total intensities are spread about the sample mean for the group becomes a factor to consider when normalizing the data  <cit> . after normalization, all the log2-ratios from the different print-tip groups are usually centered around zero. some print-tips may have larger variances compared to others and an appropriate scale adjustment is needed to account for such differences. one proposed approach is to find the maximum likelihood estimate for the scale of the variance for each print-tip group  <cit> . this method assumes that all log2-ratios from the kth print-tip group follow a normal distribution with mean zero and variance σ <dig>  where σ <dig> is the variance of the true log2-ratios and  is the estimated scale factor for the kth print-tip group. however, this is only valid for certain types of data that reasonably follow a normal distribution and in our work we observe that this assumption may often times lead to undesirable results. refer to  <cit>  for further details.

another approach proposed here that is able to deal with the variance scaling issue is to introduce a weighting factor in the calibration function that is of the form



for i =  <dig> ..., nk, k =  <dig> ..., ℓ, and where the weight is given as . the bias-corrected sample variance for the kth print-tip is denoted by  and is given as



where  denotes the sample mean for print-tip k. furthermore, the minimum sample variance is given as



compared to the maximum likelihood method outlined by  <cit> , this method stresses higher weighting on print-tip groups that exhibit less variance and lower weighting for highly variant print-tips. if such a weight is not introduced, the normalization may improperly calibrate highly variant print-tip groups that have extreme sample means and many genes may erroneously be considered as differentially expressed as a consequence. other treatments, such as the one suggested by quackenbush  <cit>  examine the geometric mean of the tip variances as a scale factor for the normalization estimate. however, such a treatment may not always scale the tips properly since some tips may still be overly compensated. our proposed scaling factor λk takes values over , we have obtained nearly identical sample means, but less total variance for the resulting data compared to previously published techniques. the computation of λk is straightforward and easy to calculate but our novel variance stabilization procedure does not take into account any heteroscedasticity in the data, namely observed increasing ratio variance with decreasing measurement intensity a. a rigorous comparison of print-tip scaling is beyond the scope of this contribution, but it is noted that the different scaling procedures affect the overall calibration scheme.

case studies
to demonstrate the utility of our optimized lowess normalization procedure, we first utilized a set of three self versus self experiments  <cit> , bt- <dig>  mcf- <dig>  and hbl- <dig>  which were obtained using the protocols delineated in the methods section. in addition, we calibrated a set of four breast cancer cell lines  <cit> , bt- <dig>  mcf- <dig>  mda-mb- <dig>  and mda-mb- <dig>  each measured in comparison to the reference cell line hbl- <dig>  which were obtained using the protocols outlined by järvinen et al.  <cit> . for each cancer cell line, three replicate slide hybridizations were available. in order to reduce the effects of spots whose intensities are not reliable due to experimental or printing errors, we used two separate quality filtering methods and normalized the intensities after discarding values that were detected unreliable. the assessment of ratio quality was performed using the method proposed by chen et al.  <cit>  and the evaluation of spot quality was performed using the method of hautaniemi et al.  <cit> . optimized parameter selection for fk was performed and print-tip lowess normalization results are compared to the results using arbitrary choices of the parameter fk. the implementation took a few minutes to run on a standard desktop pc running matlab.

self versus self experiments
self versus self experiments provide a trivial application to test our method since the amount of mrna in both the test and the reference samples is the same. thus, the points of an intensity scatterplot in the log <dig> - log <dig> space should be distributed along a straight line that intersects zero with a slope of unity. in the -coordinate system, all values of m should lie on a straight line at m =  <dig> for all values of a; this means that the calibrated ratios should ideally be unity for all variables. correspondingly, the cost measure is given when ψk,i =  <dig>  , in eq.  for the -coordinate systems. separate trials were conducted using weighted, zeroth-order , first-order , and quadratic  polynomial fits. for all trials, the number of print-tip lowess iterations was fixed at t =  <dig>  the weight function used is given by cleveland  <cit> . for each experiment, the local print-tip groups were separately normalized with their respective, optimized values of fk. as a comparison to arbitrary selections of fk, the print-tip normalization was also carried out using fk =  <dig> ,  <dig> ,  <dig> , and  <dig>  in separate trials. figure  <dig> shows the , m)-scatterplot comparison between the calibration results with d =  <dig> using optimal fk and arbitrary fk for the bt- <dig> self versus self experiment. the points that deviate from the blue line are the genes that are most affected by the choice of fk. the m data in this figure was calibrated using fk =  <dig> , ∀k.

in all three self versus self experiments, the global sample means of m were nearly the same after calibration, regardless of the choice of fk. however, the calibrations that used optimized selections of fk for each print-tip resulted in data that contained less overall variance compared to the arbitrary selections. the ultimate goal of calibration is to adjust the dynamic range for the transformed ratios and reduce the variability within the data. by using optimized selection of fk, we outperform all arbitrary formulations to achieve these goals.

typical microarray experiments
one immediate concern for typical experimental microarray data is that many genes may be over- or under-expressed and the true, transformed gene expression ratio ψk,i surely will not be equal to zero for all genes. accordingly, implementing the cost function in eq.  becomes an immediate challenge since the normalization reference level of all the genes for a typical microarray experiment may be diffcult to determine with complete accuracy. we note that our cost function still may be used with the assumption that the sample mean for each print tip before log2-transformation is unity. in most microarray experiments, many genes may be assumed to have constant rna concentrations while smaller numbers of genes may be over or under expressed, namely their sample mean over all the genes is zero, . using this assumption in eq. , our experiments show that by minimizing the cost function in this context, like in the self versus self case study, we are able to systematically choose fk and the only consequence is that the minimum of the cost will not be as low as in the self versus self scenario where all genes should be constantly expressed. the main benefit of utilizing lowess for microarray normalization is that it is robust to extreme outliers and the cost function implemented in this fashion further restricts the effects of such extreme points in the regression. ultimately, this implementation results in reliably calibrated ratios compared to the arbitrary formulation where different choices of fk affect the resulting data.

since a single microarray experiment represents an observation, multiple observations would be needed to compute a reliable estimate of the true transformed ratio values. the use of only a small number of replicate slides may be satisfactorily used to determine reliable estimates of true gene expression and one study showed that three replicates suffce for significantly reducing experimental variability  <cit> . with the growing number of publicly available microarray data, conducting replicate experiments is becoming a popular solution to assess experimental errors and reduce noise bias in the measurements  <cit> . the advantages of replicate slides also greatly help the analysis of between-slide variability and help address formal statistical considerations when drawing biological conclusions. here, we show that the optimized normalization approach may be directly extended in an iterative manner to use the estimates of the true ratio values for further specifying fk. after an initial round of optimized lowess normalization for each replicate slide with ψk,i =  <dig> in eq. , the sample mean for each gene may then be calculated using the replicates. the normalization reference levels ψk,i were reassigned these average gene expression values in eq. . each experiment was then separately calibrated a second and final time using the optimization approach and the final results were noticeably different compared to the normalized data using f =  <dig>  that järvinen et al. posted on their website  <cit> . a noteworthy consideration to address here is the overall effect of an iterative calibration process on the underlying structure of the data. experimentally, once the optimized lowess regression is computed using the average value for each gene and normalization is performed, subsequent calibration attempts using the cost function-based method do not result in drastically different data. the subsequent regressions are nearly constant lines near m =  <dig> in the -scatterplot if the cost function approach is used. consequently, the calibrated data reach a stable domain with a small dynamic range. empirically, we found that performing optimized normalization in an iterative manner will not propagate regression effects through to disrupt the underlying structure of the data.

as further illustration of the calibration differences between the optimized and arbitrary calibration results, we employ a goodness-of-fit test  <cit> . we wish to make a direct test of the data, independent of any underlying parent distribution of the ratios, and we use the following statistic for the kth print-tip group



where m and m are the arbitrary and optimized calibration results, and the denominator within the summation is simply the variance of the difference between m and m. the null hypothesis is defined to be h0: the normalized ratios using arbitrary f are comparable to ones using optimized f. we tested against p <  <dig>  for the  distribution and reported the alternative hypothesis for a few print-tip groups on almost all the slides. in this analysis, we compared optimized choices of f for each print-tip to the arbitrary choices f =  <dig> ,  <dig> ,  <dig> , and  <dig> . by looking across each replicate of the calibrated data for all four breast cancer cell lines, almost all slides in this study reported at least one print-tip to have statistically different calibration results based on the choice of fk. often times a single slide would report two or three print-tip groups that had statistically different calibration results.

in addition to statistical analysis, genes that exhibit known over-expression in the bt- <dig> cell line data  <cit>  were selected here for more detailed analysis. in particular, genes that were verified experimentally using reverse transcription-polymerase chain reaction  were of the highest interest. comparing our optimized calibration results utilizing the replicate data to the normalized data by järvinen et al.  <cit> , our results conform strongly with most of the over-expressed genes given in a list from a parallel study  <cit> . two genes in particular stand out to demonstrate the benefits of utilizing our proposed method: homeo box b <dig>  which was validated with rt-pcr  <cit> , and v-erb-b <dig>  which is known to be over-expressed in the bt- <dig> cell line  <cit> . the results posted by järvinen et al.  <cit>  for calibrating the homeo box b <dig> gene shows that it falls within the top 18% of overall gene expression, but by using the optimized approach we report it to be within the top 13%. for the v-erb-b <dig> gene, both calibration techniques report that this gene falls within the top 1% of the genes in terms of expression. as a result, for the homeo box b <dig> gene, the calibration factor fk is responsible for about 5% change in the reported gene expression. this is a dramatic result that may influence how the expression for this gene may be interpreted in comparison to the accepted biological knowledge of a certain experiment. as public data from microarray experiments continues to become available, the knowledge of certain genes will undoubtedly be uncovered for well-studied cell lines and this information will help further assess normalization and microarray quality control tasks.

CONCLUSIONS
the lowess method has recently been applied in other applications for the biological sciences. comparative genomic hybridization  is a molecular cytogenetic method of screening a tumor for genetic changes. the alterations are classified as dna gains and losses and they reveal a characteristic pattern that includes mutations at chromosomal and subchromosomal levels. our proposed optimized scheme is directly applicable to the application of calibrating cgh microarray experiments, as well as for data analysis aspects. for example, the work of clark et al.  <cit>  utilized the lowess method for identifying the regions where gene copy numbers were aberrantly high or low in prostate cancer using cgh microarray technology. the parameter f was chosen arbitrarily and its value was not reported in the study. consequently, reproduction and verification of these results may be diffcult. for instance, some of the important biological findings, such as start and end points of amplifications and deletions, may be adversely affected by different choices of f.

in addition to cgh analysis, lowess has found application in case-control studies where logistic regression has been used to model the relationship between binary responses and continuous predictor variables  <cit> . in these types of studies one may use lowess to remove systematic trends that contaminate the laboratory measurements of predictor variables. the analysis reported by borkowf et al.  <cit>  clearly shows that different choices of f result in noticeably different correction effects and the optimization method proposed here may be suitable for enhancing such a study. adaptations to the cost function may be utilized to handle this type of data. in addition, analysis of other types of scatterplot data by utilizing the lowess method with an arbitrary choice for the bandwidth parameter is undoubtedly susceptible to varied interpretations or errant conclusions  <cit> .

another result of this optimized calibration study is that we uncovered a better understanding of choosing the parameter d in the weighted polynomial fit. a higher-order , weighted polynomial is rarely needed based on the argument that such an assumption is, to a certain extent, over-fitting the data. from the findings of our study, we find that it is better to use a linear estimate based on minimizing the estimate errors across -scatterplots. consequently, different choices of d resulted in different optimized values for f. the reason is that for the higher-order polynomial, it is beneficial in general to retain a larger fraction of the values of a for the weight function in computing the polynomial coeffcients. it is very important to carefully select f since ultimately, the bandwidth is a function of the polynomial order.

here, we also reaffirmed the idea that the quality filtering of ratios and spots is a necessary step that should precede all experimental microarray data handling procedures, whether it is scatterplot-based normalization or any other normalization method, since errant ratios would surely have a deleterious affect on the calibration. for instance, in the bt- <dig> data, the first replicate slide had poor ratio quality for a handful of genes. calibration without considering or removing these errant spots resulted in less reliable results. this study addresses the issue of locating sources of experimental error for print-tips that have high sensitivity for the parameter f . for one, print-tips are physically different and they are considered to have different types of errors introduced based on these properties. in the formulation of normalization, it is imperative to address such subtle issues when choosing and implementing any algorithm.

the systematic choice of the parameters in the lowess algorithm has not been previously addressed in the microarray literature and the method proposed here may be utilized in different microarray platforms. such a treatment is also important for a wide variety of applications that employ scatterplot-based regression. the findings of this study illustrate that by choosing different values of f for the lowess algorithm results in noticeably different normalization results. this proposed method requires the calibration step to clearly state the assumptions used for within-slide normalization. our optimization algorithm is more systematic than simply choosing an arbitrary parameter value or through trial and error techniques since the optimized approach relies on the actual underlying structure of the data. we also stress that such an optimization algorithm may also be utilized for other studies in addition to dna microarray normalization treatments. proper changes need to be made to eq.  to reflect the ideal model for the data captured in the function ψk,i, but in some studies, such a function may be satisfactorily determined or estimated from the data.

