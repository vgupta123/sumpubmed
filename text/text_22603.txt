BACKGROUND
a daunting challenge faced by environmental regulators in the u.s. and other countries is the requirement that they evaluate the potential toxicity of a large number of unique chemicals that are currently in common use  but for which little toxicology information is available. the time and cost required for traditional toxicity testing approaches, coupled with the desire to reduce animal use is driving the search for new toxicity prediction methods  <cit> . several efforts are starting to address this information gap by using relatively inexpensive, high throughput screening approaches in order to link chemical and biological space  <cit> . the u.s. epa is carrying out one such large screening and prioritization experiment, called toxcast, whose goal is to develop predictive signatures or classifiers that can accurately predict whether a given chemical will or will not cause particular toxicities  <cit> . this program is investigating a variety of chemically-induced toxicity endpoints including developmental and reproductive toxicity, neurotoxicity and cancer. the initial training set being used comes from a collection of ~ <dig> pesticide active ingredients for which complete rodent toxicology profiles have been compiled. this set of chemicals will be tested in several hundred in vitro assays.

the goal of screening and prioritization projects is to discover patterns or signatures in the set of high throughput in vitro assays  that are strongly correlated with tissue, organ or whole animal toxicological endpoints. one begins with chemicals for which toxicology data is available  and develops and validates predictive classification tools. supervised machine learning  approaches can be used to develop empirical models that accurately classify the toxicological endpoints from large-scale in vitro assay data sets. this approach is similar to qsar , which uses inexpensive calculated chemical descriptors to classify a variety of chemical phenotypes, including toxicity. by analogy, one could use the term qbar  to describe the use of in vitro biological assays to predict chemical activity. the qbar strategy we describe here is also related to biomarker discovery from large-scale -omic data that is used to predict on- or off-target pharmacology in drug development, or to discover accurate surrogates for disease state or disease progression.

the qbar in vitro toxicology prioritization approach faces a number of inter-related biological and computational challenges. first, there may be multiple molecular targets and mechanisms by which a chemical can trigger a biological response. assuming that these alternative biological mechanisms of action are represented in the data, multiple techniques  may be required to discover the underlying relationships between bioassays and endpoint activity. second, our present understanding of biological mechanisms of toxicity  is relatively limited, so that one cannot a priori determine which of a set of assays will be relevant to a given toxicity phenotype. as a consequence, the relevant features may be missing from the data set and  irrelevant features may be included. here, by relevant features we mean data from assays that measure processes causally linked to the endpoint of interest. by extension, irrelevant features include data from assays not causally linked to the endpoint. the presence of multiple irrelevant assays or features must be effectively managed by ml methods. third, due to the high cost of performing the required in vivo studies, there are limited numbers of chemicals for which high quality toxicology data is available, and typically only a small fraction of these will clearly demonstrate the toxic effect being studied. the small numbers of examples and unbalanced distribution of positive and negative instance for a toxicological endpoint can limit the ability of ml methods to accurately generalize. in order to develop effective qbar models of toxicity, these issues must be considered in the ml strategy.

four critical issues for evaluating the performance of ml methods on complex datasets are:  the data set or model;  the set of algorithms evaluated;  the method that is used to assess the accuracy of the classification algorithm; and  the method that is used for feature selection. in order to address the first issue, it was necessary to develop a model of chemical toxicity that captured the key points of the information flow in a biological system. the mathematical model we use is based on the following ideas.

 <dig>  there are multiple biological steps connecting the initial interaction of a molecule with its principle target and the emergence of a toxic phenotype. the molecular interaction can trigger molecular pathways, which when activated may lead to the differential activation of more complex cellular processes. once enough cells are affected, a tissue or organ level phenotype can emerge.

 <dig>  there will often be multiple mechanisms that give rise to the same phenotype, and this multiplicity of causal mechanisms likely exists at all levels of biological organization. multiple molecular interactions can lead to a single pathway being differentially regulated. up-regulation of multiple pathways can lead to the expression of the same cellular phenotype. this process continues through the levels of tissue, organ and whole animal. one can think of the chain of causation between molecular triggers and endpoints as a many-branched tree, potentially with feedback from higher to lower levels of organization.

 <dig>  the number of assays one needs to measure is large, given our relative lack of knowledge of the underlying mechanism linking direct chemical interactions with toxic endpoints.

 <dig>  the number of example chemicals for which detailed toxicology information is available is relatively limited due to the high cost of generating the data. in most cases, if a chemical is known to be significantly toxic, further development and testing is halted, so it is unusual to have complete, multi-endpoint toxicity data on molecules that are toxic for any given mode. a corollary is that the number of positive examples for any given toxicity endpoint will be very limited, rarely making up more than 10% of all cases. this will limit the power to find true associations between assays and endpoints. a related issue is that most publicly available data sets that one can use for toxicology modeling are heavily biased toward positive or toxic chemicals, because much less public effort is put into performing extensive studies on chemicals that are negative examples. the toxcast data set is addressing this selection bias by gathering complete data from a set of chemicals without regard to their ultimate toxicity.

 <dig>  the available toxicity endpoint data tends to be categorical rather than quantitative. this is due to the nature of the in vivo experiments used to evaluate chemical toxicity. typically, too few animals are tested under any given condition to pinpoint the lowest effective dose or the rate of phenotypic toxicity at a particular dose. instead, if a toxic effect is seen at a rate statistically above that seen with a negative control, the chemical will be classified as causing that toxicity.

we have developed a simple simulation model which takes into account these ideas. here we motivate the structure of the model, while the methods and results sections provide details. we will illustrate the ideas behind our model with the multiple known pathways that can lead to rodent liver tumors. several nuclear receptors, including car , pxr  and ahr , when activated by a xenobiotic, can upregulate a common set of phase i, phase ii and phase iii metabolizing enzyme pathways  <cit> . each of these pathways can, when continually activated, lead to cellular phenotypes that include cell proliferation, hypertrophy and cell death. a second, parallel route is activated by compounds that bind to pparα  and lead to cellular hypertrophy and cellular proliferation  <cit> . in a third mechanism, chemicals can directly interact with dna, causing the activation of dna damage repair pathways, which can in turn lead to cell death and cellular proliferation. all three of these cellular phenotypes are potential precursors to liver tumors  <cit> . this collection of interconnected direct molecular targets, target-induced pathways, cellular or tissue phenotypes, and their connections to the endpoint of liver tumors are illustrated in figure  <dig> 

our model also assumes that a given chemical can interact with multiple molecular targets. it is well known that many drug compounds interact with multiple targets, as reflected in the phenomenon of off-target toxicity. relevant to the pathways shown in figure  <dig>  moore et al showed that there are compounds that simultaneously activate both car and pxr pathways  <cit> . preliminary data from the toxcast program allows us to quantify the magnitude of this multi-target effect. from a set of  <dig> biochemical targets , the  <dig> toxcast chemicals <cit>   were active against an average of  <dig>  targets with a maximum of  <dig>  a minimum of  <dig> and a standard deviation of  <dig> .

the connections shown in figure  <dig> are not deterministic but instead depend on multiple factors including the strength and duration of the initial chemical-target interaction. some pathways are more likely than others to lead to the manifestation of particular cellular processes, and some cellular processes are more likely than others to lead to liver tumors. based on this, one could assign a probability or strength to each arrow in figure  <dig>  the probability that a given chemical will cause liver tumors is then a complex integral over the individual step-to-step probabilities, modulated by the target interaction strengths for the particular chemical.

there is a vast literature on the evaluation of the performance of different ml methods, but for the present application the literature concerning the analysis of microarray genomics data sets and for qsar applications are most relevant. here we describe a pair of representative studies. ancona et al.  <cit>  used three algorithms , regularized least squares , support vector machine ) to classify microarray samples as either tumor or normal. they examined the number of training examples that would be required to find a robust classifier. in their example, svm and rls outperformed wvm. statnikov et al. studied all of the major classification issues in the context of multi-category classification using microarray data in cancer diagnosis  <cit> . they compared multi-category svm , k-nearest neighbors  and several artificial neural network  implementations and showed that mc-svm was far superior to the other algorithms they tested in their application.

the literature on machine learning methods in qsar is equally vast and extends back for  <dig> years or more. much of this work  is focused on the  task of predicting activity against molecular targets. a representative approach to target interaction prediction is the paper by burbridge et al. comparing svm to several other algorithms for the prediction of binding to dihydrofolate reductase  <cit> . lepp et al performed a similar study that showed svm performed well in finding predictive qsar models for a series of  <dig> molecular targets  <cit> . the recent state of the science for predicting whole animal toxicity using ml and qsar methods were reviewed by helma and kramer  <cit> , benigni and giuliani  <cit>  and by toivonen et al.  <cit> . they describe the outcome of an experiment  in which  <dig> groups submitted  <dig> models using a training set of  <dig> ntp compounds for which mouse carcinogenicity data was available. the goal was to predict the carcinogenicity of a set of  <dig> test compounds. only  <dig> of the  <dig> models performed better than random guessing and the highest positive predictive value for these was 55%, and this model had a false positive rate of 37%. these  <dig> models <cit>  include rule-based methods using chemical fragments plus calculated physicochemical properties, a decision tree model, and one using a voting scheme across several standard ml methods. it is difficult to draw many conclusions about the performance of ml methods from this exercise, which failed to produce significantly predictive methods. the authors of these reviews speculate that the cause is a combination of toxicity data being too noisy, the training and test chemical spaces being too large, and structure based approaches being inadequate to predict phenotypes as complex as whole animal toxicity.

one of the key issues in systematically comparing the performance of ml methods is that of estimating accuracy in an unbiased way. for example, ntzani and ioannidis  <cit>  report that many of the early studies using microarray data to classify tumor samples did not perform appropriate cross validation, which has led to inflated predictions of classification accuracy. this observation prompted our use of independent validation sets. molinaro et al. showed that 10-fold cross validation performed well for assessing accuracy of genomics classifiers  <cit> . leave one out cross-validation  typically performed somewhat better, but had a significantly higher computational cost. this was assessed by molinaro et al. in the context of using linear discriminant analysis , ann, diagonal discriminant classifiers , classification and regression trees  and ensemble classifiers. the molinaro study data set , which used simulated genomics data, was similar in size to the present work. baldi, et al.  <cit>  have systematically addressed the issue of ml performance metrics. they describe a number of accuracy metrics including the balanced accuracy or q-score we use in this paper. the q-score is the average of the sensitivity and specificity. this is most useful in the case where the classification variable is dichotomous and where the number of positive and negative cases in a training set is not well balanced. they also emphasize that the actual prediction accuracy is related to the similarity of the training and test set.

finally, sima and dougherty examined the issue of finding an optimal subset of features with which to train a classification algorithm  <cit> . they compare sequential floating forward search  <cit>  and t-test feature selection. this latter can fail when variables are only predictive when they act together. these authors' basic conclusion is that there are optimal subsets of features, but that poor classification performance can be due to either a failure to find an optimal subset or to the inability of any subset to allow accurate classification. this study examined svm, knn  and lda as classification algorithms. these authors suggest that automated feature selection methods have inherent limitations and that one should use biologically-based selection when possible. baker and kramer used the nearest centroids rule to select small subsets of genes that could be used as robust classifiers from genomics data sets  <cit> . kohavi assessed the behavior of cross validation methods to assess classifier accuracy for the c <dig>  and naïve bayes algorithms  <cit> . this author concludes that k-fold cross validation with k =  <dig> provides a good estimate of classification accuracy balanced against modest computational requirements.

in summary, the goal of the analyses we present is to evaluate a machine learning approach to develop classifiers of in vivo toxicity using in vitro assay data. in order to develop an appropriate ml strategy, we generate simulated qbar data using a mathematical model whose structure and parameters are motivated by an idealized biological response to chemical exposure based on the following concepts:  chemicals interact with multiple molecular targets;  exposure to chemicals can stimulate multiple pathways that lead to the same toxicological endpoint; and  there are multiple levels of biological organization between the direct molecular interaction and the "apical" endpoint. additional parameters for generating simulated data include model complexity, the level of noise in the features, the number of chemicals to be screened and the number of irrelevant features. we focus on the special case where there is a large imbalance between the fraction of positive and negative examples, which is found to be the case from our toxicological data  <cit> . the performance of ml methods is analyzed as a function of these parameters.

RESULTS
we evaluated the performance of different ml methods on simulated data sets generated by a biologically motivated analytic model. data sets were simulated based on two levels of complexity; the number of irrelevant assays or input features in the data ; the number of chemicals or instances; and the presence or absence of measurement noise in the data. in all cases, all of the relevant features  were included in the data set.

the network depiction of the simulation models s <dig>  and s <dig>  are illustrated in figures  <dig> and  <dig>  these networks closely resemble the one shown in figure  <dig>  which models the connections leading from direct molecular interactions with dna and a variety of nuclear receptors and to liver tumors. structurally, the simulation models are feed-forward networks that causally link direct molecular interactions  with a final organism-level toxicity endpoint, by way two levels of intervening biological processes. direct molecular interactions trigger pathway processes  which in turn trigger cellular processes . only if the cellular processes are activated to a sufficient level is the final endpoint manifested. of equal importance is the fact that many assays will be measured that are not causally linked to the endpoint. these irrelevant nodes are termed r-nodes for random. our simulations typically include many more r than m nodes or features. rules for linking molecular interaction strengths to the endpoint are described in the methods section. the essential points for the present discussion are that a given chemical can interact with one or more input nodes and that the spectrum of input interactions uniquely determines the value of the endpoint.

the performance of lda , knn , svm , ann , nb  and rpart  was evaluated both with and without filter-based feature selection, using 10-way cross-validation testing, as well as validation with independent data sets which included  <dig> instances. for each set of conditions , training was carried out on  <dig> independent samples drawn from a simulated data set of  <dig>  chemicals. for all evaluations, 10% of the chemicals were positive and 90% were negative for the endpoint being predicted. as mentioned previously, this imbalance between positive and negative examples reflects the situation with the data sets we are modeling in which the adverse phenotypes being studied are rare. predicted performance was evaluated using k-fold cross-validation with k =  <dig>  for each of the  <dig> samples, we recorded the number of true positives , false positives , true negatives  and false negatives , sensitivity and specificity and the balanced accuracy or q-score, which is the average of the sensitivity and specificity. to independently test the performance of the ml method, an independent validation set was drawn from the simulated data set and evaluated with the classification models for each of the  <dig> training sets. the results  from these  <dig> data sets were also saved. the approach is outlined in figure  <dig> 

the overall performance results of the different ml methods for the independent validation tests are shown in figure  <dig>  all results in this figure are calculated using model s <dig>  for the case where the training and validation sets contained  <dig> chemicals or instances. each panel shows the q-score trend for the ml methods as a function of the number of features included. horizontal lines are drawn at q =  <dig> , which is a point that guarantees at least 80% sensitivity and specificity, and at q =  <dig> , which occurs when sensitivity =  <dig> . the far left point is the case where only the causal features are used. error bars  are given for the lda results to provide an estimate of the level of variation. the other methods showed similar levels of variation. the figure shows the q-score curves as a function of increasing number of irrelevant input features in four blocks. in each block, each curve shows the q-score for one ml method beginning with just the causal features  and then increasing the number of irrelevant features until nfeature =  <dig>  in the first block, the curves generally show a decrease in performance going from nfeature =  <dig> to nfeature =  <dig>  which means that the accuracy of all learning methods generally decreased as irrelevant features were added.

the response of different ml methods to the addition of noise varied: lda and ann performed the best with only causal features, while with the maximum number of irrelevant features ann, nb and svm performed the best and lda the worst, at least in the absence of feature selection. with the exception of lda, the performance of different ml methods stabilized after around  <dig> irrelevant features. with the maximum number of irrelevant features the classification accuracy of knn and rpart were intermediate between that of the highest group  and the lowest .

the second block from the left shows the classification accuracy of the ml methods without feature selection but with the addition of measurement noise. with no irrelevant features the classification accuracy of all ml methods was significantly lower than in the absence of noise, as expected. lda showed the same maximum negative performance trend with the addition of irrelevant features. the main difference from the previous case  was that the performance of knn  was close to that of ann, nb and svm as the number of irrelevant features increased. as before, rpart and knn  did not perform well. in general, the classification performance of lda degraded the most with addition of noise while other methods remained more stable.

the third block from the left shows the classification accuracy of the ml methods with filter-based feature selection  in the absence of noise. comparing the performance of the ml methods with the first block , most ml methods performed better with feature selection but their overall ranking was the same. the exception was lda, which showed the greatest improvement in performance, tied with svm and ann with the greatest q-score. feature selection also decreased the overall variability in classification performance between the different ml methods.

the fourth and final block represents the performance results for the ml methods with noise and the use of t-test feature selection. compared with block  <dig>  where feature selection was not used, the performance of most ml methods increases slightly. lda showed a significant increase in performance. compared with block  <dig>  the performance of all techniques was significantly lower when irrelevant features were added. overall, lda, nb, svm, ann and knn  were quite stable i.e. their performance did not vary tremendously with the addition of noise and irrelevant features.

an alternate way to examine the data is to fix the number of features and look at trends as a function of number of chemicals sampled. these curves  display the expected trends that as the number of chemicals increases, there is a corresponding improvement in performance. the effects of the variant conditions are basically the same as has already been shown.

this data is compiled for the special case where  <dig> chemicals were used, as a function of model, feature selection and level of measurement noise. the results are organized into  <dig> blocks, corresponding to the  <dig> blocks in figure  <dig>  within a block, rows are ordered by decreasing values of q-score. the results give the average sensitivity, specificity and q-score along with their corresponding standard deviations. all ml methods were trained using  <dig> chemicals. the values come from  <dig> independent validation runs with unique samples of  <dig> chemicals. values of sensitivity, specificity and q-score >  <dig>  are bolded. rows where the q-score is less than that of the best q-score in the block minus one standard deviation for the best row are shaded.

discussion
developing predictive classifiers for complex biological data sets is a challenging problem because there are generally more features than instances ; the classification variable and input features are noisy; and there are many irrelevant features . we have developed a test bed for representing biologically motivated models and have used it to provide insight into the relative classification performance of different ml methods. though true in vitro biological systems are more complex and dynamic than our model, our approach provides empirical insight into the relative performance of different learning methods as a function of the absence and presence of experimental noise and the number of features. in particular, we have focused on the situation which is common in toxicology data sets, namely where there is an imbalance between the number of positive and negative examples.

we find several main trends from our simulated data by systematically analyzing different ml methods on the same testing, training and validation data. first, most ml methods perform well in the presence of a small number of causal features, but most show significant degradation in performance as irrelevant features are added, which is well-known  <cit> . second, all ml methods perform better with filter-based feature selection as irrelevant features are added. third, the performance depends upon noise in the input features. while most ml methods perform well in the absence of noise, some are more stable than others. fourth, in the presence of noisy and irrelevant features, and with feature selection, most ml methods perform similarly, with the exceptions of rpart and knn  which performed significantly worse. the models  resemble generalized artificial neural networks, leading one to suspect that ann methods should perform well. in general this is true, although  other methods always performed at least as well.

we found that the accuracy predicted using k-fold cross validation was statistically indistinguishable from that seen with an independent validation set except in the case of knn  with no feature selection. in this case, the k-fold cross validation predicted a higher accuracy than was seen with independent validation. this is the only situation where we detected over-fitting using the training data. this phenomenon disappeared when we tested knn against a more balanced data set in which there were equal numbers of positive and negative examples. all other parameters were unchanged. issues arising from unbalanced data sets have been previously analyzed. japkowicz et al. found that classifier performance of imbalanced datasets depends on the degree of class imbalance, the complexity of the data, the overall size of the training set and the classifier involved  <cit> . sun et al. also observed that given a fixed degree of imbalance, the sample size plays a crucial role in determining the "goodness" of a classification model  <cit> . the knn method is sensitive to imbalanced training data  <cit> , and the class distribution of our simulation data is highly skewed with the positive to negative rate of 1: <dig>  thus the sample size very likely explains the different performance between the training and validation sets.

one of the important limitations of this work is that the performance of classifiers is biased by our model of chemical-induced bioactivity and toxicity. we assume a static deterministic model of a biological system without feedback. an important aspect of the chemical simulation model is the use of multiple chemical classes, each of which contains a collection of chemicals that behave similarly . as described in the methods section, a chemical class is defined by first creating an example of the class by randomly drawing values for the assay values from a gamma distribution. the other members of the class are created through the addition of normally distributed random values to each assay value of the chemical class exemplar. this process creates a set of  clusters in the high dimensional feature space. we then draw samples of chemicals from a wide space in which some clusters contain no chemicals that are positive for the endpoint, some contain only positive cases, and some clusters contain a mix of positive and negative cases. one can qualitatively see how this clustering of feature space will affect the difficulty of the classification problem by projecting the chemical space into  <dig> dimensions and examining how positive and negative cases cluster. figures  <dig> and  <dig> shows these projections for model s <dig>  and s <dig>  using only the causal feature values for the distance calculation. projection was performed using multidimensional scaling . pca gives similar results. for model s <dig> the problem is approximately separable, so almost any method should perform well. when all features are included , there is more mixing, but still the positive cases tend to cluster together, so cluster identity  should be a good surrogate for classification. in model s <dig> there is still a reasonable separation for the case when only the causal features are included. the presence of a few outlying, all-positive clusters is very obvious when all features are included.

we have focused on the performance of single classifiers, but voting methods which combine the predictions of multiple individual methods have been used. statnikov et al. studied ensemble classifiers or voting schemes, which attempt to combine multiple sub-optimal classifiers to improve overall accuracy. that paper evaluated the utility of selecting very small subsets of genes  for classification. this has the effect of greatly reducing the danger of over-fitting from small numbers of samples. additionally, these authors demonstrated how to evaluate the comparative performance of different algorithms using permutation testing. two conclusions from the statnikov et al. work on cancer diagnosis using microarray data are relevant to the present study. first, they observe that svm methods outperformed knn and ann. our findings show that the relative rankings of these  <dig> methods is a complex function of the number of irrelevant features, the level of noise and the use  of feature selection. second, the authors observed that the ensemble classification methods tended to do worse than single methods. although we did not evaluate the performance of ensemble based classification, our results  do not suggest that voting would lead to a decrease in performance, as long as the voting rule was that the chemical was labeled positive if any method predicted it to be positive.

the present work limited the number of chemicals to  <dig> and features to  <dig>  which corresponds to the number of chemicals and assays we are using in the first phase of the toxcast program. despite the relatively small size of the data set, we were able to evaluate key issues in supervised learning from noisy and irrelevant data. we plan to expand the number of features and instance in future work as we gain additional insights from experimental data. additionally, we intend to more fully explore the use of dimensionality reduction , feature selection and classifier ensembles in future work.

CONCLUSIONS
the prediction of chemical toxicity is a significant challenge in both the environmental and drug development arenas. gold standard in vivo toxicology experiments in rodents and other species are very expensive and often do not directly provide mechanism of action information. the alternative, which has been widely pursued in the pharmaceutical industry, is to screen compounds using use in vitro or cell based assays and to use the results of these assays to prioritize compounds for further efficacy and safety testing. these in vitro screening techniques are now being introduced in a significant way into the world of environmental chemical safety assessment. here, there are unique challenges due to the modest amount of in vivo toxicology data that can be used to develop screening models, and due to the broad chemical space covered by environmental chemicals whose toxicology is poorly characterized. the epa is carrying out a significant screening and prioritization program called toxcast, whose eventual aim is to screen a large fraction of the commonly used environmental chemicals and to prioritize a subset of these for more detailed testing. the present analysis provides a novel simulation model of the linkage between direct chemical-target interactions and toxicity endpoints, and uses this model to develop guidelines for using ml algorithms to discover significant associations between in vitro screening data and in vivo toxicology.

we find several main trends from our simulated data set by systematically analyzing different ml methods on the same testing, training and validation data. first, most ml methods perform well in the presence of a small number of causal features, but most show significant degradation in performance as irrelevant features are added, which is well-known  <cit> . second, all ml methods perform better with filter-based feature selection as irrelevant features are added. third, while most ml methods perform well in the absence of measurement noise, some are more stable than others. fourth, in the presence of noisy and irrelevant features, and with feature selection, most ml methods perform similarly well, with the main exceptions being rpart and knn which underperformed the other methods.

