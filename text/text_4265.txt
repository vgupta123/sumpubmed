BACKGROUND
scientific interest in environmental microbial and viral communities is growing with every year. metagenomics is an approach widely used to characterize microbial and viral communities for ecological studies and viral discovery across a wide range of environments such as marine, insects, plants, animals, and human  <cit> . the methodologies for metagenomic studies have been developed and refined throughout the years based on the characteristics of the samples. however, the methodology for characterizing rna viral communities remains challenging, especially from small volume biological samples such as blood plasma, swab samples, and tissue biopsy, due to limited quantity and quality of the sample, as well as the low number of viral particles in these samples.

a typical metagenomic approach starts from the purification of viral particles coupled with the removal of the host and environmental materials, followed by viral nucleic acid extraction, sequence-independent amplification, and sequencing  <cit> . metagenomic sequences can be generated in high quantities using next-generation high-throughput sequencing technologies such as the genome sequencer flx system . the immense amount of metagenomic data produced today requires an automated approach for data processing and analysis. major steps of a typical sequence processing pipeline include sequence cleaning, fragment assembly, clustering, taxonomic assignment, and estimation of the community composition. the sequence cleaning step is an essential first step of the sequence processing pipeline before any further data processing in order to allow accurate downstream analysis. for metagenomic datasets, the sequence cleaning step usually includes filtering of duplicated reads, short reads, low quality reads, contaminations, and reads containing ambiguous bases  above a certain threshold.

generating rna viral-associated metagenomes may require the use of reverse transcriptase-mediated cdna library synthesis, which generates the dna template for sequencing. the transplex whole transcriptome amplification  approach  was used to generate our rna viral-associated metagenomes. this method is based on theoretical random pcr amplification using pcr primers with a random nucleotide sequence at the 3'-end and a defined sequence at the 5'-end  <cit> . transplex wta utilizes non-self complementary primers comprising a quasi-random 3'-end and a universal 5'-end in generating the cdna library. this set of primers allows the elimination of 3'-bias, maximum amplification efficiency and the maintenance of representation during cdna library amplification. pcr amplification using primers complimentary to the universal 5'-sequences is then performed to generate enough nucleic acids  for subsequent applications, such as sequencing.

the processing of metagenomic datasets generated from primer-based amplification such as the wta method requires an additional step for sequence cleaning - trimming of the primer sequences. for the purpose of this article, any such artifacts at the end of the reads will be referred to as tag sequences. algorithms for string matching that allow errors  can be used to account for sequencing errors. the approximate string matching problem is to find substrings that match the query with k or fewer errors. an error model is used to define how different two strings are. one of the most widely used error models is the so-called edit distance, which allows deleting, inserting and substituting characters in both strings. if all the operations have cost  <dig>  simple edit distance is the minimum number of insertions, deletions and substitutions to make both strings equal. in the case of matching a tag sequence to a sequence read, the simple edit distance should transform the tag sequence into a subsequence of the read . this study is focused on online searching for approximate string matching, which is different from indexed searching. indexed searching requires the process of building a persistent data structure  on the data to speed up the search later  <cit> . however, the single search on a sequence dataset for tag removal does not justify the extra space and time that is required for generating the index. furthermore, indexed approximate string searching is a much more difficult problem and not as well studied as online approximate string matching.

many algorithms have been developed during the last  <dig> to  <dig> years in the fields of signal processing, text retrieval and computational biology. due to the large amount of literature, the reader will be referred to  <cit>  for a good reference on approximate string matching applications for computational biology. a new era for approximate string matching was started in the 1990's by exploiting computational parallelism. the basic idea of parallelizing an algorithm using bits was introduced by baeza-yates  <cit> . using the bit-parallelism, the number of operations that an algorithm performs can be reduced by a factor of at most the number of bits in a computer word. this speedup can be significant considering existing architectures with  <dig> bits. there are different approaches on how to parallelize the algorithms. approximate string matching algorithms can parallelize the work of the dynamic programming matrix as described by myers in  <cit> . myers' algorithm represents the differences along columns of the dynamic programming matrix instead of the columns themselves, requiring only two bits per matrix cell. the current values of differences can be represented using binary vectors. a logical rather than an arithmetical approach as used in  <cit>  allows updating the vectors in a single operation. the result is an approximate string matching algorithm with a worst case of o, where n is the length of the text, m the length of the query and w the word size of the machine  <cit> . this algorithm for the general string matching case was adapted to process biological sequence data.

the trimming of the tag sequence is not trivial. sequencing approaches such as pyrosequencing as implemented by roche's  <dig> technology have their limitations. base repeats, for example, might not be correctly identified due to noise in the flowgrams and can therefore generate sequences with variable tag sequences. the major source of noise is that the light intensities may not correctly reflect the homopolymer lengths and therefore result in either deletions or insertions  <cit> . to use an example from a real dataset: the true tag sequence is gtg gtg tgt tgg gtg tgt ttg g, not including the random nucleotides at the 3'-end. instead, gtg gtg tgt tgg tgt gtt gg was observed, a tag sequence with two deletions . insertions and deletions were identified in every tag-labeled metagenomic dataset examined. in three example libraries, less than 90% of the sequences have the correct 5'-end tag sequence, whereas more than 9% contained one or two insertions and/or deletions . algorithms such as pyronoise  <cit>  try to account for the noise in flowgrams, but require the raw flowgram data that is not always available to the end-user.

results for the 5'-end tag sequence , 3'-end tag sequence  and the concatenated tag sequences . note that the numbers are based on the dereplicated datasets. percentages are shown in parenthesis.

tag sequences, especially wta primer sequences, may contain ambiguous or random bases used for the sequence-independent amplification. this requires an approximate string matching algorithm that can be extended from the international union of pure and applied chemistry  ambiguity code for nucleic acids to define and identify the correct tag sequence in the query data. myers' bit-vector algorithm can be easily extended with wildcard characters for the use of approximate dna sequence matching.

the algorithm implemented here was also optimized to reflect the nuances of the sequencing approach in general, and the wta approach in particular. the  <dig> adaptors are added to the wta-amplified fragments by blunt-end ligation . this step can produce fragment-to-fragment concatenations that give rise to artificial concatenated sequences . the resulting reads may contain concatenated sequence tags of more than  <dig> bp in addition to the sequence tags at the ends of the reads. further analysis of such datasets may, for example, result in incorrect assemblies of the sequences or incorrect taxonomic assignments.

fragment-to-fragment concatenations have been identified in every metagenomic dataset examined and occurred on average more than 2% of the time . these fragment-to-fragment concatenations can be computationally identified and split, generating at least two separate sequences.

in addition, the length of the fragment from wta-amplified cdna may vary from  <dig> bp to over  <dig>  bp . current high-throughput sequencing methods such as pyrosequencing can generate sequence reads in the range from less than  <dig> bp up to  <dig> bp . the difference in fragment length and possible sequencing length may result in incomplete sequences that contain only part of the tag sequence at the 3'-end while some may contain no 3'-tag sequence at all. in order to account for this, the algorithm must accommodate differential trimming parameters at the 5'-end and 3'-end of the sequencing reads. the identification and removal of tag sequences in the dataset requires the a priori knowledge of the tag sequence used in the experiment. this information is often omitted from public databases or not available to the user due to, for example, patented methods. our implementation therefore includes a feature for automatic tag sequence detection based on the nucleotide frequencies at the ends of the reads.

methods
sample collection and metagenomic library preparation
coxsackie virus infected mouse brain tissues were homogenized and dnase treated prior to rna extraction using trizol-ls . mosquitoes  were collected from the san diego zoo wild animal park in april  <dig> using co <dig> baited cdc traps . the mosquitoes were pooled and homogenized in suspension medium  buffer . samples were filtered through  <dig>  μm  to remove large particles, followed by dnase treatment and rna extraction using trizol-ls . rna was amplified using the transplex whole transcriptome amplification kit . wta-amplified cdna libraries were then used for the genome sequencer flx systems sequencing library preparation. double-stranded cdna was treated as sonicated dna and proceeded directly to fragment size selection using the titrated amount of agencourt ampure spri beads . the ends of the fragments were polished and ligated with  <dig> adaptors prior to emulsion pcr as recommended by the manufacturer's protocol . the  <dig> multiplex adaptors were generated according to the manufacturer's protocol  and used for all libraries. the amplified material was sequenced in-house with the genome sequencer flx pyrosequencing system  using the titanium chemistry. the three viral metagenomes are accessible from ncbi http://www.ncbi.nlm.nih.gov under the genome project id  <dig> 

sequence read preprocessing
multiplexed reads were separated according to their mid tags  and stored in fasta format. tagcleaner was used to trim off the mid tags from the 5'-ends and to dereplicate the datasets . sequence reads without an exact matching mid tag were excluded from this study.

the data id can be used to access the results in the tagcleaner web interface. percentages are shown in parenthesis.

adapting the bit-vector algorithm for approximate tag sequence matching
the bit-parallel algorithm of the dynamic programming matrix was described by myers  <cit> . the algorithm was extended with wildcard characters as described below. for our purposes and for completeness, some notations are introduced to show the changes made to myers' algorithm.

let w be the length of a computer word . let ∑ be a finite alphabet of the letters a, c, g, t and n. a string s is an ordered array of letters drawn from ∑. let s <dig> be the query sequence and s <dig> be the tag sequence. let n be the length of s <dig> and m ≤ w be the length of s <dig>  ed denotes the edit distance between strings s <dig> and s <dig>  which measures the minimum number of edit operations to transform s <dig> into s <dig> , ignoring end gaps. given a pair of strings and a threshold t, any edit operation is called a mismatch between the two sequences, and a sequence does not match another sequence if the number of mismatches is greater than t.

the tag sequence s <dig> is expected to be located at the ends of s <dig>  in the first search step, a subsequence  of s <dig> from  <dig> to max { <dig>  3m/2} is used to match s <dig> using the bit-parallel implementation. the algorithm is stopped at any iteration if a perfect match with ed =  <dig> is found. the algorithm continues to search in the remaining of s <dig> to identify tag sequence repeats and fragment-to-fragment concatenations.

ambiguity code extension
the bit-vector algorithm was extended with wildcard characters as described in  <cit> . a wildcard character is a character that can be substituted for any other character of a defined subset of all possible characters. the wildcard characters represent the iupac ambiguity codes for nucleic acids to allow limited regular expressions on dna sequences. the implementation of wildcard characters does not affect the performance of the initial algorithm as the wildcard characters are only processed once during the pre-processing of the tag sequence and not while processing the dataset.

detection of fragment-to-fragment concatenations
fragment-to-fragment concatenations can be identified by subsequences that match to concatenated tag sequences. if two fragments are concatenated, the resulting sequence contains the 3'-end tag of one sequence followed by the 5'-end tag of another sequence . identifying the concatenated tag sequences inside the sequence reads allows the detection of fragment-to-fragment concatenations and hence separation into the original fragments. the detection of concatenated tag sequences uses a similar approach as for detecting tag sequences at the sequence ends. this allows the user to define a maximum number of mismatches to account for the limitations of the sequencing methods. the user may choose to only allow exact matching for this part of the program to reduce the number of possibly falsely identified approximate tag sequences.

automatic tag sequence estimation
the tag sequences are automatically detected using a nucleotide frequency-based approach. assume a nucleotide ni at position i has frequency of occurrence  and the sum of all frequencies at position i is normalized to  <dig>  if ni is part of the tag sequence, it should have a frequency  close to  <dig> and all other nucleotides at position i should have a frequency close to  <dig> . if ni is not part of the tag sequence, is should have a frequency  close to  <dig>  . the 1/ <dig> parameter assumes equal distribution of the a, c, g and t nucleotides in the metagenome . to account for the non-uniform distribution of nucleotides, it is possible to first estimate the g/c content of the metagenome and adjust the frequencies accordingly. in the current implementation, however, an equal distribution of the nucleotides is assumed and this step omitted. the range and median of the nucleotide frequencies at a position of the tag sequence with a quasi-random nucleotide should show a distinctive pattern from the first two cases with  neither close to  <dig> or  <dig> .

the tag sequences might miss nucleotides at the ends  due to the limitations of the sequencing technology. this can cause an overlap of shifted nucleotides of the tag sequence and may result in noisy frequency values. therefore, nucleotide frequencies are filtered and corrected using k-mers before the tag sequence is estimated. the k-mers  at the 5'-end and 3'-end of all sequences are extracted and filtered by frequency of occurrence. the k-mers that occur in at least 10% of the sequences are sorted by decreasing frequency and all other k-mers are rejected. the first k-mer in the list  is then aligned to the second k-mer to calculate the minimum number of shift operations l to align the two k-mers without gaps . the shift direction is based on the k-mer with the higher frequency. shifts to the left have negative values assigned , whereas shifts to the right have positive values assigned . if the second k-mer can be aligned by shifts in both directions  and min{| - l|} = min{| + l|}, then the shifts will be assigned ± l, otherwise they will be assigned min{| - l|, | + l|}. if l is less than or equal to a given threshold of shift operations , then the two k-mers are joined into one k-mer of length k + l. otherwise, the second k-mer is moved to the end of the k-mer list. in the next step, the third k-mer is aligned with either the first k-mer or the joined k-mer and the same operations are performed. these steps are repeated until no remaining k-mer can be aligned under the described criteria. the values of shift operations are then adjusted by l + a, where a = |min{l}| for the 5'-end and a = - max{l} for the 3'-end. the k-mer with the highest frequency has a assigned as its adjusted shift value. the frequencies are then shifted for the sequences that contain a k-mer with an adjusted shift value. nucleotide ni is therefore used to calculate .

the difference between range and median of  is used to predict the tag sequence. a frequency range greater than three times the median indicates a specific nucleotide , whereas a frequency range less than the median plus an allowed variation  indicates nucleotides of the "real" sequence . the remaining frequencies indicate preferred nucleotides or quasi-random nucleotides . all continuous nucleotides that fall into the first or last category are defined as tag sequence. this approach works well for sufficiently big datasets .

implementation and computational platform
the web interface was implemented in perl  <dig>  using the common gateway interface  module to generate dynamic html content, and to input and output data from and to the web interface. the bit-vector algorithm and all other calculations were implemented in perl  <dig>  using dynamic programming methods. the tagcleaner web application is currently running on a pc server with fedora linux using an apache http server to support the web services. the web interface provides a high level of compatibility with heterogeneous computing environments.

it was a design decision to make tagcleaner independent from any third party programs necessary to perform the data processing and analysis. this allows the user to operate tagcleaner on their own servers without the requirement for other software to be installed.

input and output
the input for the tagcleaner web interface is fasta data containing the metagenomic reads. in addition to fasta files, the user can submit fastq files   <cit> , which will automatically be converted into fasta format. the input data is checked to be a valid fasta or fastq file with dna data. if the input data fails the validation step, further processing is restricted.

metagenomic sequence files can be of large size , and therefore the web interface additionally allows the submission of compressed fasta or fastq files to reduce the time of data upload  from the user machine to the web server. the currently supported compression types are zip and gzip. if the compressed files contain more than one fasta or fastq file, the single files will be joined into one dataset. the file formats and compression types are automatically detected and processed accordingly. there is no limit on the number of sequences or the size of the input file accepted by tagcleaner.

in addition to the sequence data, the user can specify a tag sequence. tagcleaner accepts wildcard characters in the form of the iupac ambiguity code for nucleic acids . if the tag sequence is not available to the user, the program will try to estimate the tag sequence as described above and then allows the user to modify the tag sequence before further data processing.

the user can download the results in fasta format or its compressed version. the results can either be the data passing all filters and the tag sequences trimmed, or the data not passing the filters without any changes. this allows the user to further investigate both results separately.

furthermore, the user can decide if the fasta output file should include the following additional information in the header line: initial sequence length, sequence length after trimming, 5'-end and 3'-end trimming positions, 5'-end and 3'-end mismatches, and the number of fragments the initial sequence was separated into .

reads that were split and that passed the filter parameters have a counter added to the sequence id in order to allow a valid fasta format output .

results will be stored for one week, if not otherwise requested, on the web server using a unique identifier displayed on the result page. this identifier allows the user to share the result with other researchers without having to re-submit and re-process the dataset. the filter parameters can be imported and exported to allow consistent analysis of different datasets.

summary of filter parameters and system considerations
the user can filter the data based on different parameters. unlike most other programs, this program allows the user to define filter parameters based on the input data after the data is processed. this does not require an a priori knowledge of the best parameters for a given dataset.

the filter parameters include the maximal number of mismatches  for tag sequences matching at the 5'-end and 3'-end of the reads, occurrence of tag sequences , and the sequence range from the ends in which the tag sequence has to match. the option to continuously trim tag sequences from the ends allows for trimming of concatenated tag sequences at the ends, and is also used to filter out reads that only consist of tag sequences. additional parameters are designed for filtering the data after the trimming process. these parameters include minimum and maximum sequence length, removal of exact duplicates, removal of sequences containing the ambiguous base n above a given threshold, and separating fragment-to-fragment concatenated reads. quality trimming and sequence dereplication is recommend to be performed after tag sequence trimming. the trimming of low-quality bases at the ends might truncate the tag sequence and reduce the ability to recognize the remainder of the tag sequence. in those cases, large parts of the tag sequences might still remain for further analysis and data processing steps. the dereplication before trimming may miss duplicated sequences due to variations in the tag sequences that will be trimmed off later and would therefore require an additional dereplication step after the trimming.

RESULTS
web application
tagcleaner is publicly available through a user-friendly web interface . the interactive web interface facilitates navigation through the results, definition of filter parameters, and allows the export of the results for subsequent offline analysis. the input page of tagcleaner provides a mechanism to import new datasets and to define the tag sequence. users can choose between submitting and processing a new dataset or accessing already processed datasets using a unique identifier. the import and export functionality for the filter parameters make it easy for the users to perform the same analysis on different datasets and to record the filter parameters.

application examples
in the first application example, tagcleaner was applied to three metagenomic datasets available in fasta format . the datasets were generated as described in methods and contained tag sequences  at both ends, which needed to be trimmed before further data processing. no prior knowledge of the tag sequences was assumed. the fasta files were provided as input and the tag trimming for both ends was selected. the tag sequence was in all three cases identified as 5'-gtg gtg tgt tgg gtg tgt ttg gnn nnn nnn n  and nnn nnn nnn cca aac aca ccc aac aca cca-3' . the results of the tag detection are shown in table  <dig>  the reverse complement of the 5'-end tag sequence would be expected as tag sequence at the 3'-end. however, the exact reverse complement 5'-end tag sequence  could only be identified in  <dig>  -  <dig> % of the sequences in the three datasets.

the tag sequence at the 5'-end was identified in  <dig> - 86% of the sequences without any mismatches, while  <dig> - 98% of the tag sequences was identified by allowing a maximum of five mismatches. the tag sequence at the 3'-end was identified in 16% of the sequences without allowing any mismatches and in  <dig> - 56% of the sequences by allowing up to five mismatches. fragment-to-fragment concatenations were identified in almost 2% of the sequences without any mismatches and in more than 3% of the sequences with up to three mismatches.

setting the parameters as shown in table  <dig> resulted in  <dig>  ,  <dig>   and  <dig>   sequences passing all filters for lib <dig>  lib <dig> and lib <dig>  respectively. the majority of the filtered sequences were either shorter than  <dig> bp, had an occurrence of n above the 5% threshold, or were tag sequence repeats.

in addition, eight metagenomes from nakamura et al.  <cit>  were also investigated, as this was the only published dataset still containing wta tag sequences. the metagenomes were provided in fastq files . the datasets contained the sequence reads with tag sequences at both ends. the wta tag sequences were not published in the paper and therefore, automatic tag detection was performed on all fastq files. the program detected the same tag sequences  in all datasets. the results for no mismatches and a maximum of three mismatches for the tag sequences are shown in figure  <dig>  allowing only exact matches, the datasets contained for more than 2% of the reads concatenated fragments and allowing for a maximum of three mismatches 4% of the sequences were identified as concatenated fragments. a significant number of reads matched to the tag sequences over the whole length . these reads were filtered using the continuous trimming of tag sequences from the ends. further investigation of the fragment-to-fragment concatenated reads was performed using blastn against nbci's non-redundant database. the blastn hits were filtered using the same thresholds as described in  <cit>  and taxonomy was assigned to the best hits using the ncbi taxonomy. the nasal samples  contained more than 90% eukaryotic sequences  <cit>  and showed that concatenated fragments were mainly from the same taxonomic group . the fecal sample n <dig> with more than 80% of the sequences assigned to rna viruses showed a similar behavior. the four remaining fecal samples  contained mainly prokaryotic sequences and showed that concatenated fragments were from different taxonomic groups. the number of different blastn best hits increased with increasing taxonomic levels.

improving assemblies with tagcleaner
the gs de novo assembler software version  <dig>   was used to assemble three metagenomic libraries  to illustrate how tagcleaner can improve metagenomic and other high-throughput studies. the assembly parameters were set to 95% identity over at least  <dig> bp. assemblies were generated for three different parameter sets for each of the metagenomic libraries:  raw data;  tag sequences trimmed allowing three mismatches;  tag sequences trimmed allowing three mismatches with additional splitting of the fragment-to-fragment concatenations and continuous end tag trimming. for b and c, the minimum sequence length was set to  <dig> bp, sequence duplicates were removed and all other parameters were kept at their default values.

by using tagcleaner, the resulting assemblies showed an increase in the n <dig> contig size  for the datasets assembled with parameters b compared to the raw datasets and an even higher increase using parameters c compared to using the raw data . the ratio of number of contigs to number of contigs longer than  <dig> bp increased for the three datasets from assembly run a to c. furthermore, the average contig length for all contigs and for contigs longer than  <dig> bp were also increased.

the gs de novo assembler software version  <dig>   was used to assemble three metagenomic libraries  to illustrate how tagcleaner can improve metagenomic and other high-throughput studies. the assembly parameters were set to 95% identity over at least  <dig> bp. assemblies were generated for three different parameter sets for each of the metagenomic libraries:  raw data;  tag sequences trimmed allowing three mismatches;  tag sequences trimmed allowing three mismatches with additional splitting of the fragment-to-fragment concatenations and continuous end tag trimming. for b and c, the minimum sequence length was set to  <dig> bp, sequence duplicates were removed and all other parameters were kept at their default values.

 <dig> the n <dig> contig size is a weighted median that is defined as the length of the smallest contig c in the sorted list of all contigs where the cumulative length from the largest contig to contig c is at least 50% of the total length .

 <dig> increased number of reads due to splitting of the fragment-to-fragment concatenations.

in all cases for datasets generated with parameter sets a and b, concatenated tag sequences were observed in the contigs, but not for the datasets generated with parameter set c. the contigs with concatenated tag sequences generally showed higher coverage outside the tag sequence regions . for further taxonomic analysis, the contigs were split at the concatenated tag sequences and a blastn analysis was performed. the separated fragments hit to different taxonomic groups using ncbi's taxonomy assigned to the best hits .

comparison of tagcleaner with other programs
there are different applications available that are able to trim tag sequences. tagcleaner was compared with five other available programs, each offering various additional features and functions. although most of the programs have been designed to process 16s tag sequences, they are able to process non-16s sequence data and allow the trimming of their tag sequences. pyrotagger  <cit>  is a program to process and classify multiplexed amplicon pyrosequence data from any region of the 16s rrna gene. rdp-pyro  <cit>  is part of the ribosomal database project for the analysis of 16s sequences generated with the pyrosequencing method. seqtrim  <cit>  is a sequence pre-processing pipeline. seqclean  <cit>  is a program for trimming and validation of sequences by screening for various contaminants, low quality and low-complexity sequences. mothur  <cit>  is a software package used to analyze community sequence data. it incorporated programs such as dotur and sons and contains modules to trim tag sequences. in table  <dig>  we have compared tagcleaner with these programs for features related to tag trimming.

support iupac
the comparison is based on the features related to tag trimming, as this represents the main purpose of tagcleaner. all compared applications are still in active development and new functions will undoubtedly be added over time.

 <dig> performs barcode trimming at 5'-ends only.

 <dig> trimming position and/or number of trimmed bases.

 <dig> percent similarity between tag sequence and query sequence .

 <dig> trimming of tag sequence repeats

discussion
tag sequence contaminations are a serious concern to the quality of the data used for downstream analysis. therefore, it is important to use reliable tools for the pre-processing of sequence data. we presented a web-based program that implements several features to improve the pre-processing of the data.

the assemblies of our in-house dataset showed the improvement of the pre-processed data. the results show a particularly good example of the need for allowing mismatches in the tag sequence and for identifying fragment-to-fragment concatenations.

the algorithm of myers has superior performance compared to other algorithms when applied to biological sequence data  <cit> , but is bounded by the architecture of the system used. systems with  <dig> or  <dig> bit architectures basically allow tag sequences of at most  <dig> or  <dig> nucleotides, respectively. however, longer tag sequences can be handled using perl modules for bit-vector representation. the current implementation does not make use of those modules since they will reduce the efficiency of the program. furthermore, very long tag sequences, especially primer sequences with more than  <dig> bp are rarely used for high-throughput sequencing.

the algorithm implemented in tagcleaner for the automatic detection of tag sequences assumes the randomness of a typical metagenome. datasets that do not contain random sequences from organisms in an environment, but rather contain, for example, 16s metagenomes may cause incorrect detection of the tag sequences. however, the tag sequences will most likely be over-predicted and can be redefined by the user prior to data processing.

there are several advantages in using tagcleaner to pre-process sequence data: tag sequence trimming data filtering improve the reliability of downstream data analysis; tagcleaner is a web application that allows users to pre-process their datasets without installing any software; tagcleaner is independent of third-party software and thus compatible with any computer supporting web services.

to our knowledge, tagcleaner is the first web application optimized to automatically detect and remove tag sequences from metagenomic datasets. furthermore, no other freely available web application or standalone tool implements the additional feature of detecting and splitting fragment-to-fragment concatenations. this important pre-processing step removes tag contaminations inside the sequences, which may allow, for example, more accurate assemblies. the concatenated fragments may additionally present a source of error for annotation and taxonomic assignments, since fragments from different organisms may not be assigned correctly when concatenated. the continuous trimming of tag sequences from the ends allows filtering of sequences mainly consisting of concatenated tag sequences.

tagcleaner does not require the setting of filter parameters  before the data is processed. instead, the filter parameters are set after the data is processed, which allows the user to choose parameters appropriate for their dataset and does not require them to submit and process the same data with modified parameters for several times.

the independent parameter definition for tag sequences matching at the 5'-end and 3'-end of the reads accounts for the differences in tag sequences due to the limitations of the sequencing method used to generate the datasets.

the ambiguous code extension represents another advantage over other programs that are, for example, based on blast comparisons and do not allow the use of ambiguous letters. blast is not able to perform a search on sequences that contain ambiguous bases. this means that blast-based programs either need to search for all possible combinations or are not able to match the ambiguous positions. furthermore, blast implements a heuristic that might not allow the correct identification of all tag sequences, whereas the bit-vector algorithm implemented in tagcleaner is able to return the correct positions of matching tag sequences.

tagcleaner is also able to detect the quasi-random 3'-end of wta primers. the user has the option whether or not to trim this part of the tag sequence by simply adding or removing the letter n from the end of the tag sequence. however, we do advise users to trim the complete tag sequence. it is important to trim the random parts in order to account for mismatch-induced mutations that often happen when primers anneal to similar  sequences with high enough affinity for binding . therefore, we cannot be certain that this part of the tag sequence represents the actual sequence of the sample.

tagcleaner can be used to trim tag sequences from both ends or from a single end. this allows the trimming of mid tags from the 5'-end that are exact matches, or approximate matches by allowing mismatches.

the additional filter option provided by tagcleaner include the removal of short sequences and sequences containing the ambiguous base n. excluding those sequences can reduce the error rate of the data set. huse et al. showed that the presence of the ambiguous base n was an effective indicator of a low-quality sequence and additionally suggest that shorter sequences  are more likely to be of dubious quality  <cit> .

the high error rate in the wta tag sequences  reflects the limitations of the pyrosequencing approach. we did not see such a high error rate for mid tags that were optimized for  <dig> pyrosequencing, suggesting that transplex wta tag sequences do provide a source for higher error rates due to the gt pattern.

CONCLUSIONS
this new web-application, tagcleaner, provides scientists with the means to automatically detect and remove tag sequences from metagenomic reads without prior knowledge about the sequencing protocol, thereby enabling the analysis of public data still containing tag sequences. if the tag sequence is known, tagcleaner still provides an efficient and effective alternative to other tag removal programs by providing additional filter options such as removing short reads and duplicated reads, as well as separating reads that were a result of fragment concatenations prior to sequencing.

tagcleaner's interface is simple and user-friendly. additionally, since tagcleaner is a web application and independent from third party programs such as blast, both large and small research laboratories can easily use it. tagcleaner allows users of small research laboratories, which use external applications or pipelines that are not able to remove tag sequence sufficiently, to pre-process and filter their data and continue using the external applications for downstream analysis.

availability and requirements
• project name: tagcleaner

• project home page: http://tagcleaner.sourceforge.net

• operating system: web service, platform independent

• programming language: perl

• restrictions to use by non-academics: none

authors' contributions
rs has designed and implemented the web application, tested the program, and drafted the initial manuscript. ywl processed the samples, prepared the metagenomic libraries, and participated in the gui component design and testing. fr and re conceived of the study, and participated in its design and coordination. all authors read and approved the final version of the manuscript.

supplementary material
additional file 1
fragment length distribution. length distributions are shown as estimated by the agilent  <dig> bioanalyzer  for the libraries lib <dig>  lib <dig> and lib <dig> 

click here for file

 additional file 2
nucleotide frequency logos showing the difference between raw and filtered nucleotide frequencies. nucleotide frequency logos showing the raw frequencies  and the filtered and corrected frequencies . both logos are provided to the user on the web interface to support the detected tag sequence. the tag sequence at the 3'-end can be identified more easily in the bottom logo and therefore allows more accurate predictions. 

click here for file

 additional file 3
example sequence explaining the modified header line as generated by tagcleaner.

click here for file

 additional file 4
results for exact and approximate tag sequence matching on the datasets from nakamura et al.  <cit> . the maximum allowed mismatches are abbreviated by mm. percentages are shown in parenthesis.

click here for file

 additional file 5
blastn results for concatenated fragments found in the datasets from nakamura et al.  <cit> . blastn was performed against ncbi's non-redundant database and taxonomy was assigned according to the ncbi taxonomy. blast hits had to have an e-value of less than 10- <dig> and the best hits were used to calculate the fractions.

click here for file

 additional file 6
contig coverage plots. bars mark the locations of the concatenated tag sequences.

click here for file

 additional file 7
example of imperfect primer annealing that causes mismatch-induced mutations in the sequence reads.

click here for file

 acknowledgements
coxsackie virus infected mouse brain tissues were kindly provided by dr. ralph feuer . mosquitoes were supplied by simon j. anthony . we thank matthew haynes for helpful discussions. this work was supported by grant dbi  <dig> advances in bioinformatics from the national science foundation.
