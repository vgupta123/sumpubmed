BACKGROUND
progress in biology is dependent on the ability to observe, measure and model the behavior of organisms at multiple levels of abstraction, from the microscopic to the macroscopic. there has been a tremendous growth recently in the techniques to probe the structure and workings of cellular and organ-level mechanisms. significant advances have been made in areas such as serial block face microscopy  <cit> , and knife-edge microscopy  <cit>  that allow microstructure information to be gathered at unprecedented levels of both detail and scope. at the same time, advances have also been made in gathering temporal image data streams from microscopic samples with the use of fluorescent and multi-photon imaging techniques  <cit> . the increasing spatial and temporal resolution available, combined with advanced sectioning techniques are providing extremely content-rich data to biologists and puts unprecedented power in their hands.

this is a game-changing development, since biologists are no longer limited to carrying out experiments to test a single hypothesis at a time. they are now able to vary multiple parameters simultaneously, and observe several phenomena of relevance using multi-spectral techniques. this can be combined with recent advances in data mining techniques to determine relationships and correlations amongst the many variables of interest. this allows a significantly larger parameter space to be explored.

however, there is a mounting problem being faced by practitioners and researchers today, which is the computational bottleneck: the data storage and processing needs are growing exponentially. it may take several hours or even days to process the collected data, and the resulting throughput time may be unacceptable to support desired workflows in laboratories. unless the computational issues are addressed immediately, biologists will be overwhelmed with the data collected, and will not have adequate tools to process and extract meaning from the data. though computer vision techniques have been applied in the past to partially automate some of the analysis , the current challenge is to process much larger quantities of data  with sufficiently high throughput. this would allow biologists to interpret experimental results rapidly and ideally in an interactive fashion.

a related problem is that of building models from the collected data, which is a useful technique to test the understanding of the phenomena of interest. as the data expose interactions at finer spatial and time scales, the variables that are modeled also increase in number and complexity. this increases the computational burden on the modeling effort as well.

we present a solution to this challenge, which is based on a high-performance computing  architecture. an example of this is ibm's blue gene supercomputer. there has been a long history of using hpc to model problems in physics, but its use in biology has been very recent and rather limited. in general, hpc has not been used much in bio-imaging applications due to the difficulty in porting code to parallel machines. algorithms for image processing, such as segmentation and feature extraction are not being sufficiently developed and investigated in a hpc context. though there was interest in this area in the mid-1990s  <cit> , this appears to have waned, and the use of hpc for imaging applications is currently quite limited.

however, the landscape is rapidly changing due to the increased availability of hpc platforms, improvements in parallel programming environments , and the availability of toolkits to perform parallel data mining. hpc has significant potential to be applied to problems in biology, and in microscopy imaging applications in particular. the high computational demands of simulation and modeling complex systems can also be addressed through hpc. so a single hpc architecture can support multiple computational requirements, ranging from analyzing data to building and simulating models.

requirements
we now examine the computational requirements for a total system dedicated to handle biological imaging tasks. the following tasks would need to be performed.

 data collection: the system needs to gather and store the images acquired  deconvolution: the acquired images may contain distortions such as blur that occur in the front end optics  segmentation and feature extraction: the raw images need to be segmented into regions of interest and processed to extract domain-specific features that aid in classification  analysis and interpretation: the segmented regions are interpreted as parts of a larger biological structure such as a neuron or organ.  modeling and prediction: models for function  can be built to predict the behavior of entities of interest. this phase may require the use of simulation and optimization techniques. from the list of tasks above, the following core computational requirements can be identified  the ability to handle large data sets, including storage and retrieval.  high throughput processing is required.  the visualization of results is important.

as a case study, we consider the system developed by denk and horstmann  <cit> , that consists of a serial block-face scanning electron microscope  to explore 3-d connectivity in neural tissue. light microscopy is incapable of resolving the fine structure such as dendrites, which necessitates the use of a sem. the system is able to obtain slices that are  <dig> nm thickness, with a  <dig> ×  <dig> μm area, and  <dig> nm pixel size. this results in single images of size  <dig> megabytes. typically,  <dig> slices are obtained, giving rise to a stack of  <dig> gb of data. several such stacks need to be collected to gain information about neural connectivity in a functional area of the brain. the connectivity within the neural tissue is inferred by identifying structures within the 2d slices, such as neurites, and tracking them across successive slices. this permits the reconstruction of a 3d model of structures of interest, such as a neuron with its soma, dendrites and axon. the ultimate use of such reconstructed structures would be in developing accurate computational models of cortical function.

the system developed by denk and horstmann  <cit>  operates at the nanometer scale. a similar system developed by mccormick et al  <cit>  operates at the micrometer scale, and can section an entire mouse brain. the computational requirements arising from both systems are very similar. the need for time-varying image analysis arises from biological experiments such as the analysis of macrophage images from the bioimaging institute at mit  <cit> . the goal here is to observe the cell motility of macrophages under different ambient conditions. this requires 3d deconvolution to be performed on a sequence of images. again, the computational needs of such experiments are enormous.

high-throughput processing
in this paper, we restrict our scope to solving the problem of high-throughput processing. since the performance of single cpu machines is not sufficient to handle the size of the case-study data set described above, the use of parallel processing is inevitable. we examine the characteristics of our case-study data set.

 <dig>  significant communication is required between processors. this is specifically true of image processing tasks.

 <dig>  the bulk of the communication is between nearest neighbors.

 <dig>  computation and communication needs are well balanced. for instance, consider the operation of recursive 3d median filtering, which is useful for combating noise. every iteration of the filtering operation may require significant amount of data communication. suppose we use  <dig> processors for the  <dig> gb stack, with each processor storing  <dig> mb of data. assuming up to half this data needs to be communicated, we have a communication need of  <dig> mb per process to be sent/received to/from its  <dig> neighbors. this is a large amount of data especially since it may need to be communicated at every iteration of the computation.

there are many possible ways of using parallel processing systems to meet these requirements, such as clusters of workstations, shared memory systems and the use of game processors. we chose to implement our system on the ibm blue gene supercomputer due to its scalability  and its specialized interconnect technology, offering superior communication speeds.

RESULTS
in order to make advances in microscopy, progress needs to be made on several fronts simultaneously, including new methods for image acquisition, processing algorithms for feature extraction and analysis, and the computational architecture and methodology for fast processing. the focus of this paper is on the latter issue, and deals with the use of parallel computation to address the throughput requirements. there are other research efforts to investigate algorithms for feature extraction and reconstruction  <cit> . our goal in this paper has been to show how such algorithms can be implemented on a parallel architecture.

our system is written using mpi . we present results of using our system to process images from the serial block-face sem apparatus  <cit>  on an ibm blue gene supercomputer. as an illustration, we implemented recursive 3d median filtering on blue gene, and found that a single pass of a  <dig> ×  <dig> ×  <dig> filtering operation takes  <dig>  seconds for  <dig> slices of  <dig> ×  <dig> grayscale images on  <dig> processors arranged in an  <dig> ×  <dig> ×  <dig> cartesian grid. the result is shown in figure  <dig>  the same computation running in matlab on a single intel  <dig> ghz processor took  <dig> minutes, or about  <dig> times slower. this illustrates that the use of hpc can have a dramatic improvement in the throughput of image processing tasks, and that hpc has tremendous potential to influence the fields of bio-imaging and microscopy. figure  <dig> depicts the speedup achieved as the number of processors is increased from  <dig> to  <dig> for the 3d median filtering task. clearly, one can obtain real-time performance for this task on a multiprocessor machine with sufficient number of processors. this would be extremely beneficial for interactive data analysis, algorithm development and visualization of the entire data set.

we implemented the technique described by xu and prince  <cit>  for contour processing. an initial contour is selected by the user, which encircles the object of interest, say a neural structure, such as a portion of a dendrite as shown in figure  <dig>  the snake, a deformable template, then accommodates itself to fit the exact contour of the dendritic structure. this reduces the burden on the user in specifying the detailed contour.

this procedure is performed on the first image slice. the next image slice uses the snake from the previous slice to initialize the contour, and executes the same energy-minimizing algorithm as before. this procedure is repeated across multiple image slices. the 2d contours extracted can be assembled into a 3d model of neural connectivity. a partial reconstructed model is shown in figure  <dig>  this shows a part of a dendrite that has been detected and rendered using the technique described above. there are possibly hundreds of such structures within a given slice of neural tissue. we have chosen to illustrate the reconstruction of a small portion of the data, as our work is at an early stage, and further results will be forthcoming.

in figure  <dig> we compare the performance of our algorithm running on blue gene/l versus an intel linux cluster. no changes to the mpi program were required, and it was merely recompiled on the linux cluster. the cluster used upto  <dig> nodes of intel pentium iii cpus running at  <dig>  ghz and connected via  <dig> mb/s ethernet. the communication time in blue gene for the 3d median filtering task is approximately 5% of the total time. whereas, for the linux cluster, the communication time is more than 40% of the total time. the problem size remained fixed.

the 3d torus interconnection architecture on blue gene/l is used in conjunction with other schemes such as tree-based networks to optimize communication times. in order to demonstrate the efficiency of the torus network with dedicated nearest-neighbor connectivity, we performed the following experiment. we ran the 3d median filtering algorithm in two modes. the first mode used a cartesian mapping to the torus, such that the ranks were sequentially assigned in a 3d grid in correspondence with the 3d torus on the machine. the second mode used a randomized mapping, where this orderly correspondence between ranks and the torus did not exist. figure  <dig> shows that the randomized mapping was 10% slower for a  <dig> node configuration, and 30% slower for a  <dig> node configuration.

discussion
we expect that other image processing tasks, such as iterative morphological operations , or recursive filtering  will also benefit from implementation on an hpc platform if they operate on large datasets. in general, any local neighborhood operation can be computed advantageously using our method as shown in figure  <dig> 

the procedure described to produce the result in figure  <dig> is semi-automated. there are other approaches in the literature which are more sophisticated, such as the technique developed by busse et al  <cit> , which is also semi-automatic. we have chosen to explore a parallel processing solution to the reconstruction problem, and are initially implementing simpler approaches.

other studies by almasi et al. have demonstrated that a wide variety of scientific applications can scale up to tens of thousands of processors on blue gene/l  <cit> . we view this as a significant advantage in using the blue gene/l platform. the image processing application can be written once using mpi, and the hardware platform provides the desired scaling.

the significance of the result in figure  <dig> depends on the precise task being performed. if the task is computation bound, then improvements in communication may not have a significant effect on the throughput. however, for communication-bound tasks, the demonstrated improvement of the cartesian mapping may be significant.

other studies, such as agarwal et al.  <cit>  have also demonstrated an increase in communication times when the task is not optimally mapped to the processors on blue gene/l. this shows that the mapping of communicating objects to nearby processors is desirable.

to summarize this discussion, it is advantageous to use an hpc architecture that optimizes nearest-neighbor communication on a 3d grid. depending on the task to be performed, this communication efficiency may result in significant throughput increases as compared with other network topologies. the comparisons shown in this paper are not meant to be comprehensive, and the benchmarking of image processing applications on hpc platforms needs further investigation. this may require a community-wide effort to create and measure appropriate benchmarks on multiple hpc platforms. as an example, nist has been conducting a series of trecvid workshops on measuring video image database retrieval performance for several years  <cit> .

our system has been designed with mpi, a de-facto standard environment for parallel processing. this enables the system to be used widely across different platforms. given the increasing popularity of grid computing, and the increasing availability of supercomputers, we expect to see wider usage of parallel processing techniques in areas such as microscopy. in order to fully exploit the available parallel platforms, we recommend that students at universities should be trained to use such technology. furthermore, curricula in courses such as image processing and computer vision should cover parallel processing techniques.

CONCLUSIONS
our results demonstrate that the use of hpc can have a dramatic improvement in the throughput of image processing tasks, and that hpc has tremendous potential to influence the fields of bio-imaging and microscopy.

the main contribution of this paper is to develop a parallel image processing system for handling multidimensional image data that optimizes computation and communication needs in a multiprocessor system. this is achieved through an appropriate domain decomposition that exploits support in mpi for computation in cartesian grids.

our results show that by using the blue gene/l machine, significant throughput increases can be achieved compared to conventional clusters. furthermore, the domain decomposition and algorithms presented in this paper show favorable scaling behavior as the number of processors is increased.

this paper presents early implementation results, and further work needs to be done to incorporate more sophisticated image processing algorithms in this environment. additionally, time-domain processing capability needs to be added.

