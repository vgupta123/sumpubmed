BACKGROUND
post-genomic data analysis  is often concerned with the identification of differentially expressed molecules  under different experimental conditions  using multiple biological replicates. a simple and widely used non-parametric statistical method, initially introduced by breitling et al.  <cit>  for gene expression microarrays, is to rank the molecules within each experiment in order of evidence for differential expression and to calculate the product of the ranks across experiments. this rank product method is based on the common biological belief that if a molecule is repeatedly at the top of the lists ordered by up- or down-regulation fold change in multiple treatment–control experiments, the molecule is more likely to be differentially expressed.

the rank product statistic is particularly useful for the analysis of noisy datasets and a small number of replicates, as it does not rely on any distributional assumptions . its main weakness is sensitivity to variations in molecule-specific variance, namely higher variance of weakly expressed molecules. this limitation is mitigated, in practice, by variance-stabilizing normalization  <cit> . the rank product method is used to combine ranked lists in gene expression profiling and in various other postgenomic datasets with ranked scores, including proteomics and metabolomics . such ranking is important because only a limited number of candidate molecules  can usually be followed up in a typical biological downstream analysis for confirmation or further study. another advantage of the ranking is the resulting suppression of the unwanted influence of correlated behaviour between different molecules. in contrast to traditional marginal tests, such as the t-test, in the rank product approach correlating molecules ‘compete’ for positions in the ranked list. in the extreme case of identical behaviour of all molecules, a t-test would yield the same  result for all molecules, whereas in a rank product test, ties in the ranked list would be broken randomly, guaranteeing that none of them would be considered differentially expressed. as a useful side effect of this feature, the rank product test becomes increasingly conservative as larger fractions of the set of molecules studied are differentially expressed: if all molecules are changing to the same extent, their rank ordering will again be random.

having ranked the molecules by their rank product, the next step is to obtain the p-value associated with each molecule under the null hypothesis that the molecule is not differentially expressed in any of the independent replicate experiments. the crux here is the requirement to correct for multiple testing inherent in the need to perform one test per queried molecule. methods that use the entire distribution of p-values to estimate or control the false discovery rate  assume and perform well only when accurate p-values are available  <cit> . it is therefore imperative to obtain the most accurate probability estimates in applications that involve a massive number of tests  <cit> , such as in the analysis of transcriptome profiling data.

for this reason, exact calculation is preferred in computing p-values for use in subsequent molecule-specific fdr-adjustment procedures. eisinga et al.  <cit>  recently provided a derivation of the exact probability distribution of the discrete rank product statistic and its true tail probabilities. an obstacle of exact calculation is that, whereas the p-values of small rank products can be calculated swiftly, computing the probabilities of large rank products may consume considerable amounts of time. although the speed of execution will depend on computing power, exact p-value calculation becomes time prohibitive in multiple experiments for rank product values exceeding  <dig>  unfortunately, in a typical large postgenomic molecular profiling study, such rank products may occur for the bulk of the molecules analysed.

if exact calculation is infeasible, re-sampling-based inference methods such as permutation testing may be considered. the permutation re-sampling procedure involves a trade-off between accuracy and number of permutations  <cit> . that is, the number of permutations needed is always larger than the inverse of the p-value, but a factor of the order of  <dig> or so more permutations is required so that the p-values can be accurately estimated to several decimal places for performing multiple-testing adjustment. in practice, the number of permutation samples may perhaps go up to  <dig>  but re-sampling then starts to become unrealistically expensive, meaning that it is hard to accurately estimate p-values smaller than 10− <dig>  such p-values are common in rank product analysis of the expression values of many molecules in multiple batches.

as an alternative procedure, koziol  <cit>  suggested to use the continuous gamma distribution to approximate the sampling distribution of the discrete rank product statistic. for large rank products the gamma calculation performs well, and for extremely large values the gamma p-values are close to exact. however, eisinga et al.  <cit>  have shown that for smaller rank product values, i.e., the ones biologists are most interested in, the gamma approximation has a serious bias, overestimating p-values by several orders of magnitude, and that the error increases as the p-values become smaller.

there is therefore a range of intermediate rank product values in postgenomic studies where current approaches, exact calculation and stochastic and deterministic approximations, all have serious drawbacks in terms of computation time, accuracy or both. the goal of this paper is to obtain guaranteed lower and, in particular, upper bounds for the p-values of any rank product value observed, with the conservative upper bound protecting against false positives. the strict bounds may also be exploited to quickly calculate accurate approximate p-values for rank product analysis of a variety of postgenomic molecular profiling data.

methods
the rank product approach was originally derived for paired experiments . however it can be applied for unpaired data, which are common in postgenomic molecular profiling, by creating random pairs of experiments and calculating the average rank product for several random pairings. without loss of generality, we thus consider n molecules profiled in k paired experiments. in each experiment i, a molecule receives a random ranking ri, i.e., any number between  <dig> and n. we define gk/nk as the probability that the product of these random rankings is smaller than rank product ρ: gkρ=∑r1=1n∑r2=1n…∑rk=1nΘρ−r1×r2×…×rk, with the heaviside step function Θ =  <dig> iff x ≥  <dig> and  <dig> otherwise. we obviously have gk =  <dig> for any ρ <  <dig> and gk = nk for ρ ≥ nk. our starting point is the observation that the distribution of ρ for k experiments relates to that for k −  <dig> experiments. since any rank product ρ based on k experiments can be written as the product of a rank r <dig> in the first experiment times a rank product ρ' based on k −  <dig> experiments, we have:  <dig> gkρ=∑r1=1ngk−1ρ/r1=gk−1ρ+∑r1=2minρ,ngk−1ρ/r <dig>  

here the upper limit min explicitly incorporates that ρ' can never be smaller than  <dig>  so r <dig> can never be larger than ρ. in theory, we could use this recursion to compute gk. in practice, this is unfeasible for large ρ, n, and/or k. the discrete nature of this recursion makes it difficult, if not impossible, to obtain a generic analytical solution. however, as we will show below, it is possible to bound and approximate this sum through integrals that can be evaluated analytically.

our line of reasoning is as follows. since gk is a cumulative function, it is monotonically increasing in ρ. sums over monotonically increasing or decreasing functions can be bounded by integrals. in doing so, we turn the discrete recursion  involving a summation into continuous recursions involving integrals, one for a lower bound and another one for an upper bound. recursions involving integrals are typically easier to solve than recursions involving summations. the upper limit min in the discrete recursion  translates to the same upper limit in the continuous recursion, basically implementing the fact that a rank product ρ' based on k −  <dig> experiments cannot contribute to a rank product ρ based on k experiments if ρ' > ρ. this upper limit is a highly nonlinear function of ρ, which then also does not allow for an easy solution of the continuous recursion. by consistently separating the cases ρ ≥ n and ρ' ≥ n from ρ < n and ρ' < n, we will see that the solution of the continuous recursions can be written as a piecewise function, with recursions for the separate pieces still in terms of integrals, but now with limits that are linear rather than nonlinear functions of the rank product ρ.

perhaps surprisingly, these recursions for the separate pieces, each corresponding to a different interval for the rank product ρ, can be solved analytically. that is, these solutions can be written in terms of basic functions, the parameters of which follow a simple recursion that can be implemented in a fast algorithm. so, in the end, we have managed to turn the complicated recursion  on a function gk into a simple recursion on parameters that specify piecewise continuous upper and lower bounds on gk. the following sections describe the steps in mathematical detail.

integral and piecewise recursion
since any combination of ranks that contributes to gk also contributes to gk if ρ' ≥ ρ, we easily see that gk is monotonically  increasing in ρ for any k. but then gk− <dig> is monotonically decreasing in r <dig>  summations over monotonically decreasing functions can be bounded by integrals . as the following theorem indicates, this can be used to derive upper and lower bounds that obey recursion equations involving integrals instead of summations.

theorem  <dig>  consider the two functionsg¯kρandg¯kρthat satisfy the recursions  <dig> g¯kρ=g¯k−1ρ+∫1minρ,ng¯k−1ρ/rdr and  <dig> g¯kρ=g¯k−1max <dig> ρ/n+∫1minρ,ng¯k−1ρ/rdr, and are both initialized atg¯0ρ=g¯0ρ=g0ρ=Θρ− <dig> for any k ≥  <dig> and ρ ≤ nkwe have g¯kρ≤gkρ≤g¯kρ. 

that is,g¯kρgives an upper bound on gk andg¯kρa lower bound. the proof is detailed in additional file  <dig> 

for ease of exposition, we introduce the constant ∆ and consider the recursion  <dig> g˜kρ=Δg˜k−1ρ+1−Δg˜k−1max <dig> ρ/n+∫1minρ,ng˜k−1ρ/rdr. 

setting ∆ to either  <dig> or  <dig>  we obtain the recursion for the lower and upper bound, respectively. we will argue and empirically show that an accurate approximation  can be obtained by taking the geometric mean of the upper and lower bound.

the recursion starts from g˜0ρ=Θρ− <dig>  the constraint that g˜kρ=gkρ= <dig> for ρ <  <dig>  and the consequence that the upper limit of the integral is a nonlinear function of ρ, seriously complicates the solution of the recursion . however, we will see that if we write g˜kρ as a piecewise function,  <dig> g˜kρ=g˜k0ρ=nkg˜kjρg˜k,k+1ρ=0ifρ≥nkifnk−j≤ρ<nk−j+1;j=1…kifρ< <dig>  the recursion equation for the pieces g˜kjρ simplifies considerably and can in fact be solved.

theorem  <dig>  withg˜kρa piecewise function of the form , the piecesg˜kjρsatisfy, for 1 ≤ j ≤ k −  <dig>  the recursion  <dig> g˜kjρ=Δg˜k− <dig> j−1ρ+1−Δg˜k− <dig> jρ/n+∫1ρ/nk−jdrg˜k− <dig> j−1ρ/r+∫ρ/nk−jng˜k− <dig> jρ/rdr, and, for j = k,  <dig> g˜kkρ=Δg˜k− <dig> k−1ρ+1−Δ+∫1ρg˜k− <dig> k−1ρ/rdr. 

the proof is given in additional file  <dig>  the intuition behind the piecewise construction follows if one tries to construct the recursion for k = <dig> , <dig>  and so on. for k = <dig>  ρ is always smaller than n, so max = <dig> and min = ρ. for k = <dig>  we can separate the cases ρ ≥ n and ρ < n, corresponding to the pieces g˜ <dig> and g˜ <dig>  respectively. for k = <dig>  we again separate the cases ρ ≥ n and ρ < n, but now we also have to check whether ρ/r in the integrand g˜2ρ/r is larger than n  or smaller than n . working this out, one realizes that three different pieces are needed for g˜ <dig>  induction on k leads to the piecewise function  and the recursions  and . these recursions now involve integrals, instead of summations, with limits that are either constants or linear in ρ, instead of a nonlinear function of ρ.

lattice
figure  <dig> sketches the dependencies between different combinations of k and j, where j is the index of the interval  that contains the rank product ρ, i.e., j = ceiling.figure  <dig> 
visualization of recursion. visualization of recursion with k, the numbers of experiments, on the y-axis, and j, the index of the interval that contains the rank product ρ, on the x-axis. nodes correspond to combinations of k and j. the squares are given: g˜k0ρ=nk, with n the number of molecules. the arrows show the dependencies between the nodes. for example, to compute g˜ <dig> ρ, we first need to compute g˜ <dig> ρ and g˜ <dig> ρ. the red path visualizes the calculations required to obtain g˜ <dig> ρ.




an actual implementation to compute g˜kjρ can be recursive, e.g., starting at node  and recursively computing the parameters that are needed. the alternative is to pre-calculate which parameters are needed and then go through these in two for-loops. to compute g˜kjρ, one possibility is then to have an outer loop with j' running from  <dig> to j , with an inner loop with k' running from j' to max . the other option is to have an outer loop with k' running from  <dig> to k  and j' from max to min .

functional form
the recursions  and , together with the initialization g˜k'0ρ=nk', fully determine g˜kjρ for any ρ  and k. we could replace analytical integration by numerical integration. however, trying a few steps, one soon realizes that the integrations that are required in each of the steps can be done analytically and a pattern starts to emerge. it appears that every solution can be written in the form  <dig> g˜kjρ=εkj+δkjρ+γkjtΨρ;αkj,nk−j+βkj, with Ψρ;α,λ≡ρlogρλα, and appropriate choices for the parameters α, β, γ, δ, and ε. here αkj, βkj, and γkj are vectors of equal length. we used vector notation such as γtΨρ;α,nk−j+βkj=∑mγmΨρ;αm,nk−j+βm, where the sum runs over all elements of the vectors.

theorem  <dig>  the solutions of the recursions  and , starting from the initializationg˜k'0=nk',can be written in the form . see additional file  <dig> for the proof.

updates and implementation
now that we have confirmed that the solution is indeed of the form , what remains is to find the proper updates of the parameters θ ≡ {α, β, γ, δ, ε}. these are given in the following theorem, the proof of which is given in additional file  <dig> 

theorem  <dig>  the parameters θ ≡ {α, β, γ, δ, ε} of the solutiong˜kjρobey the update equations, for 1 ≤ j ≤ k −  <dig>   <dig> αkj= <dig> ,αk− <dig> j− <dig> αk− <dig> j,αk− <dig> j−1+ <dig> αk− <dig> j+1βkj= <dig> ,βk− <dig> j− <dig> βk− <dig> j,βk− <dig> j− <dig> βk− <dig> jγkj=δk− <dig> j− <dig> −δk− <dig> j,Δγk− <dig> j− <dig> −Δnγk− <dig> j,ϕk− <dig> j− <dig> −ϕk− <dig> jδkj=Δδk− <dig> j−1+1−Δnδk− <dig> j+1nk−jεk− <dig> j−1−εk− <dig> j−ϕk− <dig> j−1t−βk− <dig> j−1logn∘αk− <dig> j−1+1+ϕk− <dig> jt1−βk− <dig> jlogn∘αk− <dig> j+1εkj=1−Δεk− <dig> j−εk− <dig> j−1+nεk− <dig> j, with shorthand ϕk',j'≡γk',j'∘/αk',j'+ <dig>  and, for j = k,  <dig> αkk= <dig> αk− <dig> k− <dig> αk− <dig> k−1+1βkk= <dig> βk− <dig> k− <dig> βk− <dig> k−1γkk=δk− <dig> k− <dig> Δγk− <dig> k− <dig> ϕk− <dig> k−1δkk=Δδk− <dig> k−1+εk− <dig> k−1εkk=1−Δ1−εk− <dig> k− <dig>  

in the above expressions, division  and exponentiation  are to be interpreted element-wise  and  stands for the concatenation of elements and vectors into a new  vector. the update equations can be initialized by setting  <dig> εk'0=nk',δk'0= <dig> andαk',0=βk',0=γk',0= <dig>  for all 0 ≤ k' ≤ k.

from the updates it can be seen that each αk,j,m ∈ { <dig>  …, k} and each βk,j,m ∈ { <dig>  1}. so, at most there will be 2 k unique combinations of α and β values. in an actual implementation, with every update we first compute and concatenate all α’s and β’s and then confine them to unique combinations by adding the γ coefficients that correspond to the same combination.

to compute g˜kρ for the whole range of rank products ρ at once, we first compute the set of corresponding intervals labelled by j. for all j ∈ j we then need to calculate the corresponding θkj. we can do this recursively or using for-loops. when doing this recursively, it is wise to keep track of the parameters that already have been computed to prevent repetitive calculations. see algorithm  <dig> in the additional file  <dig>  when using for-loops, following the same line of reasoning as suggested by figure  <dig>  we have an outer loop with j' running from  <dig> to max  and an inner loop with k' running from j' to max + j', k) . alternatively, we can have an outer loop with k' running from  <dig> to k  and j' from max, 0) to min) . this latter ordering is taken in algorithm  <dig> in additional file  <dig>  the solution for each ρ then follows by computing g˜kjρ from , with j labelling the interval containing ρ. algorithm  <dig> is implemented in r  and the r code is published in additional file  <dig> and is available at http://www.ru.nl/publish/pages/726696/rankprodbounds.zip.

exact calculation and gamma approximation
the exact p-values may be obtained by a brute force search using the discrete recursion . an alternative method, proposed by eisinga et al.  <cit> , is to use number theory to obtain a combinatorial exact expression for calculating the discrete probability distribution of the rank product statistic. the distribution is asymmetric  and in determining the p-value, all probabilities need to be calculated, from the smallest rank product possible, with ρ =  <dig>  to the rank product value of interest. this implies that the exact statistical significance of large rank products may take unacceptably long amounts of time to compute  <cit> .

in  <cit> , koziol argues that under the null hypothesis for experiment i the p-values ri/ are approximately uniformly distributed on the interval  <cit> . as the probability distribution of the negative log-transformed p-values is given by the exponential distribution with scale parameter  <dig>  the negative sum of the log-transformed p-values over k independent experiments has a gamma distribution . this approach is equivalent to fisher’s  <cit>  method of combining p-values over independent tests. as illustrated below, the assumption that the distribution of the p-values is uniform on the continuous interval  <cit> , when in fact it is uniform on the discrete set {1/, 2/, …, n/}, leads to substantial deviations from the right tail of the true distribution.

RESULTS
time performance and accuracy
the r program computes the bounds and the geometric mean p-value approximation at a very fast speed. for example, it takes approximately  <dig> milliseconds to calculate the upper bound p-value of any rank product ρ in the range  <dig> to nk, for n =  <dig> and k =  <dig>  on a hp desktop computer using the interpreted r language running under windows  <dig> with an intel core i <dig> cpu at  <dig>  ghz. it takes twice as much time to calculate the geometric mean p-value approximation. unlike exact calculation, the algorithm’s computational time is almost unrelated to the value of rank product ρ.

to examine the effect of the number of experiments k on the algorithm’s running time, we generated  <dig> random draws from the discrete uniform distribution on  and calculated the upper bounded p-value of the simulated rank products, for n =  <dig> and k =  <dig>  …,  <dig>  figure  <dig> plots the computation time  for the calculation of  <dig> p-values and a third-order polynomial fitted line.figure  <dig> 
computation time  for calculating  <dig> upper bound
p
-values for
n 
=  <dig> and
k 
=  <dig>  …,  <dig> 




the figure indicates that computation time is no limiting factor when it comes to approximate p-value calculation of rank products, even for very demanding problems. running time increases polynomially  with increasing k. also, the time needed to do the same calculation for much larger n is similar to the time figures shown in the plot, as the algorithm’s computational time is not only virtually unrelated to rank product ρ, but also unaffected by n. this implies that the proposed calculation method should work well with all sample and replicate sizes typically encountered in postgenomic molecular profiling experiments.

to assess numerical accuracy, the entire p-value distribution was obtained for both large and small values of n and k . figure  <dig> displays the gamma approximation, the upper and lower bounds, and the geometric mean p-value approximation. the exact p-values are reported only for small values of ρ  and for the entire range of rank products of the smallest n and k . exact p-value calculation of the entire distribution is computationally unmanageable  for the other values of n and k. as can be seen in figure 3a, the upper and lower bounds are rather tight. relatively speaking, i.e., on a logarithmic scale, they are most tight for large rank products. for small rank products they are in this case  at most a factor  <dig> off, that is higher/lower than the exact p-value. the approximation obtained by taking the geometric mean of the upper and lower bound is seen to be very accurate.figure  <dig> 
bounds and approximations of
p
-value distribution.  strict bounds and approximations  for n =  <dig> molecules and k =  <dig> experiments, on the left-hand side over the whole range of rank products, on the right-hand side for small rank products only . it can be seen that, on the log scale, the bounds are very tight. zooming in on small rank products, the bounds are on average about a factor  <dig>  off , yet the geometric mean approximation is still very close to the exact p-value.  same as , but for n =  <dig> and k =  <dig>  the curve on the left looks more or less the same but, as is best seen on the right, the bounds are much further off .  same as , but for n =  <dig> and k =  <dig>  the curve on the left may look worse, but that is mainly because of the scaling of the y-axis. relatively speaking, the bounds are still on average about a factor  <dig>  off.  same as , but for n =  <dig> and k =  <dig>  with very small n and relatively large k, we get the worst of both worlds.



trying different values of n and k, the curves look extremely similar when we plot them over the entire range of rank products, that is, for log-transformed p-values, between − klogn and  <dig>  the range between the log upper bound and the log lower bound is more or less independent of n and increases roughly linear with k, but then so does the range of log p-values. with increasing n, the range of log p-values does increase logarithmically with n, where the range between upper and lower bound remains about constant . this makes that curves for large n look most impressive in the sense of displaying tight bounds. results for small n and large k are least impressive . in any case, excluding extremely large rank products, the upper bounds are always orders of magnitude better than the gamma approximation. the latter assumes a continuous distribution and this assumption is too strong for the analysis of discrete rank products.

when trying to find an even better approximation or bound for gk, one option is to use the continuous approximation scheme to compute g˜k−1ρ' for all ρ' ≤ ρ and then apply the discrete recursion  to arrive at better g˜kρ. initial attempts revealed that this indeed yields somewhat tighter bounds  and a more accurate approximation, but not to the extent that it seems worth the computational effort.

application
to illustrate our method in a real-world application, gene expression data on human aging were obtained from van den akker et al.  <cit> , available at http://onlinelibrary.wiley.com/doi/ <dig> /acel.12160/suppinfo . the data set contains the statistical results for  <dig> unique genes  from four different studies. the authors employed rank product analysis to identify genes consistently up- or down-regulated with age across the four data sets. table  <dig> displays the top  <dig> genes having increased expression with age.table  <dig> 
top- <dig> age-associated genes with increased expression level 



symbol
geneid
ρ
p
-value
exact
gamma
upper bound
geometric mean
lower bound


we obtained the exact p-values and, ideally, one should use these values in correcting for multiple testing as they are the gold standard in the sense that the sampling distribution is known exactly. only by deciding to accept or reject the null on the basis of exact p-values are we guaranteed to be protected from type- <dig> errors at the desired significance level. however, it takes considerable amounts of time to calculate the p-value for the gene listed in the bottom of table  <dig>  and it is  not feasible to obtain the exact p-values of the largest rank products on a timely enough basis. the strict upper and lower bounds, however, perform well in the sense that the limits are narrow and the bias is tiny. although the geometric mean p-value approximation provides no absolute guarantee to protection from type- <dig> errors, the estimates and the exact probabilities are seen to be very close. the gamma distribution is seen to produce rather inaccurate approximate results.

bonferroni corrections are one approach for controlling the experiment-wide false positive rate  by specifying what α value should be used for each individual test, taking α = π/n. for the current study, π =  <dig>  gives α =  <dig> /9047 ≈  <dig>  × 10− <dig>  we declare a test  to be significant if p ≤ α.

the results for both up- and down-regulated genes are shown in the left panel of table  <dig>  under a strict bonferroni correction, we reject the null hypothesis of no differential expression with associated exact p-value for  <dig> up- and  <dig> down-regulated genes. the geometric mean p-value approximation produces results identical to the exact method. the asymptotic gamma approximation is too conservative in that it tends to understate the evidence against the null hypothesis. while reducing the number of false positives, it also reduces the number of true discoveries, especially for down-regulated genes. the bonferroni method applied to the gamma p-values declared  <dig> genes to be significant, instead of  <dig> table  <dig> 
number of genes called significant according to bonferroni correction and fdr
q
-values


bonferroni correction
q
-value
<  <dig> 
<  <dig> 
<  <dig> 

up-regulated genes

down-regulated genes


the traditional bonferroni correction may be too stringent in postgenomic multiple testing, where the number of molecules profiled in parallel is very large, and falsely detecting a small number of molecules as differently expressed will usually not be a serious problem if the majority of significant molecules are properly selected. a less stringent method is to estimate the fdr for the entire collection of p-values, defined as the expected number of false positives amongst the molecules selected as significantly differentially expressed, described in detail in storey  <cit>  and storey and tibshirani  <cit> . we obtained the fdr adjusted p-values, i.e., q-values, for all approximate p-value estimates, using storey’s r program q-value . the estimated q-value for any particular test is a function of the p-value for that test and the distribution of the entire set of p-values. as it utilizes information from all the p-values at once, it is impossible to obtain q-values based on the exact probabilities. the right panel of table  <dig> presents the number of significant calls for various thresholds by p-value approximation method. as can be seen, about  30% of the differentially expressed up-regulated genes selected using the upper bounded p-values at a q-value of  <dig> , were not detected by the overly conservative gamma approach.

CONCLUSIONS
in replicated molecular profiling experiments, where large numbers of molecules are simultaneously tested, accurately estimated p-values are essential for making justified, reproducible decisions about which molecules to consider as significantly differentially expressed in the downstream analysis. we provide a tailor-made solution to calculate strict bounds and accurate approximate p-values for rank product analysis of postgenomic molecular profiling data. the proposed algorithm runs very fast and gives a slightly conservative upper bound protecting against false positives and a close approximate estimate of the true p-values.

over the past decade, the rank product method, developed originally for the analysis of microarray datasets, has found widespread use in various settings such as proteomics  <cit> , metabolomics  <cit> , rnai screening  <cit> , meta-analysis  <cit> , and classification  <cit> . however, its application has been restricted to medium sample and replicate sizes due to an intensive permutation test used to calculate significance. the algorithm presented here can provide an order of magnitude increase in throughput as compared with permutation testing. it also allows researchers to explore new application domains with even larger multiple testing issue, e.g., in large genetics studies with millions of markers or rnaseq analyses where the number of studies transcripts is larger than the number of genes or in applications to image analysis.

software availability
the r code is also freely available at http://www.ru.nl/publish/pages/726696/rankprodbounds.zip.

additional files
additional file 1: 
proof of theorem  <dig> 


additional file 2: 
proof of theorem  <dig> 


additional file 3: 
proof of theorem  <dig> 


additional file 4: 
proof of theorem  <dig> 


additional file 5: 
pseudo code. a pdf file providing the pseudo code of two algorithms.

additional file 6: 
rankprodbounds. a .zip file providing the code of the algorithm implemented in r.



competing interests

the authors declare that they have no competing interests.

authors’ contributions

th designed the approximation method, implemented the algorithm, and drafted the manuscript. re participated in the design of the method, performed the data analysis and drafted the manuscript. rb supervised the study and drafted the manuscript. all authors read and approved the final manuscript.

