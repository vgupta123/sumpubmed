BACKGROUND
over the last few decades, dna sequencing has firmly established its role in the broader enterprises of scientific and medical research. enabled by ongoing development and refinement of laboratory techniques, instruments, and software, investigators are now studying a wide array of genomes at a level of sophistication not before possible. while a number of sequencing approaches have been devised, experience indicates that the efficacy of any particular one depends strongly upon the context of the target sequence. for instance, the whole genome shotgun  procedure has proved especially suited to microbes  <cit> . conversely, mammalian projects are being completed using large-insert mapped clones, which are better able to resolve long-range assembly issues related to dna repeats  <cit> .

repetitive sequences are especially abundant in many of the economically and agriculturally important plant species. for example, the maize genome  is comparable in size to the human genome, yet up to 80% of it consists of retroelements  <cit> . the degree to which established sequencing techniques will be successful for such cases is not clear  <cit> . two notable methods have been proposed to address high-repeat projects. both seek to filter out repetitive regions, leaving primarily low-copy "islands" to be amplified in a dna library. methyl filtering excludes repetitive elements based on their elevated levels of cytosine methylation  <cit> . conversely, high-cot purification preferentially selects low-copy genic regions based upon characteristic re-association rates  <cit> . in this context, it can be considered a form of normalizing procedure. methyl filtering appears to be compatible only with plant genomes  <cit> , while cot selection can be applied broadly.

genomic projects are generally guided by probabilistic models of the underlying random processes. the seminal work of lander and waterman  <cit>  has long served as the theoretical foundation for standard fingerprint mapping and shotgun sequencing methods. although not strictly correct, the coverage model first used by clarke and carbon  <cit>  is also treated as a de facto part of lander-waterman  theory. these formulations are predicated upon an infinitely long genome, whose sequence is completely represented in the form of a non-biased clone library. in mathematical terms, these clones and their resulting sequence reads are taken to be independently and identically distributed . the lw model allows one to estimate parameters of interest, e.g. sequence coverage and the number of gaps, as functions of the number of reads processed .

filtered libraries are, however, an incomplete representation of the target sequence. specifically, they are punctuated by fixed gaps of unknown size . when using the filtering schemes mentioned above, the number of such gaps is expected to be large and this introduces additional modeling issues. consider, for example, a de novo assembly. without independent linking information, it is not strictly possible to distinguish between the fixed gaps native to the library and the sequence gaps that evolve stochastically as a part of the coverage process. under ideal conditions, library gaps would clearly manifest themselves only in the limit of an infinite number of clones, because all sequence gaps should vanish.

so-called "edge effects" must also be considered for filtered libraries. demonstrating this phenomenon is a matter of simple probability. suppose a genic island of size σ is being covered by random sequence reads of length λ, where λ ≪σ. the terminal base position has only about a 1/σ chance of being covered by any individual read, while the associated probability for an interior base position  is roughly λ/σ. such differences can be regarded as a form of position-based sampling bias because preference for coverage is clearly shifted toward the interior island regions. the fraction of an island affected in this way can have significant implications on the evolution of coverage and gaps.

here, we report an extension to the standard lw theory for filtered library configurations. it describes not only the analogs of established lw parameters, but also several new quantities of interest that arise as a consequence of the fragmented nature of the library. preliminary experimental results have been favorable  <cit> , suggesting that filtering procedures will be applied on a broader scale to the most recalcitrant genomes. for such projects, investigators must currently rely on a casual, but unproven adaptation of lw theory. here, all genic islands are artificially concatenated into a single "super-island" and the size of this island is taken as the effective genome size. we will refer to this idealization as the lander-waterman super island  model. because this representation neglects library gaps and the associated edge effects, the degree to which it is applicable to actual projects is not known.

RESULTS
the mathematical model
a gap consists of any genomic region following a read that is not manifested as coverage. two types of gaps arise under this definition. if the uncovered region is represented in the library it is called a "sequence gap", otherwise it is called a "library gap" . the numbers of sequence and library gaps are denoted by the random variables s and l, respectively. we also define the following random variables: c is the number of bases covered, i is the number of islands hit by at least one sequence read, and r denotes the number of reads hitting a particular island. 

let the filtered library consist of i islands, each of which is σ base-pairs in size. reads are taken to be of length λ base-pairs and are assumed to be iid, as in the standard lw model. read length may include a reduction factor to account for the number of bases effectively lost in detecting overlap with another read  <cit> . we presume, as an upper bound, that read length does not exceed island length, i.e. λ ≤ σ

there are π = σ - λ +  <dig> possible placements of a read on each island, and consequently, Π = iπ total placements within the library. assuming n reads have been processed, the expected values of the random variables are given by the following theorems.

theorem  <dig> . the expected number of library gaps is

e〈l〉 = i,

where e ≈  <dig>  is euler's number.

theorem  <dig> . the expected number of sequence gaps when λ ≤ / <dig> is



where n/Π is constrained according to lemma  <dig>  ρ = nλ/Π, and δ = σ -  <dig> 

theorem  <dig> . the expected number of bases represented by the library that are covered by at least one sequence read is



where λ ≤ σ/ <dig> 

theorem  <dig> . the number of sequence reads placed on a specific island follows a poisson distribution with an average value  of e〈r〉= n/i. in particular, the probability that the island is not hit by any reads is exp.

theorem  <dig> . the distribution of the number of genic islands hit by one or more sequence reads is



where μ = i exp and the expected value is

e〈i〉 = i.

theorems  <dig> and  <dig> have been derived according to parameters of current biological interest . they also adhere to their respectively stated, but less-restrictive mathematical conditions. however, it is straightforward to modify them when λ is larger relative to σ .

these results enable one to probabilistically characterize the shotgun sequencing process for filtered dna libraries in much the same way that standard lw theory is used for conventional libraries. filtering is expected to play a significant role for the most difficult, repeat-laden genomes, where cost and assembly issues may limit the success of conventional techniques.

investigators have had to rely on a rudimentary adaptation of lw theory, whereby the fragmented library is modeled as a single "super-island"  <cit> . here, there are i σ - λ +  <dig> ≈ i σ possibilities for placing clones of length λ. in actuality, significantly fewer placements exist, i, owing to the discontinuities between islands. some statistics will be dramatically skewed as a consequence, for example the expected contig size will not converge to the correct value of σ. accuracy of other quantities is not clear. also, there is no provision to estimate island-specific statistics, such as the number of islands hit by at least one read. the idealized lwsi model is correct only for the special case i =  <dig>  although errors for some of the variables will be minimal if λ/σ is sufficiently small.

here, we examine the sequencing process over a range of parameters to discern the general trends that one should be aware of. we also characterize some of the practical applications relevant to filtered libraries and assess the applicability of the "super island"  assumption. our model can readily be applied to specific projects, as well.

coverage characteristics
maize can be taken as a representative high-repeat genome. whitelaw et al.  <cit>  describe shotgun sequencing from filtered libraries. they report an average read length of  <dig> bases and  <dig> base minimal overlap, so that the effective read length is λ ≈  <dig> bp. genome size and repeat content are taken as  <dig>  gb and 80%, respectively  <cit> . if, for the moment, we assume perfect filtering, the resulting library would comprise about  <dig> mb of dna sequence.

island size, unlike read length, cannot readily be characterized a priori. maize genes are thought to reside predominantly in small, roughly  <dig> kb regions of unmethylated dna, which are surrounded by tracts of 20– <dig> kb highly methylated, high-copy sequence  <cit> . thus, the maize gene space appears to be well-dispersed across the physical genome with most genes being distinctly separated from one another  <cit> . recent analysis of bac clones supports this view  <cit> . in order to demonstrate trends of interest, we will assume a representative island size of  <dig>  bases, but will additionally examine several hypothetical islands that are multiples of this value .

evolution of the coverage process is shown in fig.  <dig> for the various island lengths, as well as the idealized lwsi model  <cit> . evidently, there is little difference in performance up to about 1× sequence redundancy. that is, coverage is largely independent of the size of islands in the library. this reflects the tendency of reads to generate new coverage early in a project, rather than increasing overlaps of existing coverage. gaps in the library appear to have little influence in this stage. even in the case of small islands, it is likely that reads are preferentially populating the uncovered, central portions of the various islands.

as more reads are processed, we would expect this trend to change. recall that the probability of a read covering a specific base position decreases closer to the edge of an island. this is the "edge effect". in this case, reads will tend to generate commensurately higher rates of overlap in the central regions, while the end regions will be covered at a slower pace. indeed, fig.  <dig> shows that behavior begins to diverge appreciably above 2× redundancy. coverage becomes a strong function of island size.

fig.  <dig> also indicates that the fraction of a filtered library that one can reasonably hope to obtain via random methods depends upon island size. for example, in the typical case of  <dig>  bp islands, one would still expect to be missing about 4% of the sequence after processing 8× worth of reads. this figure contrasts with a 4% vacancy rate at slightly more than 3× redundancy with conventional libraries. here, we would anticipate essentially complete coverage at the 8× milestone. for libraries consisting of  <dig>  bp and  <dig>  bp islands, the situation is more favorable. the model predicts vacancy rates of only about  <dig> % and  <dig> %, respectively, at 8× redundancy. directed methods may be necessary for resolving the sequence at island edges.

the above observations call attention to a somewhat puzzling difference between filtered and conventional libraries. it is well-known that longer reads yield improved coverage performance for the latter. specifically, coverage goes exponentially according to the redundancy, defined as nλ/g, where g is the genome size. increasing the read length, in particular the ratio λ/g, implies that commensurately higher coverages could be obtained with a given number of reads. however, we have just observed that increasing the analogous ratio λ/σ in filtered libraries seems to slow the overall coverage rate.

this rather paradoxical behavior can be explained precisely in terms of the edge effect. in examining thm.  <dig> more closely, we see that the first term  quantifies the coverage dynamics of the end regions. the coverage probability for any specific base in this region is not a function of read length , but the fraction of the island affected in this way is. thus, longer reads impart edge effects over a larger percentage of each island. moreover, the average difference in coverage probability between boundary and interior regions for a read is λ/ <dig>  thus, the disparity in coverage probability between the two regions also grows in proportion to read length. again paradoxically, this effect starts to diminish if reads become sufficiently long and finally vanishes in the limit of λ → σ. however, this is simply a consequence of the fact that all base positions once again have an equal chance of being covered, so edge effects disappear. although perhaps not obvious, this limiting case is described by thm.  <dig> 

gap census and contig length trends
again using maize parameters as an example, fig.  <dig> shows evolution of sequence gaps for the three island lengths in table  <dig>  as well as the idealized lwsi model. these curves are computed from thm.  <dig> and are shown in the usual units of i σ/λ  <cit> .

as with coverage, performance appears to be mostly independent of island size up to about 1× sequence redundancy, but the cases differ appreciably after that. the rates of gap closure decline significantly as the islands become smaller. underlying dynamics are similar to those discussed above for coverage. it is worth noting that these trends are fundamentally different from what one realizes when varying effective read length. in that instance, the apparent number of gaps rises as reads become effectively shorter. we find a similar behavior here when island length is held fixed, although the convergence point is independent of read length . this effect is a rather subtle consequence of the original method devised for modeling detection thresholds. it is discussed extensively in ref.  <cit> , primarily in the context of fingerprint mapping. however, the phenomenon is not as relevant to shotgun sequencing because detection thresholds are small relative to read length and largely constant. here, we expect island size to be the more influential variable.

strictly speaking, library and sequence gaps are not completely independent of one another as we have implied here. for instance, the generation of a library gap is synonymous with placing a read in the end position of an island. this event may inadvertently eliminate a sequence gap, as well. we cannot rigorously claim that the total number of gaps at any given point is simply the sum of the two gap types. however, according to thm.  <dig>  the actual number of library gaps should always be small compared to the number of sequence gaps . to be more specific, the rate of library gap formation is very slow; there are only i placements of a possible i  for which a read will spawn such a gap. consequently, we can take the sequence gap census alone as a good approximation for the total number of gaps.

assuming independence of the variables, we can approximate the expected length of contiguous segments as e〈c〉/e〈s〉. this expression is plotted in fig.  <dig>  note that curves derived from the filtered model converge essentially to their respective island lengths, while the lwsi model diverges. this is a well-known anomaly in the basic lander-waterman formulation  <cit> , although it has since been resolved  <cit> . convergence to maximum contig length also appears to be faster for shorter islands. for example, for  <dig>  bp islands there is little increase in average contig length after 5× sequence redundancy, while the  <dig>  bp case is still developing even at 7× redundancy. given the fundamental difference in longer-term behavior, it is somewhat surprising that the lwsi seems to be a better short-term indicator for contig length as compared to coverage and gaps. specifically, predicted lengths seem to be independent of island size up to about 2× sequence redundancy, rather than the 1× limit observed for the other variables.

application for gene tagging
one of the growing applications we anticipate for filtered libraries is as a sampling method to rapidly prototype gene sets. this procedure is referred to as "gene tagging"  <cit> . here, one simply obtains a light random sampling of the filtered library and assesses gene hits via homology searching. a number of fundamental questions revolve around how gene hits will be distributed for a given number of sequencing reads. if we take island hits as an analog of gene hits, thms.  <dig> and  <dig> are useful for formulating predictions. conversely, the lwsi model is not suited to such calculations because there is no consideration of how islands are actually separated from one another.

investigators are often interested in rudimentary estimates of the number of genes hit, for which we can apply either of these theorems. here, the governing parameter is exp, so that island and read lengths are irrelevant. data from two recent projects are available for comparison: a methyl-filtered sorghum  library sampled at roughly  <dig> × redundancy  <cit>  and a combination methyl-filtered high-cot maize  library sampled at roughly  <dig> × redundancy  <cit> . in the former case, library size and average gene size were estimated as iσ ≈  <dig> mb and σ ≈  <dig> kb, respectively. tagging results are based on comparisons to  <dig> genes annotated from finished sorghum bac clones  <cit> . for the latter case, we calculate theoretical performance using the maize estimates described above. maize tagging results are based on wu-blastn  comparisons to  <dig> highly-annotated maize b <dig> genes  <cit>  at a minimum identity of 98%.

fig.  <dig> shows the expected fraction of genes hit according to both thm.  <dig> and the experimental data. theoretical curves depend on the number of islands, as calculated from parameter estimates. in particular, the sorghum library is modeled as having i =  <dig> × 106/ <dig>  =  <dig>   <dig> islands, while the number of maize islands is estimated at  <dig> × 106/ <dig>  =  <dig> . agreement is relatively good in both cases up to about 60–70% of the gene space, after which the theory begins to over-predict the actual gene representation. here, each empirical curve lies > <dig> standard deviations below its respective theoretical prediction . this suggests systematic rather than stochastic factors account for the difference. specifically, biases in the data are assumed to be present, although they are difficult to characterize at this stage. for example, bedell et al.  <cit>  speculate that perhaps 10% of sorghum repeats may be under-methylated, and thus able to survive the filtering process to some degree. similarly, whitelaw et al.  <cit>  found a non-trivial number of retrotransposons in their combined methyl-filtered high-cot maize library. tagging also depends on the ability to identify suitable genes to assess, which itself is difficult and subject to error.

a more sophisticated calculation can be made with the probability distribution given by thm.  <dig>  again using the parameters cited by bedell et al.  <cit> , we plot the tail probability of hitting various fractions of the gene space as a function of sequence redundancy in fig.  <dig>  as we would intuitively expect, the required redundancy increases with the fraction of the gene space desired. the curves are surprisingly sharp in all cases. that is, the theoretical milestones for gene-tagging appear to be very-well defined. for example, the probability of tagging at least 95% of the gene space is vanishingly small below  <dig> × sequence redundancy, but approaches unity upon reaching  <dig> × redundancy. these analyses are clearly subject to the biases discussed above. for example, fig.  <dig> suggests that the  <dig> × sequencing depth should probably have captured about 99% of the sorghum genes. however, bedell et al.  <cit>  calculated the actual value to be about 95%. the gene tagging process becomes more efficient as gene size increases because the number of genic islands is commensurately less for a given library size .

estimating genic enrichment
whitelaw et al.  <cit>  proposed the idea of using the gap census predicted by lw theory, specifically the lwsi adaptation, to compute effective filtered genome size gl from preliminary shotgun data. the lw equation can readily be solved for gl as



so that the number of sequence gaps e〈s〉 serves as an indicator of gl. one can then estimate a genic enrichment factor g/gl, where g is the full genome size. whitelaw et al.  <cit>  performed such calculations for methyl-filtered and high-cot maize libraries at less than  <dig> × redundancy. bedell et al.  <cit>  also applied this concept to a methyl-filtered sorghum library at about 1× redundancy.

these calculations are founded on speculation that library gaps and edge effects could be ignored. we already described how performance is essentially independent of such factors when sequence redundancy remains below 1×. it therefore appears that these two particular computations are reasonable. however, this is clearly not the case in general. standard lw theory will tend to under-estimate gaps, and consequently to under-estimate gl for higher redundancies. genic enrichment factors would be artificially high. from a practical standpoint, light shotgun redundancy in conjunction with eq.  <dig> seems to be a legitimate and convenient way to characterize enrichment; there is little penalty in neglecting edge effects and one need not estimate island size.

we note that gl in eq.  <dig> can be further characterized in terms of lower and upper bounds using the appropriate distribution moments  <cit> . for example, whitelaw et al.  <cit>  calculated the size of the combination methyl-filtered high-cot maize library to be roughly  <dig> mb. performing similar computations at  <dig> standard deviations above and below the mean, we estimate that the lower and upper limits for library size are approximately  <dig>  and  <dig>  mb, respectively.

peterson et al.  <cit>  proposed a method for the complementary task; they compute the number of reads needed to cover a given filtered library fraction based on the "super-island" assumption inherent in the clarke-carbon equation  <cit> . according to the above discussion, this is, in principle, a reasonable approach. however, their specific calculations are synonymous with a redundancy exceeding 4× , making their estimates for n too low. in fact, fig.  <dig> suggests that edge effects will make 99% random coverage difficult to achieve for any filtered library.

CONCLUSIONS
the primary assumption associated with dna processing models is that entities are distributed in an iid fashion. because there is little in the broad spectrum of experimental data that corroborates this supposition  <cit> , we must regard our results in the context of upper bounds of performance. actual projects should generally fall somewhat short of predictions. moreover, it can be difficult to a priori estimate input parameters, especially the number and average size of islands. consequently, theoretical results for specific projects should be interpreted with these limitations in mind.

according to the trends discussed here, it is clear that if island size is sufficiently large compared to read length, the lwsi model will be sufficient for predicting a number of relevant parameters. however, with the exception of enrichment estimation discussed above, it does not appear that this will be the case for most projects. for example, we examined island sizes up to  <dig> kb, but edge effects were still noticeable for reads ~ <dig> bp in length. it is unclear whether there are species whose average island length would be substantially larger than this. moreover, there is an ongoing trend toward longer reads  <cit> . coupled with the need to calculate island-specific parameters, we feel the model described here will play a role for filtered library projects analogous to the one already established by standard lw theory for conventional libraries.

we also suggest potential application of this model for other non-traditional sequencing scenarios. for example, there is increasing interest in sequencing ciliated protozoa  <cit> . the macronuclear genomes of such organisms consist of > <dig>  distinct "nano-chromosomes", with an average length of less than  <dig>  base pairs. because most of these chromosome structures are too long to be traversed with end-sequences, it is likely that a shotgun approach will be necessary.

the observations made here have a number of practical implications for the planning and execution of future filtered library shotgun projects. in general, the progress realized when using standard "full-length" reads will be less than that of the equivalent wgs project. in many cases, this implies stopping at what are conventionally considered to be only moderate redundancies. for example, results shown in figs  <dig>   <dig>  and  <dig> suggest little is gained in sequencing  <dig> kb islands past about 5×. likewise, they indicate that assemblies would have less sequence coverage and less contiguity as compared to equivalent wgs projects. improved economy and performance of directed methods become commensurately more important.

the model establishes λ/σ as the primary parameter governing edge effects. by varying island size, we found that results for a given value of sequence depth improved as λ/σ decreases. the same effect can clearly be obtained by decreasing read length for a given island size. pyro-sequencing platforms immediately suggest themselves as a good potential match for this application. for example, current effective read lengths of about  <dig> bp  <cit>  imply λ/σ =  <dig>  for  <dig> kb islands. here, results would be roughly equivalent to what is shown for the  <dig> kb islands in figs  <dig>   <dig>  and  <dig> using full-length reads. that is, contiguity and sequence coverage would be much improved. because islands correlate with low-copy sequence content, we would not expect reduced read lengths to substantially impede the assembly process.

